{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346a5716",
   "metadata": {},
   "source": [
    "Authored by Arqam Patel. \n",
    "\n",
    "CC BY 4.0 License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1cdb7",
   "metadata": {},
   "source": [
    "# Logistic Regression for binary classification\n",
    "\n",
    "In this notebook, we'll explore three ways of doing logistic regression for binary classification with gradient descent:\n",
    "* Computing the gradient function manually\n",
    "* Abstracting gradient computation using Torch autograd\n",
    "* Using all Torch neural network abstractions, including optimisers\n",
    "\n",
    "The aim is to realise how deep learning frameworks make our life significantly easier- imagine having to calculate gradients manually for much more complex networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636a0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f445095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size= 0.1, random_state = 42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# X_train = torch.tensor(X_train, dtype = torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype = torch.float32)\n",
    "y_train = torch.unsqueeze(y_train, 1)\n",
    "# X_test = torch.tensor(X_test, dtype = torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype = torch.float32)\n",
    "y_test = torch.unsqueeze(y_test, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054c378",
   "metadata": {},
   "source": [
    "The sigmoid function transforms real values into the target range from 0 to 1. It is thus useful for converting linear regression values into \"probability\" values that a certain datapoint belongs to a certain class.\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "$$ \\frac{d\\sigma(x)}{dx} = \\sigma(x) (1- \\sigma(x)) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad38535",
   "metadata": {},
   "source": [
    "$\\hat{y}_i$ represent predicted probability that the given sample belongs to class 1.\n",
    "$$\\hat{y}_i = \\sigma(x_i'\\beta)$$\n",
    "\n",
    "$$\\frac{d\\hat{y}_i}{d\\beta} = X'(\\sigma'(x_i'\\beta))$$\n",
    "\n",
    "Cross entropy loss L is given by:\n",
    "$$L = - \\sum (y_i log(\\hat{y}_i) + (1-y_i)log(1-\\hat{y}_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703b606a",
   "metadata": {},
   "source": [
    "# Gradient computation for logistic regression\n",
    "\n",
    "Given an input vector $ X $ (with each row as a data point and each column as a feature) and a weight vector $ \\beta $, the logistic regression model predicts the probability $ \\hat{y} $ as:\n",
    "\n",
    "$ \\hat{y} = \\sigma(X\\beta) $\n",
    "\n",
    "where $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ is the sigmoid function.\n",
    "\n",
    "### Derivative of the Sigmoid Function\n",
    "\n",
    "The derivative of the sigmoid function, $ \\sigma'(z) $, is:\n",
    "\n",
    "$ \\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z)) $\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The binary cross-entropy loss is:\n",
    "\n",
    "$ L = -\\sum [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y_i})] $\n",
    "\n",
    "### Gradient of the Loss Function with Respect to Predictions\n",
    "\n",
    "The gradient of the loss function with respect to the predictions $ \\hat{y} $ is:\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}} $\n",
    "\n",
    "### Chain Rule for the Complete Gradient\n",
    "\n",
    "To find the gradient with respect to the weights $ \\beta $, we apply the chain rule. The complete gradient of the loss with respect to the weights is the product of the gradient of the loss with respect to the predictions and the derivative of the predictions with respect to the weights.\n",
    "\n",
    "Since $ \\hat{y} = \\sigma(X\\beta) $, the derivative of $ \\hat{y} $ with respect to $ \\beta $ is $ X^T \\cdot \\sigma'(X\\beta) $.\n",
    "\n",
    "Combining these, the gradient of the loss with respect to the weights is:\n",
    "\n",
    "$ \\nabla_\\beta L = X^T \\left( \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\sigma'(X\\beta) \\right) $\n",
    "\n",
    "Plugging in the expressions for $ \\frac{\\partial L}{\\partial \\hat{y}} $ and $ \\sigma'(z) $, we get:\n",
    "\n",
    "$ \\nabla_\\beta L = X^T \\left( \\left( -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}} \\right) \\cdot \\left( \\hat{y} \\cdot (1 - \\hat{y}) \\right) \\right) $\n",
    "\n",
    "Simplifying further:\n",
    "\n",
    "$ \\nabla_\\beta L =  X^T (\\hat{y} - y) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af9f76",
   "metadata": {},
   "source": [
    "# Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b30bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLR():\n",
    "    def __init__(self, p, lr = 0.01):\n",
    "        # initialise parameters as px1 column vector of zeros\n",
    "        self.params = torch.zeros((p,1))\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data: n x p-1\n",
    "        n = data.shape[0]\n",
    "        X = torch.cat((data, torch.ones(n, 1)), 1)\n",
    "        return torch.sigmoid(X @ self.params)\n",
    "    \n",
    "    def loss(self, data, true):\n",
    "        n = data.shape[0]\n",
    "        pred = self.forward(data)\n",
    "        # binary cross entropy loss\n",
    "        loss = 1/n*torch.sum(-(true* torch.log(pred) + (1-true)*torch.log(1-pred)))\n",
    "        return loss\n",
    "             \n",
    "    def grad(self, data, true):\n",
    "        n = data.shape[0]\n",
    "        pred = self.forward(data)\n",
    "        n = data.shape[0]\n",
    "        X = torch.cat((data, torch.ones(n, 1)), 1)\n",
    "        return 1/n* X.t()@(pred - true) \n",
    "        \n",
    "    def step(self, data, true):\n",
    "        self.params = self.params - self.lr * self.grad(data, true)\n",
    "        \n",
    "    def train(self, data, true, n_steps):\n",
    "        logs = []\n",
    "        for i in range(n_steps):\n",
    "            print(f\"Step no {i}\")\n",
    "            self.step(data, true)\n",
    "            loss = self.loss(data, true)\n",
    "            print(f\"Loss {loss.item()}\")\n",
    "            logs.append(loss.item())\n",
    "            \n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfddeaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic1 = ScratchLR(31, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77133ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step no 0\n",
      "Loss 0.6733601093292236\n",
      "Step no 1\n",
      "Loss 0.654815673828125\n",
      "Step no 2\n",
      "Loss 0.6374273300170898\n",
      "Step no 3\n",
      "Loss 0.6211110353469849\n",
      "Step no 4\n",
      "Loss 0.6057862639427185\n",
      "Step no 5\n",
      "Loss 0.5913771986961365\n",
      "Step no 6\n",
      "Loss 0.5778127908706665\n",
      "Step no 7\n",
      "Loss 0.5650271773338318\n",
      "Step no 8\n",
      "Loss 0.5529598593711853\n",
      "Step no 9\n",
      "Loss 0.5415550470352173\n",
      "Step no 10\n",
      "Loss 0.5307618975639343\n",
      "Step no 11\n",
      "Loss 0.5205340385437012\n",
      "Step no 12\n",
      "Loss 0.5108290910720825\n",
      "Step no 13\n",
      "Loss 0.5016086101531982\n",
      "Step no 14\n",
      "Loss 0.49283748865127563\n",
      "Step no 15\n",
      "Loss 0.48448359966278076\n",
      "Step no 16\n",
      "Loss 0.47651779651641846\n",
      "Step no 17\n",
      "Loss 0.4689134657382965\n",
      "Step no 18\n",
      "Loss 0.46164631843566895\n",
      "Step no 19\n",
      "Loss 0.4546939432621002\n",
      "Step no 20\n",
      "Loss 0.4480360746383667\n",
      "Step no 21\n",
      "Loss 0.44165390729904175\n",
      "Step no 22\n",
      "Loss 0.43553024530410767\n",
      "Step no 23\n",
      "Loss 0.42964935302734375\n",
      "Step no 24\n",
      "Loss 0.4239965081214905\n",
      "Step no 25\n",
      "Loss 0.4185584485530853\n",
      "Step no 26\n",
      "Loss 0.41332265734672546\n",
      "Step no 27\n",
      "Loss 0.4082776606082916\n",
      "Step no 28\n",
      "Loss 0.4034128189086914\n",
      "Step no 29\n",
      "Loss 0.39871838688850403\n",
      "Step no 30\n",
      "Loss 0.39418506622314453\n",
      "Step no 31\n",
      "Loss 0.3898043632507324\n",
      "Step no 32\n",
      "Loss 0.38556843996047974\n",
      "Step no 33\n",
      "Loss 0.38146984577178955\n",
      "Step no 34\n",
      "Loss 0.3775016963481903\n",
      "Step no 35\n",
      "Loss 0.37365755438804626\n",
      "Step no 36\n",
      "Loss 0.36993148922920227\n",
      "Step no 37\n",
      "Loss 0.3663177192211151\n",
      "Step no 38\n",
      "Loss 0.36281105875968933\n",
      "Step no 39\n",
      "Loss 0.35940659046173096\n",
      "Step no 40\n",
      "Loss 0.35609957575798035\n",
      "Step no 41\n",
      "Loss 0.35288575291633606\n",
      "Step no 42\n",
      "Loss 0.34976089000701904\n",
      "Step no 43\n",
      "Loss 0.34672120213508606\n",
      "Step no 44\n",
      "Loss 0.3437630534172058\n",
      "Step no 45\n",
      "Loss 0.34088295698165894\n",
      "Step no 46\n",
      "Loss 0.33807769417762756\n",
      "Step no 47\n",
      "Loss 0.335344135761261\n",
      "Step no 48\n",
      "Loss 0.33267951011657715\n",
      "Step no 49\n",
      "Loss 0.3300809860229492\n",
      "Step no 50\n",
      "Loss 0.32754600048065186\n",
      "Step no 51\n",
      "Loss 0.3250720500946045\n",
      "Step no 52\n",
      "Loss 0.32265686988830566\n",
      "Step no 53\n",
      "Loss 0.3202981650829315\n",
      "Step no 54\n",
      "Loss 0.3179938793182373\n",
      "Step no 55\n",
      "Loss 0.31574204564094543\n",
      "Step no 56\n",
      "Loss 0.3135406970977783\n",
      "Step no 57\n",
      "Loss 0.3113880157470703\n",
      "Step no 58\n",
      "Loss 0.3092823922634125\n",
      "Step no 59\n",
      "Loss 0.30722206830978394\n",
      "Step no 60\n",
      "Loss 0.3052055835723877\n",
      "Step no 61\n",
      "Loss 0.3032313585281372\n",
      "Step no 62\n",
      "Loss 0.30129796266555786\n",
      "Step no 63\n",
      "Loss 0.2994041442871094\n",
      "Step no 64\n",
      "Loss 0.2975485324859619\n",
      "Step no 65\n",
      "Loss 0.2957299053668976\n",
      "Step no 66\n",
      "Loss 0.29394710063934326\n",
      "Step no 67\n",
      "Loss 0.29219892621040344\n",
      "Step no 68\n",
      "Loss 0.29048433899879456\n",
      "Step no 69\n",
      "Loss 0.28880223631858826\n",
      "Step no 70\n",
      "Loss 0.2871517539024353\n",
      "Step no 71\n",
      "Loss 0.28553181886672974\n",
      "Step no 72\n",
      "Loss 0.2839415669441223\n",
      "Step no 73\n",
      "Loss 0.2823801338672638\n",
      "Step no 74\n",
      "Loss 0.28084656596183777\n",
      "Step no 75\n",
      "Loss 0.2793402075767517\n",
      "Step no 76\n",
      "Loss 0.27786019444465637\n",
      "Step no 77\n",
      "Loss 0.27640581130981445\n",
      "Step no 78\n",
      "Loss 0.27497631311416626\n",
      "Step no 79\n",
      "Loss 0.27357104420661926\n",
      "Step no 80\n",
      "Loss 0.27218928933143616\n",
      "Step no 81\n",
      "Loss 0.2708304524421692\n",
      "Step no 82\n",
      "Loss 0.26949384808540344\n",
      "Step no 83\n",
      "Loss 0.2681789696216583\n",
      "Step no 84\n",
      "Loss 0.2668852210044861\n",
      "Step no 85\n",
      "Loss 0.26561203598976135\n",
      "Step no 86\n",
      "Loss 0.26435887813568115\n",
      "Step no 87\n",
      "Loss 0.2631252110004425\n",
      "Step no 88\n",
      "Loss 0.261910617351532\n",
      "Step no 89\n",
      "Loss 0.2607145309448242\n",
      "Step no 90\n",
      "Loss 0.2595365345478058\n",
      "Step no 91\n",
      "Loss 0.25837618112564087\n",
      "Step no 92\n",
      "Loss 0.2572330832481384\n",
      "Step no 93\n",
      "Loss 0.2561066746711731\n",
      "Step no 94\n",
      "Loss 0.25499674677848816\n",
      "Step no 95\n",
      "Loss 0.253902792930603\n",
      "Step no 96\n",
      "Loss 0.25282445549964905\n",
      "Step no 97\n",
      "Loss 0.25176137685775757\n",
      "Step no 98\n",
      "Loss 0.2507132291793823\n",
      "Step no 99\n",
      "Loss 0.24967961013317108\n",
      "Step no 100\n",
      "Loss 0.24866023659706116\n",
      "Step no 101\n",
      "Loss 0.24765479564666748\n",
      "Step no 102\n",
      "Loss 0.2466629296541214\n",
      "Step no 103\n",
      "Loss 0.24568438529968262\n",
      "Step no 104\n",
      "Loss 0.24471883475780487\n",
      "Step no 105\n",
      "Loss 0.24376605451107025\n",
      "Step no 106\n",
      "Loss 0.24282565712928772\n",
      "Step no 107\n",
      "Loss 0.24189749360084534\n",
      "Step no 108\n",
      "Loss 0.24098126590251923\n",
      "Step no 109\n",
      "Loss 0.2400766909122467\n",
      "Step no 110\n",
      "Loss 0.23918354511260986\n",
      "Step no 111\n",
      "Loss 0.2383016049861908\n",
      "Step no 112\n",
      "Loss 0.2374306470155716\n",
      "Step no 113\n",
      "Loss 0.23657044768333435\n",
      "Step no 114\n",
      "Loss 0.23572072386741638\n",
      "Step no 115\n",
      "Loss 0.23488134145736694\n",
      "Step no 116\n",
      "Loss 0.23405207693576813\n",
      "Step no 117\n",
      "Loss 0.23323270678520203\n",
      "Step no 118\n",
      "Loss 0.2324230670928955\n",
      "Step no 119\n",
      "Loss 0.23162294924259186\n",
      "Step no 120\n",
      "Loss 0.23083218932151794\n",
      "Step no 121\n",
      "Loss 0.23005059361457825\n",
      "Step no 122\n",
      "Loss 0.22927798330783844\n",
      "Step no 123\n",
      "Loss 0.22851420938968658\n",
      "Step no 124\n",
      "Loss 0.22775906324386597\n",
      "Step no 125\n",
      "Loss 0.22701247036457062\n",
      "Step no 126\n",
      "Loss 0.22627416253089905\n",
      "Step no 127\n",
      "Loss 0.2255440652370453\n",
      "Step no 128\n",
      "Loss 0.22482198476791382\n",
      "Step no 129\n",
      "Loss 0.22410786151885986\n",
      "Step no 130\n",
      "Loss 0.22340142726898193\n",
      "Step no 131\n",
      "Loss 0.22270265221595764\n",
      "Step no 132\n",
      "Loss 0.22201132774353027\n",
      "Step no 133\n",
      "Loss 0.22132739424705505\n",
      "Step no 134\n",
      "Loss 0.22065065801143646\n",
      "Step no 135\n",
      "Loss 0.21998101472854614\n",
      "Step no 136\n",
      "Loss 0.21931834518909454\n",
      "Step no 137\n",
      "Loss 0.2186625599861145\n",
      "Step no 138\n",
      "Loss 0.21801349520683289\n",
      "Step no 139\n",
      "Loss 0.21737107634544373\n",
      "Step no 140\n",
      "Loss 0.21673515439033508\n",
      "Step no 141\n",
      "Loss 0.2161056399345398\n",
      "Step no 142\n",
      "Loss 0.2154824435710907\n",
      "Step no 143\n",
      "Loss 0.21486544609069824\n",
      "Step no 144\n",
      "Loss 0.21425452828407288\n",
      "Step no 145\n",
      "Loss 0.2136496752500534\n",
      "Step no 146\n",
      "Loss 0.2130506932735443\n",
      "Step no 147\n",
      "Loss 0.21245749294757843\n",
      "Step no 148\n",
      "Loss 0.21187004446983337\n",
      "Step no 149\n",
      "Loss 0.2112881988286972\n",
      "Step no 150\n",
      "Loss 0.21071192622184753\n",
      "Step no 151\n",
      "Loss 0.210141122341156\n",
      "Step no 152\n",
      "Loss 0.20957565307617188\n",
      "Step no 153\n",
      "Loss 0.20901547372341156\n",
      "Step no 154\n",
      "Loss 0.2084605097770691\n",
      "Step no 155\n",
      "Loss 0.2079106718301773\n",
      "Step no 156\n",
      "Loss 0.20736590027809143\n",
      "Step no 157\n",
      "Loss 0.2068260908126831\n",
      "Step no 158\n",
      "Loss 0.20629119873046875\n",
      "Step no 159\n",
      "Loss 0.2057611048221588\n",
      "Step no 160\n",
      "Loss 0.2052358090877533\n",
      "Step no 161\n",
      "Loss 0.20471519231796265\n",
      "Step no 162\n",
      "Loss 0.2041991800069809\n",
      "Step no 163\n",
      "Loss 0.20368772745132446\n",
      "Step no 164\n",
      "Loss 0.20318078994750977\n",
      "Step no 165\n",
      "Loss 0.20267824828624725\n",
      "Step no 166\n",
      "Loss 0.20218008756637573\n",
      "Step no 167\n",
      "Loss 0.20168620347976685\n",
      "Step no 168\n",
      "Loss 0.2011965811252594\n",
      "Step no 169\n",
      "Loss 0.20071113109588623\n",
      "Step no 170\n",
      "Loss 0.20022980868816376\n",
      "Step no 171\n",
      "Loss 0.199752539396286\n",
      "Step no 172\n",
      "Loss 0.1992792785167694\n",
      "Step no 173\n",
      "Loss 0.19880998134613037\n",
      "Step no 174\n",
      "Loss 0.19834460318088531\n",
      "Step no 175\n",
      "Loss 0.19788308441638947\n",
      "Step no 176\n",
      "Loss 0.19742533564567566\n",
      "Step no 177\n",
      "Loss 0.19697131216526031\n",
      "Step no 178\n",
      "Loss 0.19652104377746582\n",
      "Step no 179\n",
      "Loss 0.19607439637184143\n",
      "Step no 180\n",
      "Loss 0.19563134014606476\n",
      "Step no 181\n",
      "Loss 0.19519184529781342\n",
      "Step no 182\n",
      "Loss 0.19475585222244263\n",
      "Step no 183\n",
      "Loss 0.19432333111763\n",
      "Step no 184\n",
      "Loss 0.19389422237873077\n",
      "Step no 185\n",
      "Loss 0.19346849620342255\n",
      "Step no 186\n",
      "Loss 0.19304607808589935\n",
      "Step no 187\n",
      "Loss 0.192626953125\n",
      "Step no 188\n",
      "Loss 0.1922110915184021\n",
      "Step no 189\n",
      "Loss 0.19179841876029968\n",
      "Step no 190\n",
      "Loss 0.19138894975185394\n",
      "Step no 191\n",
      "Loss 0.19098256528377533\n",
      "Step no 192\n",
      "Loss 0.19057928025722504\n",
      "Step no 193\n",
      "Loss 0.1901790350675583\n",
      "Step no 194\n",
      "Loss 0.18978184461593628\n",
      "Step no 195\n",
      "Loss 0.18938758969306946\n",
      "Step no 196\n",
      "Loss 0.18899627029895782\n",
      "Step no 197\n",
      "Loss 0.18860788643360138\n",
      "Step no 198\n",
      "Loss 0.18822236359119415\n",
      "Step no 199\n",
      "Loss 0.18783970177173615\n",
      "Step no 200\n",
      "Loss 0.1874598115682602\n",
      "Step no 201\n",
      "Loss 0.1870826780796051\n",
      "Step no 202\n",
      "Loss 0.18670830130577087\n",
      "Step no 203\n",
      "Loss 0.18633662164211273\n",
      "Step no 204\n",
      "Loss 0.18596762418746948\n",
      "Step no 205\n",
      "Loss 0.18560126423835754\n",
      "Step no 206\n",
      "Loss 0.18523749709129333\n",
      "Step no 207\n",
      "Loss 0.18487633764743805\n",
      "Step no 208\n",
      "Loss 0.18451771140098572\n",
      "Step no 209\n",
      "Loss 0.18416161835193634\n",
      "Step no 210\n",
      "Loss 0.18380801379680634\n",
      "Step no 211\n",
      "Loss 0.18345686793327332\n",
      "Step no 212\n",
      "Loss 0.1831081658601761\n",
      "Step no 213\n",
      "Loss 0.18276187777519226\n",
      "Step no 214\n",
      "Loss 0.18241798877716064\n",
      "Step no 215\n",
      "Loss 0.18207645416259766\n",
      "Step no 216\n",
      "Loss 0.18173722922801971\n",
      "Step no 217\n",
      "Loss 0.181400328874588\n",
      "Step no 218\n",
      "Loss 0.18106567859649658\n",
      "Step no 219\n",
      "Loss 0.1807333081960678\n",
      "Step no 220\n",
      "Loss 0.18040315806865692\n",
      "Step no 221\n",
      "Loss 0.18007522821426392\n",
      "Step no 222\n",
      "Loss 0.1797494888305664\n",
      "Step no 223\n",
      "Loss 0.1794258952140808\n",
      "Step no 224\n",
      "Loss 0.17910441756248474\n",
      "Step no 225\n",
      "Loss 0.1787850707769394\n",
      "Step no 226\n",
      "Loss 0.17846783995628357\n",
      "Step no 227\n",
      "Loss 0.1781526654958725\n",
      "Step no 228\n",
      "Loss 0.17783953249454498\n",
      "Step no 229\n",
      "Loss 0.17752841114997864\n",
      "Step no 230\n",
      "Loss 0.17721930146217346\n",
      "Step no 231\n",
      "Loss 0.17691218852996826\n",
      "Step no 232\n",
      "Loss 0.17660702764987946\n",
      "Step no 233\n",
      "Loss 0.17630383372306824\n",
      "Step no 234\n",
      "Loss 0.17600254714488983\n",
      "Step no 235\n",
      "Loss 0.17570313811302185\n",
      "Step no 236\n",
      "Loss 0.17540566623210907\n",
      "Step no 237\n",
      "Loss 0.17511002719402313\n",
      "Step no 238\n",
      "Loss 0.17481623589992523\n",
      "Step no 239\n",
      "Loss 0.17452429234981537\n",
      "Step no 240\n",
      "Loss 0.17423415184020996\n",
      "Step no 241\n",
      "Loss 0.17394578456878662\n",
      "Step no 242\n",
      "Loss 0.17365919053554535\n",
      "Step no 243\n",
      "Loss 0.17337438464164734\n",
      "Step no 244\n",
      "Loss 0.17309130728244781\n",
      "Step no 245\n",
      "Loss 0.1728099286556244\n",
      "Step no 246\n",
      "Loss 0.17253027856349945\n",
      "Step no 247\n",
      "Loss 0.17225231230258942\n",
      "Step no 248\n",
      "Loss 0.1719760149717331\n",
      "Step no 249\n",
      "Loss 0.1717013716697693\n",
      "Step no 250\n",
      "Loss 0.1714283674955368\n",
      "Step no 251\n",
      "Loss 0.17115698754787445\n",
      "Step no 252\n",
      "Loss 0.17088721692562103\n",
      "Step no 253\n",
      "Loss 0.17061905562877655\n",
      "Step no 254\n",
      "Loss 0.17035244405269623\n",
      "Step no 255\n",
      "Loss 0.17008744180202484\n",
      "Step no 256\n",
      "Loss 0.16982394456863403\n",
      "Step no 257\n",
      "Loss 0.16956199705600739\n",
      "Step no 258\n",
      "Loss 0.1693015694618225\n",
      "Step no 259\n",
      "Loss 0.16904261708259583\n",
      "Step no 260\n",
      "Loss 0.1687851995229721\n",
      "Step no 261\n",
      "Loss 0.16852925717830658\n",
      "Step no 262\n",
      "Loss 0.16827477514743805\n",
      "Step no 263\n",
      "Loss 0.16802172362804413\n",
      "Step no 264\n",
      "Loss 0.167770117521286\n",
      "Step no 265\n",
      "Loss 0.1675199419260025\n",
      "Step no 266\n",
      "Loss 0.16727116703987122\n",
      "Step no 267\n",
      "Loss 0.16702380776405334\n",
      "Step no 268\n",
      "Loss 0.1667778193950653\n",
      "Step no 269\n",
      "Loss 0.1665332019329071\n",
      "Step no 270\n",
      "Loss 0.16628997027873993\n",
      "Step no 271\n",
      "Loss 0.166048064827919\n",
      "Step no 272\n",
      "Loss 0.16580748558044434\n",
      "Step no 273\n",
      "Loss 0.1655682474374771\n",
      "Step no 274\n",
      "Loss 0.16533033549785614\n",
      "Step no 275\n",
      "Loss 0.16509371995925903\n",
      "Step no 276\n",
      "Loss 0.1648583859205246\n",
      "Step no 277\n",
      "Loss 0.16462433338165283\n",
      "Step no 278\n",
      "Loss 0.16439154744148254\n",
      "Step no 279\n",
      "Loss 0.16415999829769135\n",
      "Step no 280\n",
      "Loss 0.16392973065376282\n",
      "Step no 281\n",
      "Loss 0.163700670003891\n",
      "Step no 282\n",
      "Loss 0.16347286105155945\n",
      "Step no 283\n",
      "Loss 0.1632462441921234\n",
      "Step no 284\n",
      "Loss 0.16302081942558289\n",
      "Step no 285\n",
      "Loss 0.16279661655426025\n",
      "Step no 286\n",
      "Loss 0.16257359087467194\n",
      "Step no 287\n",
      "Loss 0.16235172748565674\n",
      "Step no 288\n",
      "Loss 0.16213102638721466\n",
      "Step no 289\n",
      "Loss 0.1619114875793457\n",
      "Step no 290\n",
      "Loss 0.16169308125972748\n",
      "Step no 291\n",
      "Loss 0.16147582232952118\n",
      "Step no 292\n",
      "Loss 0.16125966608524323\n",
      "Step no 293\n",
      "Loss 0.1610446572303772\n",
      "Step no 294\n",
      "Loss 0.16083070635795593\n",
      "Step no 295\n",
      "Loss 0.1606179028749466\n",
      "Step no 296\n",
      "Loss 0.1604061722755432\n",
      "Step no 297\n",
      "Loss 0.1601954996585846\n",
      "Step no 298\n",
      "Loss 0.15998591482639313\n",
      "Step no 299\n",
      "Loss 0.15977738797664642\n",
      "Step no 300\n",
      "Loss 0.15956991910934448\n",
      "Step no 301\n",
      "Loss 0.15936346352100372\n",
      "Step no 302\n",
      "Loss 0.15915809571743011\n",
      "Step no 303\n",
      "Loss 0.1589537262916565\n",
      "Step no 304\n",
      "Loss 0.15875037014484406\n",
      "Step no 305\n",
      "Loss 0.1585480272769928\n",
      "Step no 306\n",
      "Loss 0.15834668278694153\n",
      "Step no 307\n",
      "Loss 0.15814632177352905\n",
      "Step no 308\n",
      "Loss 0.15794697403907776\n",
      "Step no 309\n",
      "Loss 0.15774860978126526\n",
      "Step no 310\n",
      "Loss 0.15755119919776917\n",
      "Step no 311\n",
      "Loss 0.15735475718975067\n",
      "Step no 312\n",
      "Loss 0.15715926885604858\n",
      "Step no 313\n",
      "Loss 0.1569647341966629\n",
      "Step no 314\n",
      "Loss 0.15677112340927124\n",
      "Step no 315\n",
      "Loss 0.15657846629619598\n",
      "Step no 316\n",
      "Loss 0.15638671815395355\n",
      "Step no 317\n",
      "Loss 0.15619590878486633\n",
      "Step no 318\n",
      "Loss 0.15600597858428955\n",
      "Step no 319\n",
      "Loss 0.15581698715686798\n",
      "Step no 320\n",
      "Loss 0.15562888979911804\n",
      "Step no 321\n",
      "Loss 0.15544165670871735\n",
      "Step no 322\n",
      "Loss 0.15525531768798828\n",
      "Step no 323\n",
      "Loss 0.15506987273693085\n",
      "Step no 324\n",
      "Loss 0.15488529205322266\n",
      "Step no 325\n",
      "Loss 0.15470156073570251\n",
      "Step no 326\n",
      "Loss 0.1545187085866928\n",
      "Step no 327\n",
      "Loss 0.15433669090270996\n",
      "Step no 328\n",
      "Loss 0.15415555238723755\n",
      "Step no 329\n",
      "Loss 0.1539752036333084\n",
      "Step no 330\n",
      "Loss 0.15379571914672852\n",
      "Step no 331\n",
      "Loss 0.1536170393228531\n",
      "Step no 332\n",
      "Loss 0.15343919396400452\n",
      "Step no 333\n",
      "Loss 0.1532621532678604\n",
      "Step no 334\n",
      "Loss 0.15308593213558197\n",
      "Step no 335\n",
      "Loss 0.1529105007648468\n",
      "Step no 336\n",
      "Loss 0.1527358889579773\n",
      "Step no 337\n",
      "Loss 0.15256203711032867\n",
      "Step no 338\n",
      "Loss 0.15238898992538452\n",
      "Step no 339\n",
      "Loss 0.15221673250198364\n",
      "Step no 340\n",
      "Loss 0.15204522013664246\n",
      "Step no 341\n",
      "Loss 0.15187448263168335\n",
      "Step no 342\n",
      "Loss 0.15170450508594513\n",
      "Step no 343\n",
      "Loss 0.1515352725982666\n",
      "Step no 344\n",
      "Loss 0.15136681497097015\n",
      "Step no 345\n",
      "Loss 0.151199072599411\n",
      "Step no 346\n",
      "Loss 0.15103211998939514\n",
      "Step no 347\n",
      "Loss 0.15086586773395538\n",
      "Step no 348\n",
      "Loss 0.15070034563541412\n",
      "Step no 349\n",
      "Loss 0.15053555369377136\n",
      "Step no 350\n",
      "Loss 0.1503715068101883\n",
      "Step no 351\n",
      "Loss 0.15020813047885895\n",
      "Step no 352\n",
      "Loss 0.1500454843044281\n",
      "Step no 353\n",
      "Loss 0.14988353848457336\n",
      "Step no 354\n",
      "Loss 0.14972227811813354\n",
      "Step no 355\n",
      "Loss 0.14956174790859222\n",
      "Step no 356\n",
      "Loss 0.14940188825130463\n",
      "Step no 357\n",
      "Loss 0.14924269914627075\n",
      "Step no 358\n",
      "Loss 0.149084210395813\n",
      "Step no 359\n",
      "Loss 0.14892637729644775\n",
      "Step no 360\n",
      "Loss 0.14876924455165863\n",
      "Step no 361\n",
      "Loss 0.14861273765563965\n",
      "Step no 362\n",
      "Loss 0.14845693111419678\n",
      "Step no 363\n",
      "Loss 0.14830178022384644\n",
      "Step no 364\n",
      "Loss 0.14814725518226624\n",
      "Step no 365\n",
      "Loss 0.14799340069293976\n",
      "Step no 366\n",
      "Loss 0.14784017205238342\n",
      "Step no 367\n",
      "Loss 0.14768759906291962\n",
      "Step no 368\n",
      "Loss 0.14753565192222595\n",
      "Step no 369\n",
      "Loss 0.14738433063030243\n",
      "Step no 370\n",
      "Loss 0.14723365008831024\n",
      "Step no 371\n",
      "Loss 0.147083580493927\n",
      "Step no 372\n",
      "Loss 0.1469341367483139\n",
      "Step no 373\n",
      "Loss 0.14678528904914856\n",
      "Step no 374\n",
      "Loss 0.14663708209991455\n",
      "Step no 375\n",
      "Loss 0.1464894711971283\n",
      "Step no 376\n",
      "Loss 0.1463424414396286\n",
      "Step no 377\n",
      "Loss 0.14619602262973785\n",
      "Step no 378\n",
      "Loss 0.14605019986629486\n",
      "Step no 379\n",
      "Loss 0.14590497314929962\n",
      "Step no 380\n",
      "Loss 0.14576032757759094\n",
      "Step no 381\n",
      "Loss 0.14561624825000763\n",
      "Step no 382\n",
      "Loss 0.14547276496887207\n",
      "Step no 383\n",
      "Loss 0.14532984793186188\n",
      "Step no 384\n",
      "Loss 0.14518751204013824\n",
      "Step no 385\n",
      "Loss 0.14504572749137878\n",
      "Step no 386\n",
      "Loss 0.14490452408790588\n",
      "Step no 387\n",
      "Loss 0.14476387202739716\n",
      "Step no 388\n",
      "Loss 0.1446237862110138\n",
      "Step no 389\n",
      "Loss 0.1444842368364334\n",
      "Step no 390\n",
      "Loss 0.1443452537059784\n",
      "Step no 391\n",
      "Loss 0.14420680701732635\n",
      "Step no 392\n",
      "Loss 0.1440688818693161\n",
      "Step no 393\n",
      "Loss 0.1439315378665924\n",
      "Step no 394\n",
      "Loss 0.1437947154045105\n",
      "Step no 395\n",
      "Loss 0.14365839958190918\n",
      "Step no 396\n",
      "Loss 0.14352263510227203\n",
      "Step no 397\n",
      "Loss 0.14338739216327667\n",
      "Step no 398\n",
      "Loss 0.1432526707649231\n",
      "Step no 399\n",
      "Loss 0.1431184709072113\n",
      "Step no 400\n",
      "Loss 0.1429847776889801\n",
      "Step no 401\n",
      "Loss 0.14285160601139069\n",
      "Step no 402\n",
      "Loss 0.14271895587444305\n",
      "Step no 403\n",
      "Loss 0.14258679747581482\n",
      "Step no 404\n",
      "Loss 0.14245513081550598\n",
      "Step no 405\n",
      "Loss 0.14232397079467773\n",
      "Step no 406\n",
      "Loss 0.14219330251216888\n",
      "Step no 407\n",
      "Loss 0.14206315577030182\n",
      "Step no 408\n",
      "Loss 0.14193347096443176\n",
      "Step no 409\n",
      "Loss 0.1418042778968811\n",
      "Step no 410\n",
      "Loss 0.14167557656764984\n",
      "Step no 411\n",
      "Loss 0.14154735207557678\n",
      "Step no 412\n",
      "Loss 0.14141960442066193\n",
      "Step no 413\n",
      "Loss 0.14129233360290527\n",
      "Step no 414\n",
      "Loss 0.14116555452346802\n",
      "Step no 415\n",
      "Loss 0.14103922247886658\n",
      "Step no 416\n",
      "Loss 0.14091336727142334\n",
      "Step no 417\n",
      "Loss 0.14078795909881592\n",
      "Step no 418\n",
      "Loss 0.1406630426645279\n",
      "Step no 419\n",
      "Loss 0.1405385583639145\n",
      "Step no 420\n",
      "Loss 0.1404145509004593\n",
      "Step no 421\n",
      "Loss 0.1402909755706787\n",
      "Step no 422\n",
      "Loss 0.14016787707805634\n",
      "Step no 423\n",
      "Loss 0.14004521071910858\n",
      "Step no 424\n",
      "Loss 0.13992299139499664\n",
      "Step no 425\n",
      "Loss 0.13980121910572052\n",
      "Step no 426\n",
      "Loss 0.13967987895011902\n",
      "Step no 427\n",
      "Loss 0.13955898582935333\n",
      "Step no 428\n",
      "Loss 0.13943853974342346\n",
      "Step no 429\n",
      "Loss 0.13931849598884583\n",
      "Step no 430\n",
      "Loss 0.1391988843679428\n",
      "Step no 431\n",
      "Loss 0.1390797197818756\n",
      "Step no 432\n",
      "Loss 0.13896097242832184\n",
      "Step no 433\n",
      "Loss 0.1388426423072815\n",
      "Step no 434\n",
      "Loss 0.13872474431991577\n",
      "Step no 435\n",
      "Loss 0.13860724866390228\n",
      "Step no 436\n",
      "Loss 0.13849018514156342\n",
      "Step no 437\n",
      "Loss 0.1383735090494156\n",
      "Step no 438\n",
      "Loss 0.13825726509094238\n",
      "Step no 439\n",
      "Loss 0.1381414234638214\n",
      "Step no 440\n",
      "Loss 0.13802598416805267\n",
      "Step no 441\n",
      "Loss 0.13791096210479736\n",
      "Step no 442\n",
      "Loss 0.1377963274717331\n",
      "Step no 443\n",
      "Loss 0.13768208026885986\n",
      "Step no 444\n",
      "Loss 0.13756826519966125\n",
      "Step no 445\n",
      "Loss 0.1374547928571701\n",
      "Step no 446\n",
      "Loss 0.13734175264835358\n",
      "Step no 447\n",
      "Loss 0.1372290998697281\n",
      "Step no 448\n",
      "Loss 0.13711681962013245\n",
      "Step no 449\n",
      "Loss 0.13700494170188904\n",
      "Step no 450\n",
      "Loss 0.13689343631267548\n",
      "Step no 451\n",
      "Loss 0.13678231835365295\n",
      "Step no 452\n",
      "Loss 0.13667157292366028\n",
      "Step no 453\n",
      "Loss 0.13656120002269745\n",
      "Step no 454\n",
      "Loss 0.13645121455192566\n",
      "Step no 455\n",
      "Loss 0.13634158670902252\n",
      "Step no 456\n",
      "Loss 0.13623234629631042\n",
      "Step no 457\n",
      "Loss 0.13612347841262817\n",
      "Step no 458\n",
      "Loss 0.13601495325565338\n",
      "Step no 459\n",
      "Loss 0.13590680062770844\n",
      "Step no 460\n",
      "Loss 0.13579902052879333\n",
      "Step no 461\n",
      "Loss 0.1356915980577469\n",
      "Step no 462\n",
      "Loss 0.1355845332145691\n",
      "Step no 463\n",
      "Loss 0.13547781109809875\n",
      "Step no 464\n",
      "Loss 0.13537146151065826\n",
      "Step no 465\n",
      "Loss 0.13526546955108643\n",
      "Step no 466\n",
      "Loss 0.13515982031822205\n",
      "Step no 467\n",
      "Loss 0.13505448400974274\n",
      "Step no 468\n",
      "Loss 0.13494956493377686\n",
      "Step no 469\n",
      "Loss 0.13484494388103485\n",
      "Step no 470\n",
      "Loss 0.1347406804561615\n",
      "Step no 471\n",
      "Loss 0.1346367448568344\n",
      "Step no 472\n",
      "Loss 0.13453316688537598\n",
      "Step no 473\n",
      "Loss 0.134429931640625\n",
      "Step no 474\n",
      "Loss 0.1343270242214203\n",
      "Step no 475\n",
      "Loss 0.13422444462776184\n",
      "Step no 476\n",
      "Loss 0.13412219285964966\n",
      "Step no 477\n",
      "Loss 0.13402028381824493\n",
      "Step no 478\n",
      "Loss 0.13391870260238647\n",
      "Step no 479\n",
      "Loss 0.13381744921207428\n",
      "Step no 480\n",
      "Loss 0.13371650874614716\n",
      "Step no 481\n",
      "Loss 0.1336159110069275\n",
      "Step no 482\n",
      "Loss 0.1335156112909317\n",
      "Step no 483\n",
      "Loss 0.13341566920280457\n",
      "Step no 484\n",
      "Loss 0.1333160102367401\n",
      "Step no 485\n",
      "Loss 0.13321669399738312\n",
      "Step no 486\n",
      "Loss 0.13311767578125\n",
      "Step no 487\n",
      "Loss 0.13301898539066315\n",
      "Step no 488\n",
      "Loss 0.13292059302330017\n",
      "Step no 489\n",
      "Loss 0.13282252848148346\n",
      "Step no 490\n",
      "Loss 0.13272474706172943\n",
      "Step no 491\n",
      "Loss 0.13262729346752167\n",
      "Step no 492\n",
      "Loss 0.13253013789653778\n",
      "Step no 493\n",
      "Loss 0.13243331015110016\n",
      "Step no 494\n",
      "Loss 0.13233675062656403\n",
      "Step no 495\n",
      "Loss 0.13224050402641296\n",
      "Step no 496\n",
      "Loss 0.13214455544948578\n",
      "Step no 497\n",
      "Loss 0.13204890489578247\n",
      "Step no 498\n",
      "Loss 0.13195356726646423\n",
      "Step no 499\n",
      "Loss 0.13185851275920868\n",
      "Step no 500\n",
      "Loss 0.1317637413740158\n",
      "Step no 501\n",
      "Loss 0.131669282913208\n",
      "Step no 502\n",
      "Loss 0.1315751075744629\n",
      "Step no 503\n",
      "Loss 0.13148121535778046\n",
      "Step no 504\n",
      "Loss 0.1313876062631607\n",
      "Step no 505\n",
      "Loss 0.13129429519176483\n",
      "Step no 506\n",
      "Loss 0.13120125234127045\n",
      "Step no 507\n",
      "Loss 0.13110852241516113\n",
      "Step no 508\n",
      "Loss 0.1310160607099533\n",
      "Step no 509\n",
      "Loss 0.13092386722564697\n",
      "Step no 510\n",
      "Loss 0.13083195686340332\n",
      "Step no 511\n",
      "Loss 0.13074032962322235\n",
      "Step no 512\n",
      "Loss 0.13064897060394287\n",
      "Step no 513\n",
      "Loss 0.13055790960788727\n",
      "Step no 514\n",
      "Loss 0.13046711683273315\n",
      "Step no 515\n",
      "Loss 0.13037657737731934\n",
      "Step no 516\n",
      "Loss 0.130286306142807\n",
      "Step no 517\n",
      "Loss 0.13019633293151855\n",
      "Step no 518\n",
      "Loss 0.1301065981388092\n",
      "Step no 519\n",
      "Loss 0.13001716136932373\n",
      "Step no 520\n",
      "Loss 0.12992796301841736\n",
      "Step no 521\n",
      "Loss 0.12983904778957367\n",
      "Step no 522\n",
      "Loss 0.12975040078163147\n",
      "Step no 523\n",
      "Loss 0.12966199219226837\n",
      "Step no 524\n",
      "Loss 0.12957385182380676\n",
      "Step no 525\n",
      "Loss 0.12948597967624664\n",
      "Step no 526\n",
      "Loss 0.129398375749588\n",
      "Step no 527\n",
      "Loss 0.12931101024150848\n",
      "Step no 528\n",
      "Loss 0.12922389805316925\n",
      "Step no 529\n",
      "Loss 0.1291370391845703\n",
      "Step no 530\n",
      "Loss 0.12905044853687286\n",
      "Step no 531\n",
      "Loss 0.12896409630775452\n",
      "Step no 532\n",
      "Loss 0.12887799739837646\n",
      "Step no 533\n",
      "Loss 0.1287921667098999\n",
      "Step no 534\n",
      "Loss 0.12870655953884125\n",
      "Step no 535\n",
      "Loss 0.1286212056875229\n",
      "Step no 536\n",
      "Loss 0.12853609025478363\n",
      "Step no 537\n",
      "Loss 0.12845124304294586\n",
      "Step no 538\n",
      "Loss 0.1283666342496872\n",
      "Step no 539\n",
      "Loss 0.12828224897384644\n",
      "Step no 540\n",
      "Loss 0.12819813191890717\n",
      "Step no 541\n",
      "Loss 0.1281142383813858\n",
      "Step no 542\n",
      "Loss 0.12803058326244354\n",
      "Step no 543\n",
      "Loss 0.12794716656208038\n",
      "Step no 544\n",
      "Loss 0.12786398828029633\n",
      "Step no 545\n",
      "Loss 0.12778104841709137\n",
      "Step no 546\n",
      "Loss 0.12769834697246552\n",
      "Step no 547\n",
      "Loss 0.12761586904525757\n",
      "Step no 548\n",
      "Loss 0.12753362953662872\n",
      "Step no 549\n",
      "Loss 0.12745161354541779\n",
      "Step no 550\n",
      "Loss 0.12736985087394714\n",
      "Step no 551\n",
      "Loss 0.1272883117198944\n",
      "Step no 552\n",
      "Loss 0.1272069811820984\n",
      "Step no 553\n",
      "Loss 0.12712587416172028\n",
      "Step no 554\n",
      "Loss 0.12704502046108246\n",
      "Step no 555\n",
      "Loss 0.12696437537670135\n",
      "Step no 556\n",
      "Loss 0.12688396871089935\n",
      "Step no 557\n",
      "Loss 0.12680375576019287\n",
      "Step no 558\n",
      "Loss 0.12672381103038788\n",
      "Step no 559\n",
      "Loss 0.1266440600156784\n",
      "Step no 560\n",
      "Loss 0.12656451761722565\n",
      "Step no 561\n",
      "Loss 0.1264851987361908\n",
      "Step no 562\n",
      "Loss 0.12640611827373505\n",
      "Step no 563\n",
      "Loss 0.126327246427536\n",
      "Step no 564\n",
      "Loss 0.1262485831975937\n",
      "Step no 565\n",
      "Loss 0.12617012858390808\n",
      "Step no 566\n",
      "Loss 0.12609189748764038\n",
      "Step no 567\n",
      "Loss 0.1260138899087906\n",
      "Step no 568\n",
      "Loss 0.12593607604503632\n",
      "Step no 569\n",
      "Loss 0.12585848569869995\n",
      "Step no 570\n",
      "Loss 0.1257811039686203\n",
      "Step no 571\n",
      "Loss 0.12570393085479736\n",
      "Step no 572\n",
      "Loss 0.12562698125839233\n",
      "Step no 573\n",
      "Loss 0.12555019557476044\n",
      "Step no 574\n",
      "Loss 0.12547364830970764\n",
      "Step no 575\n",
      "Loss 0.12539732456207275\n",
      "Step no 576\n",
      "Loss 0.1253211796283722\n",
      "Step no 577\n",
      "Loss 0.12524524331092834\n",
      "Step no 578\n",
      "Loss 0.1251695156097412\n",
      "Step no 579\n",
      "Loss 0.1250939965248108\n",
      "Step no 580\n",
      "Loss 0.1250186562538147\n",
      "Step no 581\n",
      "Loss 0.12494352459907532\n",
      "Step no 582\n",
      "Loss 0.12486859411001205\n",
      "Step no 583\n",
      "Loss 0.12479385733604431\n",
      "Step no 584\n",
      "Loss 0.12471933662891388\n",
      "Step no 585\n",
      "Loss 0.12464500963687897\n",
      "Step no 586\n",
      "Loss 0.12457086145877838\n",
      "Step no 587\n",
      "Loss 0.12449690699577332\n",
      "Step no 588\n",
      "Loss 0.12442316859960556\n",
      "Step no 589\n",
      "Loss 0.12434962391853333\n",
      "Step no 590\n",
      "Loss 0.12427625805139542\n",
      "Step no 591\n",
      "Loss 0.12420307099819183\n",
      "Step no 592\n",
      "Loss 0.12413010001182556\n",
      "Step no 593\n",
      "Loss 0.12405731528997421\n",
      "Step no 594\n",
      "Loss 0.12398470938205719\n",
      "Step no 595\n",
      "Loss 0.12391230463981628\n",
      "Step no 596\n",
      "Loss 0.1238400787115097\n",
      "Step no 597\n",
      "Loss 0.12376803159713745\n",
      "Step no 598\n",
      "Loss 0.12369618564844131\n",
      "Step no 599\n",
      "Loss 0.1236245185136795\n",
      "Step no 600\n",
      "Loss 0.12355304509401321\n",
      "Step no 601\n",
      "Loss 0.12348173558712006\n",
      "Step no 602\n",
      "Loss 0.12341062724590302\n",
      "Step no 603\n",
      "Loss 0.1233396977186203\n",
      "Step no 604\n",
      "Loss 0.12326894700527191\n",
      "Step no 605\n",
      "Loss 0.12319839000701904\n",
      "Step no 606\n",
      "Loss 0.12312798947095871\n",
      "Step no 607\n",
      "Loss 0.1230577751994133\n",
      "Step no 608\n",
      "Loss 0.12298774719238281\n",
      "Step no 609\n",
      "Loss 0.12291790544986725\n",
      "Step no 610\n",
      "Loss 0.12284823507070541\n",
      "Step no 611\n",
      "Loss 0.12277872860431671\n",
      "Step no 612\n",
      "Loss 0.12270939350128174\n",
      "Step no 613\n",
      "Loss 0.12264023721218109\n",
      "Step no 614\n",
      "Loss 0.12257128208875656\n",
      "Step no 615\n",
      "Loss 0.12250247597694397\n",
      "Step no 616\n",
      "Loss 0.12243384122848511\n",
      "Step no 617\n",
      "Loss 0.12236539274454117\n",
      "Step no 618\n",
      "Loss 0.12229710072278976\n",
      "Step no 619\n",
      "Loss 0.12222900241613388\n",
      "Step no 620\n",
      "Loss 0.12216103821992874\n",
      "Step no 621\n",
      "Loss 0.12209326028823853\n",
      "Step no 622\n",
      "Loss 0.12202565371990204\n",
      "Step no 623\n",
      "Loss 0.12195821851491928\n",
      "Step no 624\n",
      "Loss 0.12189094722270966\n",
      "Step no 625\n",
      "Loss 0.12182383239269257\n",
      "Step no 626\n",
      "Loss 0.1217568963766098\n",
      "Step no 627\n",
      "Loss 0.12169011682271957\n",
      "Step no 628\n",
      "Loss 0.12162350863218307\n",
      "Step no 629\n",
      "Loss 0.12155706435441971\n",
      "Step no 630\n",
      "Loss 0.12149078398942947\n",
      "Step no 631\n",
      "Loss 0.12142466008663177\n",
      "Step no 632\n",
      "Loss 0.12135870009660721\n",
      "Step no 633\n",
      "Loss 0.12129289656877518\n",
      "Step no 634\n",
      "Loss 0.12122726440429688\n",
      "Step no 635\n",
      "Loss 0.12116176635026932\n",
      "Step no 636\n",
      "Loss 0.12109646201133728\n",
      "Step no 637\n",
      "Loss 0.12103128433227539\n",
      "Step no 638\n",
      "Loss 0.12096628546714783\n",
      "Step no 639\n",
      "Loss 0.1209014505147934\n",
      "Step no 640\n",
      "Loss 0.1208367571234703\n",
      "Step no 641\n",
      "Loss 0.12077222019433975\n",
      "Step no 642\n",
      "Loss 0.12070783972740173\n",
      "Step no 643\n",
      "Loss 0.12064360827207565\n",
      "Step no 644\n",
      "Loss 0.1205795407295227\n",
      "Step no 645\n",
      "Loss 0.12051562964916229\n",
      "Step no 646\n",
      "Loss 0.12045186758041382\n",
      "Step no 647\n",
      "Loss 0.12038827687501907\n",
      "Step no 648\n",
      "Loss 0.12032480537891388\n",
      "Step no 649\n",
      "Loss 0.12026149779558182\n",
      "Step no 650\n",
      "Loss 0.1201983317732811\n",
      "Step no 651\n",
      "Loss 0.12013532966375351\n",
      "Step no 652\n",
      "Loss 0.12007248401641846\n",
      "Step no 653\n",
      "Loss 0.12000975757837296\n",
      "Step no 654\n",
      "Loss 0.11994720995426178\n",
      "Step no 655\n",
      "Loss 0.11988479644060135\n",
      "Step no 656\n",
      "Loss 0.11982253193855286\n",
      "Step no 657\n",
      "Loss 0.11976039409637451\n",
      "Step no 658\n",
      "Loss 0.1196984276175499\n",
      "Step no 659\n",
      "Loss 0.11963661015033722\n",
      "Step no 660\n",
      "Loss 0.11957491934299469\n",
      "Step no 661\n",
      "Loss 0.1195133775472641\n",
      "Step no 662\n",
      "Loss 0.11945198476314545\n",
      "Step no 663\n",
      "Loss 0.11939074099063873\n",
      "Step no 664\n",
      "Loss 0.11932961642742157\n",
      "Step no 665\n",
      "Loss 0.11926865577697754\n",
      "Step no 666\n",
      "Loss 0.11920783668756485\n",
      "Step no 667\n",
      "Loss 0.1191471517086029\n",
      "Step no 668\n",
      "Loss 0.1190866008400917\n",
      "Step no 669\n",
      "Loss 0.11902620643377304\n",
      "Step no 670\n",
      "Loss 0.11896592378616333\n",
      "Step no 671\n",
      "Loss 0.11890579760074615\n",
      "Step no 672\n",
      "Loss 0.11884580552577972\n",
      "Step no 673\n",
      "Loss 0.11878595501184464\n",
      "Step no 674\n",
      "Loss 0.11872624605894089\n",
      "Step no 675\n",
      "Loss 0.11866666376590729\n",
      "Step no 676\n",
      "Loss 0.11860722303390503\n",
      "Step no 677\n",
      "Loss 0.11854792386293411\n",
      "Step no 678\n",
      "Loss 0.11848874390125275\n",
      "Step no 679\n",
      "Loss 0.11842971295118332\n",
      "Step no 680\n",
      "Loss 0.11837082356214523\n",
      "Step no 681\n",
      "Loss 0.1183120459318161\n",
      "Step no 682\n",
      "Loss 0.11825340986251831\n",
      "Step no 683\n",
      "Loss 0.11819490790367126\n",
      "Step no 684\n",
      "Loss 0.11813654005527496\n",
      "Step no 685\n",
      "Loss 0.1180783063173294\n",
      "Step no 686\n",
      "Loss 0.118020199239254\n",
      "Step no 687\n",
      "Loss 0.11796222627162933\n",
      "Step no 688\n",
      "Loss 0.11790437996387482\n",
      "Step no 689\n",
      "Loss 0.11784666031599045\n",
      "Step no 690\n",
      "Loss 0.11778906732797623\n",
      "Step no 691\n",
      "Loss 0.11773160099983215\n",
      "Step no 692\n",
      "Loss 0.11767429113388062\n",
      "Step no 693\n",
      "Loss 0.11761707812547684\n",
      "Step no 694\n",
      "Loss 0.1175599992275238\n",
      "Step no 695\n",
      "Loss 0.11750305444002151\n",
      "Step no 696\n",
      "Loss 0.11744623631238937\n",
      "Step no 697\n",
      "Loss 0.11738953739404678\n",
      "Step no 698\n",
      "Loss 0.11733297258615494\n",
      "Step no 699\n",
      "Loss 0.11727651953697205\n",
      "Step no 700\n",
      "Loss 0.1172202005982399\n",
      "Step no 701\n",
      "Loss 0.1171639934182167\n",
      "Step no 702\n",
      "Loss 0.11710794270038605\n",
      "Step no 703\n",
      "Loss 0.11705199629068375\n",
      "Step no 704\n",
      "Loss 0.1169961541891098\n",
      "Step no 705\n",
      "Loss 0.1169404536485672\n",
      "Step no 706\n",
      "Loss 0.11688487231731415\n",
      "Step no 707\n",
      "Loss 0.11682941764593124\n",
      "Step no 708\n",
      "Loss 0.11677408218383789\n",
      "Step no 709\n",
      "Loss 0.1167188510298729\n",
      "Step no 710\n",
      "Loss 0.11666374653577805\n",
      "Step no 711\n",
      "Loss 0.11660877615213394\n",
      "Step no 712\n",
      "Loss 0.11655392497777939\n",
      "Step no 713\n",
      "Loss 0.11649917811155319\n",
      "Step no 714\n",
      "Loss 0.11644455045461655\n",
      "Step no 715\n",
      "Loss 0.11639004945755005\n",
      "Step no 716\n",
      "Loss 0.1163356602191925\n",
      "Step no 717\n",
      "Loss 0.11628139019012451\n",
      "Step no 718\n",
      "Loss 0.11622724682092667\n",
      "Step no 719\n",
      "Loss 0.11617320775985718\n",
      "Step no 720\n",
      "Loss 0.11611928045749664\n",
      "Step no 721\n",
      "Loss 0.11606547236442566\n",
      "Step no 722\n",
      "Loss 0.11601177603006363\n",
      "Step no 723\n",
      "Loss 0.11595820635557175\n",
      "Step no 724\n",
      "Loss 0.11590474098920822\n",
      "Step no 725\n",
      "Loss 0.11585140228271484\n",
      "Step no 726\n",
      "Loss 0.11579816788434982\n",
      "Step no 727\n",
      "Loss 0.11574503779411316\n",
      "Step no 728\n",
      "Loss 0.11569203436374664\n",
      "Step no 729\n",
      "Loss 0.11563914269208908\n",
      "Step no 730\n",
      "Loss 0.11558634787797928\n",
      "Step no 731\n",
      "Loss 0.11553369462490082\n",
      "Step no 732\n",
      "Loss 0.11548112332820892\n",
      "Step no 733\n",
      "Loss 0.11542867869138718\n",
      "Step no 734\n",
      "Loss 0.11537633836269379\n",
      "Step no 735\n",
      "Loss 0.11532410234212875\n",
      "Step no 736\n",
      "Loss 0.11527197062969208\n",
      "Step no 737\n",
      "Loss 0.11521996557712555\n",
      "Step no 738\n",
      "Loss 0.11516805738210678\n",
      "Step no 739\n",
      "Loss 0.11511626094579697\n",
      "Step no 740\n",
      "Loss 0.1150645986199379\n",
      "Step no 741\n",
      "Loss 0.1150130107998848\n",
      "Step no 742\n",
      "Loss 0.11496153473854065\n",
      "Step no 743\n",
      "Loss 0.11491017043590546\n",
      "Step no 744\n",
      "Loss 0.11485891044139862\n",
      "Step no 745\n",
      "Loss 0.11480776220560074\n",
      "Step no 746\n",
      "Loss 0.11475671827793121\n",
      "Step no 747\n",
      "Loss 0.11470577865839005\n",
      "Step no 748\n",
      "Loss 0.11465494334697723\n",
      "Step no 749\n",
      "Loss 0.11460421979427338\n",
      "Step no 750\n",
      "Loss 0.11455359309911728\n",
      "Step no 751\n",
      "Loss 0.11450307071208954\n",
      "Step no 752\n",
      "Loss 0.11445265263319016\n",
      "Step no 753\n",
      "Loss 0.11440234631299973\n",
      "Step no 754\n",
      "Loss 0.11435213685035706\n",
      "Step no 755\n",
      "Loss 0.11430202424526215\n",
      "Step no 756\n",
      "Loss 0.1142520159482956\n",
      "Step no 757\n",
      "Loss 0.114202119410038\n",
      "Step no 758\n",
      "Loss 0.11415231227874756\n",
      "Step no 759\n",
      "Loss 0.11410261690616608\n",
      "Step no 760\n",
      "Loss 0.11405301094055176\n",
      "Step no 761\n",
      "Loss 0.1140035018324852\n",
      "Step no 762\n",
      "Loss 0.113954097032547\n",
      "Step no 763\n",
      "Loss 0.11390481889247894\n",
      "Step no 764\n",
      "Loss 0.11385560780763626\n",
      "Step no 765\n",
      "Loss 0.11380651593208313\n",
      "Step no 766\n",
      "Loss 0.11375750601291656\n",
      "Step no 767\n",
      "Loss 0.11370861530303955\n",
      "Step no 768\n",
      "Loss 0.1136598140001297\n",
      "Step no 769\n",
      "Loss 0.11361110210418701\n",
      "Step no 770\n",
      "Loss 0.11356248706579208\n",
      "Step no 771\n",
      "Loss 0.11351397633552551\n",
      "Step no 772\n",
      "Loss 0.1134655624628067\n",
      "Step no 773\n",
      "Loss 0.11341723799705505\n",
      "Step no 774\n",
      "Loss 0.11336903274059296\n",
      "Step no 775\n",
      "Loss 0.11332091689109802\n",
      "Step no 776\n",
      "Loss 0.11327287554740906\n",
      "Step no 777\n",
      "Loss 0.11322492361068726\n",
      "Step no 778\n",
      "Loss 0.1131771057844162\n",
      "Step no 779\n",
      "Loss 0.11312935501337051\n",
      "Step no 780\n",
      "Loss 0.11308170855045319\n",
      "Step no 781\n",
      "Loss 0.11303415894508362\n",
      "Step no 782\n",
      "Loss 0.11298669129610062\n",
      "Step no 783\n",
      "Loss 0.11293932795524597\n",
      "Step no 784\n",
      "Loss 0.1128920465707779\n",
      "Step no 785\n",
      "Loss 0.11284486949443817\n",
      "Step no 786\n",
      "Loss 0.11279777437448502\n",
      "Step no 787\n",
      "Loss 0.11275077611207962\n",
      "Step no 788\n",
      "Loss 0.11270387470722198\n",
      "Step no 789\n",
      "Loss 0.11265706270933151\n",
      "Step no 790\n",
      "Loss 0.1126103401184082\n",
      "Step no 791\n",
      "Loss 0.11256371438503265\n",
      "Step no 792\n",
      "Loss 0.11251717805862427\n",
      "Step no 793\n",
      "Loss 0.11247072368860245\n",
      "Step no 794\n",
      "Loss 0.11242436617612839\n",
      "Step no 795\n",
      "Loss 0.1123780831694603\n",
      "Step no 796\n",
      "Loss 0.11233191192150116\n",
      "Step no 797\n",
      "Loss 0.11228581517934799\n",
      "Step no 798\n",
      "Loss 0.11223982274532318\n",
      "Step no 799\n",
      "Loss 0.11219391226768494\n",
      "Step no 800\n",
      "Loss 0.11214808374643326\n",
      "Step no 801\n",
      "Loss 0.11210235208272934\n",
      "Step no 802\n",
      "Loss 0.11205670982599258\n",
      "Step no 803\n",
      "Loss 0.1120111346244812\n",
      "Step no 804\n",
      "Loss 0.11196568608283997\n",
      "Step no 805\n",
      "Loss 0.1119202971458435\n",
      "Step no 806\n",
      "Loss 0.11187499761581421\n",
      "Step no 807\n",
      "Loss 0.11182978749275208\n",
      "Step no 808\n",
      "Loss 0.1117846667766571\n",
      "Step no 809\n",
      "Loss 0.1117396205663681\n",
      "Step no 810\n",
      "Loss 0.11169467121362686\n",
      "Step no 811\n",
      "Loss 0.11164981126785278\n",
      "Step no 812\n",
      "Loss 0.11160503327846527\n",
      "Step no 813\n",
      "Loss 0.11156032979488373\n",
      "Step no 814\n",
      "Loss 0.11151570826768875\n",
      "Step no 815\n",
      "Loss 0.11147119104862213\n",
      "Step no 816\n",
      "Loss 0.11142674088478088\n",
      "Step no 817\n",
      "Loss 0.1113823726773262\n",
      "Step no 818\n",
      "Loss 0.11133809387683868\n",
      "Step no 819\n",
      "Loss 0.11129391938447952\n",
      "Step no 820\n",
      "Loss 0.11124979704618454\n",
      "Step no 821\n",
      "Loss 0.11120577156543732\n",
      "Step no 822\n",
      "Loss 0.11116183549165726\n",
      "Step no 823\n",
      "Loss 0.11111797392368317\n",
      "Step no 824\n",
      "Loss 0.11107419431209564\n",
      "Step no 825\n",
      "Loss 0.1110304743051529\n",
      "Step no 826\n",
      "Loss 0.1109868660569191\n",
      "Step no 827\n",
      "Loss 0.11094333976507187\n",
      "Step no 828\n",
      "Loss 0.11089988797903061\n",
      "Step no 829\n",
      "Loss 0.11085651069879532\n",
      "Step no 830\n",
      "Loss 0.11081322282552719\n",
      "Step no 831\n",
      "Loss 0.11077001690864563\n",
      "Step no 832\n",
      "Loss 0.11072687804698944\n",
      "Step no 833\n",
      "Loss 0.11068382859230042\n",
      "Step no 834\n",
      "Loss 0.11064085364341736\n",
      "Step no 835\n",
      "Loss 0.11059796810150146\n",
      "Step no 836\n",
      "Loss 0.11055514961481094\n",
      "Step no 837\n",
      "Loss 0.11051242053508759\n",
      "Step no 838\n",
      "Loss 0.1104697585105896\n",
      "Step no 839\n",
      "Loss 0.11042718589305878\n",
      "Step no 840\n",
      "Loss 0.11038467288017273\n",
      "Step no 841\n",
      "Loss 0.11034226417541504\n",
      "Step no 842\n",
      "Loss 0.11029990762472153\n",
      "Step no 843\n",
      "Loss 0.11025765538215637\n",
      "Step no 844\n",
      "Loss 0.1102154478430748\n",
      "Step no 845\n",
      "Loss 0.11017334461212158\n",
      "Step no 846\n",
      "Loss 0.11013130098581314\n",
      "Step no 847\n",
      "Loss 0.11008935421705246\n",
      "Step no 848\n",
      "Loss 0.11004747450351715\n",
      "Step no 849\n",
      "Loss 0.11000564694404602\n",
      "Step no 850\n",
      "Loss 0.10996393114328384\n",
      "Step no 851\n",
      "Loss 0.10992226749658585\n",
      "Step no 852\n",
      "Loss 0.10988069325685501\n",
      "Step no 853\n",
      "Loss 0.10983918607234955\n",
      "Step no 854\n",
      "Loss 0.10979776084423065\n",
      "Step no 855\n",
      "Loss 0.10975640267133713\n",
      "Step no 856\n",
      "Loss 0.10971513390541077\n",
      "Step no 857\n",
      "Loss 0.10967390984296799\n",
      "Step no 858\n",
      "Loss 0.10963278263807297\n",
      "Step no 859\n",
      "Loss 0.10959172993898392\n",
      "Step no 860\n",
      "Loss 0.10955072939395905\n",
      "Step no 861\n",
      "Loss 0.10950984060764313\n",
      "Step no 862\n",
      "Loss 0.10946899652481079\n",
      "Step no 863\n",
      "Loss 0.10942822694778442\n",
      "Step no 864\n",
      "Loss 0.10938753932714462\n",
      "Step no 865\n",
      "Loss 0.10934692621231079\n",
      "Step no 866\n",
      "Loss 0.10930637270212173\n",
      "Step no 867\n",
      "Loss 0.10926589369773865\n",
      "Step no 868\n",
      "Loss 0.10922549664974213\n",
      "Step no 869\n",
      "Loss 0.10918515920639038\n",
      "Step no 870\n",
      "Loss 0.1091448962688446\n",
      "Step no 871\n",
      "Loss 0.1091047078371048\n",
      "Step no 872\n",
      "Loss 0.10906460136175156\n",
      "Step no 873\n",
      "Loss 0.1090245470404625\n",
      "Step no 874\n",
      "Loss 0.1089845672249794\n",
      "Step no 875\n",
      "Loss 0.10894465446472168\n",
      "Step no 876\n",
      "Loss 0.10890482366085052\n",
      "Step no 877\n",
      "Loss 0.10886506736278534\n",
      "Step no 878\n",
      "Loss 0.10882537066936493\n",
      "Step no 879\n",
      "Loss 0.10878574103116989\n",
      "Step no 880\n",
      "Loss 0.10874618589878082\n",
      "Step no 881\n",
      "Loss 0.10870669782161713\n",
      "Step no 882\n",
      "Loss 0.1086672693490982\n",
      "Step no 883\n",
      "Loss 0.10862793028354645\n",
      "Step no 884\n",
      "Loss 0.10858865082263947\n",
      "Step no 885\n",
      "Loss 0.10854943841695786\n",
      "Step no 886\n",
      "Loss 0.10851028561592102\n",
      "Step no 887\n",
      "Loss 0.10847120732069016\n",
      "Step no 888\n",
      "Loss 0.10843220353126526\n",
      "Step no 889\n",
      "Loss 0.10839325934648514\n",
      "Step no 890\n",
      "Loss 0.10835438966751099\n",
      "Step no 891\n",
      "Loss 0.10831557959318161\n",
      "Step no 892\n",
      "Loss 0.1082768440246582\n",
      "Step no 893\n",
      "Loss 0.10823816806077957\n",
      "Step no 894\n",
      "Loss 0.10819955915212631\n",
      "Step no 895\n",
      "Loss 0.10816101729869843\n",
      "Step no 896\n",
      "Loss 0.10812254250049591\n",
      "Step no 897\n",
      "Loss 0.10808414220809937\n",
      "Step no 898\n",
      "Loss 0.108045794069767\n",
      "Step no 899\n",
      "Loss 0.1080075278878212\n",
      "Step no 900\n",
      "Loss 0.10796931385993958\n",
      "Step no 901\n",
      "Loss 0.10793117433786392\n",
      "Step no 902\n",
      "Loss 0.10789309442043304\n",
      "Step no 903\n",
      "Loss 0.10785506665706635\n",
      "Step no 904\n",
      "Loss 0.10781712085008621\n",
      "Step no 905\n",
      "Loss 0.10777924209833145\n",
      "Step no 906\n",
      "Loss 0.10774141550064087\n",
      "Step no 907\n",
      "Loss 0.10770365595817566\n",
      "Step no 908\n",
      "Loss 0.10766597837209702\n",
      "Step no 909\n",
      "Loss 0.10762834548950195\n",
      "Step no 910\n",
      "Loss 0.10759077966213226\n",
      "Step no 911\n",
      "Loss 0.10755326598882675\n",
      "Step no 912\n",
      "Loss 0.1075158342719078\n",
      "Step no 913\n",
      "Loss 0.10747845470905304\n",
      "Step no 914\n",
      "Loss 0.10744115710258484\n",
      "Step no 915\n",
      "Loss 0.10740390419960022\n",
      "Step no 916\n",
      "Loss 0.10736670345067978\n",
      "Step no 917\n",
      "Loss 0.1073295921087265\n",
      "Step no 918\n",
      "Loss 0.1072925254702568\n",
      "Step no 919\n",
      "Loss 0.10725553333759308\n",
      "Step no 920\n",
      "Loss 0.10721859335899353\n",
      "Step no 921\n",
      "Loss 0.10718171298503876\n",
      "Step no 922\n",
      "Loss 0.10714489966630936\n",
      "Step no 923\n",
      "Loss 0.10710814595222473\n",
      "Step no 924\n",
      "Loss 0.10707146674394608\n",
      "Step no 925\n",
      "Loss 0.107034832239151\n",
      "Step no 926\n",
      "Loss 0.1069982498884201\n",
      "Step no 927\n",
      "Loss 0.10696173459291458\n",
      "Step no 928\n",
      "Loss 0.10692530870437622\n",
      "Step no 929\n",
      "Loss 0.10688891261816025\n",
      "Step no 930\n",
      "Loss 0.10685257613658905\n",
      "Step no 931\n",
      "Loss 0.10681632161140442\n",
      "Step no 932\n",
      "Loss 0.10678011178970337\n",
      "Step no 933\n",
      "Loss 0.10674396902322769\n",
      "Step no 934\n",
      "Loss 0.10670787841081619\n",
      "Step no 935\n",
      "Loss 0.10667184740304947\n",
      "Step no 936\n",
      "Loss 0.10663588345050812\n",
      "Step no 937\n",
      "Loss 0.10659997165203094\n",
      "Step no 938\n",
      "Loss 0.10656411200761795\n",
      "Step no 939\n",
      "Loss 0.10652831941843033\n",
      "Step no 940\n",
      "Loss 0.10649258643388748\n",
      "Step no 941\n",
      "Loss 0.10645690560340881\n",
      "Step no 942\n",
      "Loss 0.10642129927873611\n",
      "Step no 943\n",
      "Loss 0.1063857302069664\n",
      "Step no 944\n",
      "Loss 0.10635022819042206\n",
      "Step no 945\n",
      "Loss 0.10631479322910309\n",
      "Step no 946\n",
      "Loss 0.1062794104218483\n",
      "Step no 947\n",
      "Loss 0.10624407231807709\n",
      "Step no 948\n",
      "Loss 0.10620880872011185\n",
      "Step no 949\n",
      "Loss 0.10617359727621078\n",
      "Step no 950\n",
      "Loss 0.1061384528875351\n",
      "Step no 951\n",
      "Loss 0.1061033308506012\n",
      "Step no 952\n",
      "Loss 0.10606829822063446\n",
      "Step no 953\n",
      "Loss 0.1060333102941513\n",
      "Step no 954\n",
      "Loss 0.10599839687347412\n",
      "Step no 955\n",
      "Loss 0.10596351325511932\n",
      "Step no 956\n",
      "Loss 0.1059286892414093\n",
      "Step no 957\n",
      "Loss 0.10589393973350525\n",
      "Step no 958\n",
      "Loss 0.10585922002792358\n",
      "Step no 959\n",
      "Loss 0.10582458972930908\n",
      "Step no 960\n",
      "Loss 0.10578998178243637\n",
      "Step no 961\n",
      "Loss 0.10575544834136963\n",
      "Step no 962\n",
      "Loss 0.10572095215320587\n",
      "Step no 963\n",
      "Loss 0.10568653792142868\n",
      "Step no 964\n",
      "Loss 0.10565216094255447\n",
      "Step no 965\n",
      "Loss 0.10561784356832504\n",
      "Step no 966\n",
      "Loss 0.10558358579874039\n",
      "Step no 967\n",
      "Loss 0.10554936528205872\n",
      "Step no 968\n",
      "Loss 0.10551521182060242\n",
      "Step no 969\n",
      "Loss 0.10548112541437149\n",
      "Step no 970\n",
      "Loss 0.10544705390930176\n",
      "Step no 971\n",
      "Loss 0.10541307181119919\n",
      "Step no 972\n",
      "Loss 0.1053791493177414\n",
      "Step no 973\n",
      "Loss 0.10534526407718658\n",
      "Step no 974\n",
      "Loss 0.10531143099069595\n",
      "Step no 975\n",
      "Loss 0.1052776426076889\n",
      "Step no 976\n",
      "Loss 0.10524393618106842\n",
      "Step no 977\n",
      "Loss 0.10521025210618973\n",
      "Step no 978\n",
      "Loss 0.105176642537117\n",
      "Step no 979\n",
      "Loss 0.10514307022094727\n",
      "Step no 980\n",
      "Loss 0.1051095575094223\n",
      "Step no 981\n",
      "Loss 0.10507610440254211\n",
      "Step no 982\n",
      "Loss 0.10504268109798431\n",
      "Step no 983\n",
      "Loss 0.10500932484865189\n",
      "Step no 984\n",
      "Loss 0.10497602075338364\n",
      "Step no 985\n",
      "Loss 0.10494277626276016\n",
      "Step no 986\n",
      "Loss 0.10490958392620087\n",
      "Step no 987\n",
      "Loss 0.10487643629312515\n",
      "Step no 988\n",
      "Loss 0.10484334081411362\n",
      "Step no 989\n",
      "Loss 0.10481029003858566\n",
      "Step no 990\n",
      "Loss 0.10477729886770248\n",
      "Step no 991\n",
      "Loss 0.10474434494972229\n",
      "Step no 992\n",
      "Loss 0.10471147298812866\n",
      "Step no 993\n",
      "Loss 0.10467860847711563\n",
      "Step no 994\n",
      "Loss 0.10464582592248917\n",
      "Step no 995\n",
      "Loss 0.10461309552192688\n",
      "Step no 996\n",
      "Loss 0.10458040237426758\n",
      "Step no 997\n",
      "Loss 0.10454777628183365\n",
      "Step no 998\n",
      "Loss 0.1045151799917221\n",
      "Step no 999\n",
      "Loss 0.10448265820741653\n"
     ]
    }
   ],
   "source": [
    "logs = logistic1.train(X_train_scaled, y_train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c6c5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot training loss curve\n",
    "plt.plot(logs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06fbaa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9785)\n"
     ]
    }
   ],
   "source": [
    "train_preds = logistic1.forward(X_train_scaled)\n",
    "print(torch.mean((1*(train_preds > 0.5) == y_train)*1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4104374e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9825)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = logistic1.forward(X_test_scaled)\n",
    "torch.mean((1*(preds > 0.5) == y_test)*1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8164b9",
   "metadata": {},
   "source": [
    "# Logistic Regression with Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb79241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5bd89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c135b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use Torch autograd to make our task simpler, by removing the grad method and using loss.backward()\n",
    "\n",
    "class AutogradLR(nn.Module):\n",
    "    def __init__(self, p, lr = 0.01):\n",
    "        # initialise parameters as px1 column vector of zeros\n",
    "        super().__init__()\n",
    "        self.params = torch.nn.Parameter(torch.zeros(p, 1))\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data: n x p-1\n",
    "        n = data.shape[0]\n",
    "        X = torch.cat((data, torch.ones(n, 1)), 1)\n",
    "        return sigmoid(X @ self.params)\n",
    "    \n",
    "    def loss(self, data, true):\n",
    "        n = data.shape[0]\n",
    "        pred = self.forward(data)\n",
    "        # binary cross entropy loss\n",
    "        loss = 1/n*torch.sum(-(true* torch.log(pred) + (1-true)*torch.log(1-pred)))\n",
    "        return loss\n",
    "        \n",
    "    def step(self, data, true):\n",
    "        loss = self.loss(data, true)\n",
    "        loss.backward() # magically computes the gradients of the loss wrt each parameter\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # gradient descent step\n",
    "            self.params -= self.lr *self.params.grad\n",
    "            # clear gradients for next step\n",
    "            self.params.grad.zero_()\n",
    "            \n",
    "        return loss\n",
    "        \n",
    "    def train(self, data, true, n_steps):\n",
    "        logs = []\n",
    "        for i in range(n_steps):\n",
    "            print(f\"Step no {i}\")\n",
    "            loss = self.step(data, true)\n",
    "            print(f\"Loss {loss.item()}\")\n",
    "            logs.append(loss.item())\n",
    "            \n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27163d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic2 = AutogradLR(31, 0.01)\n",
    "logistic2.loss(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "760ec5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step no 0\n",
      "Loss 0.6931472420692444\n",
      "Step no 1\n",
      "Loss 0.6733601093292236\n",
      "Step no 2\n",
      "Loss 0.654815673828125\n",
      "Step no 3\n",
      "Loss 0.6374273300170898\n",
      "Step no 4\n",
      "Loss 0.6211110353469849\n",
      "Step no 5\n",
      "Loss 0.6057862639427185\n",
      "Step no 6\n",
      "Loss 0.5913771986961365\n",
      "Step no 7\n",
      "Loss 0.5778127908706665\n",
      "Step no 8\n",
      "Loss 0.5650271773338318\n",
      "Step no 9\n",
      "Loss 0.5529598593711853\n",
      "Step no 10\n",
      "Loss 0.5415550470352173\n",
      "Step no 11\n",
      "Loss 0.5307618379592896\n",
      "Step no 12\n",
      "Loss 0.5205340385437012\n",
      "Step no 13\n",
      "Loss 0.5108290910720825\n",
      "Step no 14\n",
      "Loss 0.5016086101531982\n",
      "Step no 15\n",
      "Loss 0.49283748865127563\n",
      "Step no 16\n",
      "Loss 0.4844835698604584\n",
      "Step no 17\n",
      "Loss 0.47651779651641846\n",
      "Step no 18\n",
      "Loss 0.4689134657382965\n",
      "Step no 19\n",
      "Loss 0.46164631843566895\n",
      "Step no 20\n",
      "Loss 0.4546939432621002\n",
      "Step no 21\n",
      "Loss 0.4480360746383667\n",
      "Step no 22\n",
      "Loss 0.44165390729904175\n",
      "Step no 23\n",
      "Loss 0.43553024530410767\n",
      "Step no 24\n",
      "Loss 0.42964932322502136\n",
      "Step no 25\n",
      "Loss 0.4239965081214905\n",
      "Step no 26\n",
      "Loss 0.4185584485530853\n",
      "Step no 27\n",
      "Loss 0.41332265734672546\n",
      "Step no 28\n",
      "Loss 0.4082776606082916\n",
      "Step no 29\n",
      "Loss 0.4034128487110138\n",
      "Step no 30\n",
      "Loss 0.39871832728385925\n",
      "Step no 31\n",
      "Loss 0.39418506622314453\n",
      "Step no 32\n",
      "Loss 0.3898043632507324\n",
      "Step no 33\n",
      "Loss 0.38556843996047974\n",
      "Step no 34\n",
      "Loss 0.38146984577178955\n",
      "Step no 35\n",
      "Loss 0.3775016963481903\n",
      "Step no 36\n",
      "Loss 0.37365755438804626\n",
      "Step no 37\n",
      "Loss 0.36993148922920227\n",
      "Step no 38\n",
      "Loss 0.3663177192211151\n",
      "Step no 39\n",
      "Loss 0.3628110885620117\n",
      "Step no 40\n",
      "Loss 0.35940659046173096\n",
      "Step no 41\n",
      "Loss 0.35609957575798035\n",
      "Step no 42\n",
      "Loss 0.3528856933116913\n",
      "Step no 43\n",
      "Loss 0.34976089000701904\n",
      "Step no 44\n",
      "Loss 0.34672120213508606\n",
      "Step no 45\n",
      "Loss 0.3437630534172058\n",
      "Step no 46\n",
      "Loss 0.34088295698165894\n",
      "Step no 47\n",
      "Loss 0.3380776643753052\n",
      "Step no 48\n",
      "Loss 0.335344135761261\n",
      "Step no 49\n",
      "Loss 0.33267951011657715\n",
      "Step no 50\n",
      "Loss 0.3300809860229492\n",
      "Step no 51\n",
      "Loss 0.32754600048065186\n",
      "Step no 52\n",
      "Loss 0.3250720500946045\n",
      "Step no 53\n",
      "Loss 0.32265686988830566\n",
      "Step no 54\n",
      "Loss 0.3202981650829315\n",
      "Step no 55\n",
      "Loss 0.3179938793182373\n",
      "Step no 56\n",
      "Loss 0.31574204564094543\n",
      "Step no 57\n",
      "Loss 0.3135406970977783\n",
      "Step no 58\n",
      "Loss 0.3113880157470703\n",
      "Step no 59\n",
      "Loss 0.3092823922634125\n",
      "Step no 60\n",
      "Loss 0.30722206830978394\n",
      "Step no 61\n",
      "Loss 0.3052055835723877\n",
      "Step no 62\n",
      "Loss 0.3032313585281372\n",
      "Step no 63\n",
      "Loss 0.30129796266555786\n",
      "Step no 64\n",
      "Loss 0.2994041442871094\n",
      "Step no 65\n",
      "Loss 0.2975485324859619\n",
      "Step no 66\n",
      "Loss 0.2957299053668976\n",
      "Step no 67\n",
      "Loss 0.29394710063934326\n",
      "Step no 68\n",
      "Loss 0.29219892621040344\n",
      "Step no 69\n",
      "Loss 0.29048433899879456\n",
      "Step no 70\n",
      "Loss 0.28880223631858826\n",
      "Step no 71\n",
      "Loss 0.2871517539024353\n",
      "Step no 72\n",
      "Loss 0.28553181886672974\n",
      "Step no 73\n",
      "Loss 0.2839415669441223\n",
      "Step no 74\n",
      "Loss 0.2823801338672638\n",
      "Step no 75\n",
      "Loss 0.28084656596183777\n",
      "Step no 76\n",
      "Loss 0.2793402671813965\n",
      "Step no 77\n",
      "Loss 0.27786019444465637\n",
      "Step no 78\n",
      "Loss 0.27640581130981445\n",
      "Step no 79\n",
      "Loss 0.27497631311416626\n",
      "Step no 80\n",
      "Loss 0.27357104420661926\n",
      "Step no 81\n",
      "Loss 0.27218928933143616\n",
      "Step no 82\n",
      "Loss 0.2708304524421692\n",
      "Step no 83\n",
      "Loss 0.26949384808540344\n",
      "Step no 84\n",
      "Loss 0.2681789696216583\n",
      "Step no 85\n",
      "Loss 0.2668852210044861\n",
      "Step no 86\n",
      "Loss 0.26561200618743896\n",
      "Step no 87\n",
      "Loss 0.26435887813568115\n",
      "Step no 88\n",
      "Loss 0.2631252110004425\n",
      "Step no 89\n",
      "Loss 0.261910617351532\n",
      "Step no 90\n",
      "Loss 0.2607145309448242\n",
      "Step no 91\n",
      "Loss 0.2595365345478058\n",
      "Step no 92\n",
      "Loss 0.25837618112564087\n",
      "Step no 93\n",
      "Loss 0.2572330832481384\n",
      "Step no 94\n",
      "Loss 0.2561066746711731\n",
      "Step no 95\n",
      "Loss 0.25499674677848816\n",
      "Step no 96\n",
      "Loss 0.253902792930603\n",
      "Step no 97\n",
      "Loss 0.25282445549964905\n",
      "Step no 98\n",
      "Loss 0.25176137685775757\n",
      "Step no 99\n",
      "Loss 0.2507132291793823\n",
      "Step no 100\n",
      "Loss 0.24967961013317108\n",
      "Step no 101\n",
      "Loss 0.24866025149822235\n",
      "Step no 102\n",
      "Loss 0.24765479564666748\n",
      "Step no 103\n",
      "Loss 0.2466629296541214\n",
      "Step no 104\n",
      "Loss 0.24568438529968262\n",
      "Step no 105\n",
      "Loss 0.24471883475780487\n",
      "Step no 106\n",
      "Loss 0.24376603960990906\n",
      "Step no 107\n",
      "Loss 0.24282565712928772\n",
      "Step no 108\n",
      "Loss 0.24189749360084534\n",
      "Step no 109\n",
      "Loss 0.24098126590251923\n",
      "Step no 110\n",
      "Loss 0.2400766909122467\n",
      "Step no 111\n",
      "Loss 0.23918354511260986\n",
      "Step no 112\n",
      "Loss 0.2383016049861908\n",
      "Step no 113\n",
      "Loss 0.2374306470155716\n",
      "Step no 114\n",
      "Loss 0.23657044768333435\n",
      "Step no 115\n",
      "Loss 0.23572072386741638\n",
      "Step no 116\n",
      "Loss 0.23488134145736694\n",
      "Step no 117\n",
      "Loss 0.23405207693576813\n",
      "Step no 118\n",
      "Loss 0.23323270678520203\n",
      "Step no 119\n",
      "Loss 0.2324230670928955\n",
      "Step no 120\n",
      "Loss 0.23162294924259186\n",
      "Step no 121\n",
      "Loss 0.23083218932151794\n",
      "Step no 122\n",
      "Loss 0.23005059361457825\n",
      "Step no 123\n",
      "Loss 0.22927798330783844\n",
      "Step no 124\n",
      "Loss 0.22851420938968658\n",
      "Step no 125\n",
      "Loss 0.22775906324386597\n",
      "Step no 126\n",
      "Loss 0.22701245546340942\n",
      "Step no 127\n",
      "Loss 0.22627416253089905\n",
      "Step no 128\n",
      "Loss 0.2255440652370453\n",
      "Step no 129\n",
      "Loss 0.22482198476791382\n",
      "Step no 130\n",
      "Loss 0.22410786151885986\n",
      "Step no 131\n",
      "Loss 0.22340142726898193\n",
      "Step no 132\n",
      "Loss 0.22270265221595764\n",
      "Step no 133\n",
      "Loss 0.22201134264469147\n",
      "Step no 134\n",
      "Loss 0.22132739424705505\n",
      "Step no 135\n",
      "Loss 0.22065065801143646\n",
      "Step no 136\n",
      "Loss 0.21998101472854614\n",
      "Step no 137\n",
      "Loss 0.21931836009025574\n",
      "Step no 138\n",
      "Loss 0.2186625599861145\n",
      "Step no 139\n",
      "Loss 0.21801349520683289\n",
      "Step no 140\n",
      "Loss 0.21737107634544373\n",
      "Step no 141\n",
      "Loss 0.21673515439033508\n",
      "Step no 142\n",
      "Loss 0.2161056399345398\n",
      "Step no 143\n",
      "Loss 0.2154824435710907\n",
      "Step no 144\n",
      "Loss 0.21486544609069824\n",
      "Step no 145\n",
      "Loss 0.21425452828407288\n",
      "Step no 146\n",
      "Loss 0.2136496603488922\n",
      "Step no 147\n",
      "Loss 0.21305067837238312\n",
      "Step no 148\n",
      "Loss 0.21245749294757843\n",
      "Step no 149\n",
      "Loss 0.21187004446983337\n",
      "Step no 150\n",
      "Loss 0.2112881988286972\n",
      "Step no 151\n",
      "Loss 0.21071192622184753\n",
      "Step no 152\n",
      "Loss 0.210141122341156\n",
      "Step no 153\n",
      "Loss 0.20957565307617188\n",
      "Step no 154\n",
      "Loss 0.20901547372341156\n",
      "Step no 155\n",
      "Loss 0.2084605097770691\n",
      "Step no 156\n",
      "Loss 0.2079106569290161\n",
      "Step no 157\n",
      "Loss 0.20736590027809143\n",
      "Step no 158\n",
      "Loss 0.2068260908126831\n",
      "Step no 159\n",
      "Loss 0.20629119873046875\n",
      "Step no 160\n",
      "Loss 0.2057611048221588\n",
      "Step no 161\n",
      "Loss 0.2052358090877533\n",
      "Step no 162\n",
      "Loss 0.20471519231796265\n",
      "Step no 163\n",
      "Loss 0.2041991800069809\n",
      "Step no 164\n",
      "Loss 0.20368772745132446\n",
      "Step no 165\n",
      "Loss 0.20318076014518738\n",
      "Step no 166\n",
      "Loss 0.20267824828624725\n",
      "Step no 167\n",
      "Loss 0.20218008756637573\n",
      "Step no 168\n",
      "Loss 0.20168620347976685\n",
      "Step no 169\n",
      "Loss 0.2011965811252594\n",
      "Step no 170\n",
      "Loss 0.20071113109588623\n",
      "Step no 171\n",
      "Loss 0.20022979378700256\n",
      "Step no 172\n",
      "Loss 0.199752539396286\n",
      "Step no 173\n",
      "Loss 0.1992792785167694\n",
      "Step no 174\n",
      "Loss 0.19880998134613037\n",
      "Step no 175\n",
      "Loss 0.19834460318088531\n",
      "Step no 176\n",
      "Loss 0.19788308441638947\n",
      "Step no 177\n",
      "Loss 0.19742533564567566\n",
      "Step no 178\n",
      "Loss 0.19697131216526031\n",
      "Step no 179\n",
      "Loss 0.19652104377746582\n",
      "Step no 180\n",
      "Loss 0.19607439637184143\n",
      "Step no 181\n",
      "Loss 0.19563134014606476\n",
      "Step no 182\n",
      "Loss 0.19519184529781342\n",
      "Step no 183\n",
      "Loss 0.19475585222244263\n",
      "Step no 184\n",
      "Loss 0.19432333111763\n",
      "Step no 185\n",
      "Loss 0.19389422237873077\n",
      "Step no 186\n",
      "Loss 0.19346848130226135\n",
      "Step no 187\n",
      "Loss 0.19304607808589935\n",
      "Step no 188\n",
      "Loss 0.192626953125\n",
      "Step no 189\n",
      "Loss 0.1922110915184021\n",
      "Step no 190\n",
      "Loss 0.19179841876029968\n",
      "Step no 191\n",
      "Loss 0.19138894975185394\n",
      "Step no 192\n",
      "Loss 0.19098256528377533\n",
      "Step no 193\n",
      "Loss 0.19057928025722504\n",
      "Step no 194\n",
      "Loss 0.1901790350675583\n",
      "Step no 195\n",
      "Loss 0.18978184461593628\n",
      "Step no 196\n",
      "Loss 0.18938758969306946\n",
      "Step no 197\n",
      "Loss 0.18899627029895782\n",
      "Step no 198\n",
      "Loss 0.18860787153244019\n",
      "Step no 199\n",
      "Loss 0.18822236359119415\n",
      "Step no 200\n",
      "Loss 0.18783968687057495\n",
      "Step no 201\n",
      "Loss 0.1874598115682602\n",
      "Step no 202\n",
      "Loss 0.1870826780796051\n",
      "Step no 203\n",
      "Loss 0.18670830130577087\n",
      "Step no 204\n",
      "Loss 0.18633662164211273\n",
      "Step no 205\n",
      "Loss 0.1859676092863083\n",
      "Step no 206\n",
      "Loss 0.18560126423835754\n",
      "Step no 207\n",
      "Loss 0.18523749709129333\n",
      "Step no 208\n",
      "Loss 0.18487633764743805\n",
      "Step no 209\n",
      "Loss 0.18451771140098572\n",
      "Step no 210\n",
      "Loss 0.18416161835193634\n",
      "Step no 211\n",
      "Loss 0.18380801379680634\n",
      "Step no 212\n",
      "Loss 0.18345686793327332\n",
      "Step no 213\n",
      "Loss 0.1831081658601761\n",
      "Step no 214\n",
      "Loss 0.18276187777519226\n",
      "Step no 215\n",
      "Loss 0.18241798877716064\n",
      "Step no 216\n",
      "Loss 0.18207645416259766\n",
      "Step no 217\n",
      "Loss 0.18173722922801971\n",
      "Step no 218\n",
      "Loss 0.181400328874588\n",
      "Step no 219\n",
      "Loss 0.18106567859649658\n",
      "Step no 220\n",
      "Loss 0.1807333081960678\n",
      "Step no 221\n",
      "Loss 0.18040315806865692\n",
      "Step no 222\n",
      "Loss 0.18007522821426392\n",
      "Step no 223\n",
      "Loss 0.1797494888305664\n",
      "Step no 224\n",
      "Loss 0.17942588031291962\n",
      "Step no 225\n",
      "Loss 0.17910441756248474\n",
      "Step no 226\n",
      "Loss 0.1787850707769394\n",
      "Step no 227\n",
      "Loss 0.17846783995628357\n",
      "Step no 228\n",
      "Loss 0.1781526654958725\n",
      "Step no 229\n",
      "Loss 0.1778395175933838\n",
      "Step no 230\n",
      "Loss 0.17752841114997864\n",
      "Step no 231\n",
      "Loss 0.17721930146217346\n",
      "Step no 232\n",
      "Loss 0.17691218852996826\n",
      "Step no 233\n",
      "Loss 0.17660702764987946\n",
      "Step no 234\n",
      "Loss 0.17630383372306824\n",
      "Step no 235\n",
      "Loss 0.17600254714488983\n",
      "Step no 236\n",
      "Loss 0.17570313811302185\n",
      "Step no 237\n",
      "Loss 0.17540566623210907\n",
      "Step no 238\n",
      "Loss 0.17511002719402313\n",
      "Step no 239\n",
      "Loss 0.17481623589992523\n",
      "Step no 240\n",
      "Loss 0.17452429234981537\n",
      "Step no 241\n",
      "Loss 0.17423415184020996\n",
      "Step no 242\n",
      "Loss 0.17394578456878662\n",
      "Step no 243\n",
      "Loss 0.17365919053554535\n",
      "Step no 244\n",
      "Loss 0.17337438464164734\n",
      "Step no 245\n",
      "Loss 0.17309130728244781\n",
      "Step no 246\n",
      "Loss 0.1728099286556244\n",
      "Step no 247\n",
      "Loss 0.17253027856349945\n",
      "Step no 248\n",
      "Loss 0.17225231230258942\n",
      "Step no 249\n",
      "Loss 0.1719760149717331\n",
      "Step no 250\n",
      "Loss 0.1717013716697693\n",
      "Step no 251\n",
      "Loss 0.1714283674955368\n",
      "Step no 252\n",
      "Loss 0.17115698754787445\n",
      "Step no 253\n",
      "Loss 0.17088721692562103\n",
      "Step no 254\n",
      "Loss 0.17061905562877655\n",
      "Step no 255\n",
      "Loss 0.17035244405269623\n",
      "Step no 256\n",
      "Loss 0.17008744180202484\n",
      "Step no 257\n",
      "Loss 0.16982394456863403\n",
      "Step no 258\n",
      "Loss 0.16956199705600739\n",
      "Step no 259\n",
      "Loss 0.1693015694618225\n",
      "Step no 260\n",
      "Loss 0.16904261708259583\n",
      "Step no 261\n",
      "Loss 0.1687851995229721\n",
      "Step no 262\n",
      "Loss 0.16852925717830658\n",
      "Step no 263\n",
      "Loss 0.16827477514743805\n",
      "Step no 264\n",
      "Loss 0.16802172362804413\n",
      "Step no 265\n",
      "Loss 0.167770117521286\n",
      "Step no 266\n",
      "Loss 0.1675199419260025\n",
      "Step no 267\n",
      "Loss 0.16727116703987122\n",
      "Step no 268\n",
      "Loss 0.16702380776405334\n",
      "Step no 269\n",
      "Loss 0.1667778193950653\n",
      "Step no 270\n",
      "Loss 0.1665332019329071\n",
      "Step no 271\n",
      "Loss 0.16628997027873993\n",
      "Step no 272\n",
      "Loss 0.166048064827919\n",
      "Step no 273\n",
      "Loss 0.16580748558044434\n",
      "Step no 274\n",
      "Loss 0.1655682474374771\n",
      "Step no 275\n",
      "Loss 0.16533033549785614\n",
      "Step no 276\n",
      "Loss 0.16509371995925903\n",
      "Step no 277\n",
      "Loss 0.1648583859205246\n",
      "Step no 278\n",
      "Loss 0.16462433338165283\n",
      "Step no 279\n",
      "Loss 0.16439154744148254\n",
      "Step no 280\n",
      "Loss 0.16415999829769135\n",
      "Step no 281\n",
      "Loss 0.16392973065376282\n",
      "Step no 282\n",
      "Loss 0.163700670003891\n",
      "Step no 283\n",
      "Loss 0.16347286105155945\n",
      "Step no 284\n",
      "Loss 0.1632462441921234\n",
      "Step no 285\n",
      "Loss 0.16302081942558289\n",
      "Step no 286\n",
      "Loss 0.16279660165309906\n",
      "Step no 287\n",
      "Loss 0.16257359087467194\n",
      "Step no 288\n",
      "Loss 0.16235172748565674\n",
      "Step no 289\n",
      "Loss 0.16213102638721466\n",
      "Step no 290\n",
      "Loss 0.1619114875793457\n",
      "Step no 291\n",
      "Loss 0.16169308125972748\n",
      "Step no 292\n",
      "Loss 0.16147582232952118\n",
      "Step no 293\n",
      "Loss 0.16125966608524323\n",
      "Step no 294\n",
      "Loss 0.1610446572303772\n",
      "Step no 295\n",
      "Loss 0.16083070635795593\n",
      "Step no 296\n",
      "Loss 0.1606178730726242\n",
      "Step no 297\n",
      "Loss 0.1604061722755432\n",
      "Step no 298\n",
      "Loss 0.1601954996585846\n",
      "Step no 299\n",
      "Loss 0.15998591482639313\n",
      "Step no 300\n",
      "Loss 0.15977737307548523\n",
      "Step no 301\n",
      "Loss 0.15956991910934448\n",
      "Step no 302\n",
      "Loss 0.15936346352100372\n",
      "Step no 303\n",
      "Loss 0.15915809571743011\n",
      "Step no 304\n",
      "Loss 0.1589537113904953\n",
      "Step no 305\n",
      "Loss 0.15875035524368286\n",
      "Step no 306\n",
      "Loss 0.1585480272769928\n",
      "Step no 307\n",
      "Loss 0.15834668278694153\n",
      "Step no 308\n",
      "Loss 0.15814632177352905\n",
      "Step no 309\n",
      "Loss 0.15794697403907776\n",
      "Step no 310\n",
      "Loss 0.15774860978126526\n",
      "Step no 311\n",
      "Loss 0.15755119919776917\n",
      "Step no 312\n",
      "Loss 0.15735475718975067\n",
      "Step no 313\n",
      "Loss 0.15715926885604858\n",
      "Step no 314\n",
      "Loss 0.1569647192955017\n",
      "Step no 315\n",
      "Loss 0.15677112340927124\n",
      "Step no 316\n",
      "Loss 0.15657846629619598\n",
      "Step no 317\n",
      "Loss 0.15638671815395355\n",
      "Step no 318\n",
      "Loss 0.15619590878486633\n",
      "Step no 319\n",
      "Loss 0.15600597858428955\n",
      "Step no 320\n",
      "Loss 0.15581698715686798\n",
      "Step no 321\n",
      "Loss 0.15562888979911804\n",
      "Step no 322\n",
      "Loss 0.15544165670871735\n",
      "Step no 323\n",
      "Loss 0.15525531768798828\n",
      "Step no 324\n",
      "Loss 0.15506987273693085\n",
      "Step no 325\n",
      "Loss 0.15488529205322266\n",
      "Step no 326\n",
      "Loss 0.15470156073570251\n",
      "Step no 327\n",
      "Loss 0.1545187085866928\n",
      "Step no 328\n",
      "Loss 0.15433669090270996\n",
      "Step no 329\n",
      "Loss 0.15415552258491516\n",
      "Step no 330\n",
      "Loss 0.1539752036333084\n",
      "Step no 331\n",
      "Loss 0.15379571914672852\n",
      "Step no 332\n",
      "Loss 0.1536170393228531\n",
      "Step no 333\n",
      "Loss 0.15343919396400452\n",
      "Step no 334\n",
      "Loss 0.1532621532678604\n",
      "Step no 335\n",
      "Loss 0.15308593213558197\n",
      "Step no 336\n",
      "Loss 0.1529105007648468\n",
      "Step no 337\n",
      "Loss 0.1527358889579773\n",
      "Step no 338\n",
      "Loss 0.15256203711032867\n",
      "Step no 339\n",
      "Loss 0.15238898992538452\n",
      "Step no 340\n",
      "Loss 0.15221673250198364\n",
      "Step no 341\n",
      "Loss 0.15204520523548126\n",
      "Step no 342\n",
      "Loss 0.15187448263168335\n",
      "Step no 343\n",
      "Loss 0.15170449018478394\n",
      "Step no 344\n",
      "Loss 0.1515352725982666\n",
      "Step no 345\n",
      "Loss 0.15136681497097015\n",
      "Step no 346\n",
      "Loss 0.151199072599411\n",
      "Step no 347\n",
      "Loss 0.15103210508823395\n",
      "Step no 348\n",
      "Loss 0.15086586773395538\n",
      "Step no 349\n",
      "Loss 0.15070034563541412\n",
      "Step no 350\n",
      "Loss 0.15053555369377136\n",
      "Step no 351\n",
      "Loss 0.1503715068101883\n",
      "Step no 352\n",
      "Loss 0.15020813047885895\n",
      "Step no 353\n",
      "Loss 0.1500454843044281\n",
      "Step no 354\n",
      "Loss 0.14988353848457336\n",
      "Step no 355\n",
      "Loss 0.14972229301929474\n",
      "Step no 356\n",
      "Loss 0.14956174790859222\n",
      "Step no 357\n",
      "Loss 0.14940188825130463\n",
      "Step no 358\n",
      "Loss 0.14924269914627075\n",
      "Step no 359\n",
      "Loss 0.149084210395813\n",
      "Step no 360\n",
      "Loss 0.14892637729644775\n",
      "Step no 361\n",
      "Loss 0.14876924455165863\n",
      "Step no 362\n",
      "Loss 0.14861273765563965\n",
      "Step no 363\n",
      "Loss 0.14845693111419678\n",
      "Step no 364\n",
      "Loss 0.14830178022384644\n",
      "Step no 365\n",
      "Loss 0.14814725518226624\n",
      "Step no 366\n",
      "Loss 0.14799340069293976\n",
      "Step no 367\n",
      "Loss 0.14784017205238342\n",
      "Step no 368\n",
      "Loss 0.14768759906291962\n",
      "Step no 369\n",
      "Loss 0.14753563702106476\n",
      "Step no 370\n",
      "Loss 0.14738433063030243\n",
      "Step no 371\n",
      "Loss 0.14723365008831024\n",
      "Step no 372\n",
      "Loss 0.147083580493927\n",
      "Step no 373\n",
      "Loss 0.1469341367483139\n",
      "Step no 374\n",
      "Loss 0.14678530395030975\n",
      "Step no 375\n",
      "Loss 0.14663708209991455\n",
      "Step no 376\n",
      "Loss 0.1464894711971283\n",
      "Step no 377\n",
      "Loss 0.1463424414396286\n",
      "Step no 378\n",
      "Loss 0.14619602262973785\n",
      "Step no 379\n",
      "Loss 0.14605019986629486\n",
      "Step no 380\n",
      "Loss 0.14590497314929962\n",
      "Step no 381\n",
      "Loss 0.14576032757759094\n",
      "Step no 382\n",
      "Loss 0.14561624825000763\n",
      "Step no 383\n",
      "Loss 0.14547276496887207\n",
      "Step no 384\n",
      "Loss 0.14532984793186188\n",
      "Step no 385\n",
      "Loss 0.14518751204013824\n",
      "Step no 386\n",
      "Loss 0.14504572749137878\n",
      "Step no 387\n",
      "Loss 0.14490452408790588\n",
      "Step no 388\n",
      "Loss 0.14476387202739716\n",
      "Step no 389\n",
      "Loss 0.1446237862110138\n",
      "Step no 390\n",
      "Loss 0.1444842368364334\n",
      "Step no 391\n",
      "Loss 0.1443452537059784\n",
      "Step no 392\n",
      "Loss 0.14420680701732635\n",
      "Step no 393\n",
      "Loss 0.1440688967704773\n",
      "Step no 394\n",
      "Loss 0.1439315378665924\n",
      "Step no 395\n",
      "Loss 0.1437947154045105\n",
      "Step no 396\n",
      "Loss 0.14365839958190918\n",
      "Step no 397\n",
      "Loss 0.14352263510227203\n",
      "Step no 398\n",
      "Loss 0.14338739216327667\n",
      "Step no 399\n",
      "Loss 0.1432526707649231\n",
      "Step no 400\n",
      "Loss 0.1431184709072113\n",
      "Step no 401\n",
      "Loss 0.1429847776889801\n",
      "Step no 402\n",
      "Loss 0.14285160601139069\n",
      "Step no 403\n",
      "Loss 0.14271895587444305\n",
      "Step no 404\n",
      "Loss 0.14258679747581482\n",
      "Step no 405\n",
      "Loss 0.1424551159143448\n",
      "Step no 406\n",
      "Loss 0.14232397079467773\n",
      "Step no 407\n",
      "Loss 0.14219330251216888\n",
      "Step no 408\n",
      "Loss 0.14206315577030182\n",
      "Step no 409\n",
      "Loss 0.14193347096443176\n",
      "Step no 410\n",
      "Loss 0.1418042778968811\n",
      "Step no 411\n",
      "Loss 0.14167557656764984\n",
      "Step no 412\n",
      "Loss 0.14154735207557678\n",
      "Step no 413\n",
      "Loss 0.14141960442066193\n",
      "Step no 414\n",
      "Loss 0.14129233360290527\n",
      "Step no 415\n",
      "Loss 0.14116555452346802\n",
      "Step no 416\n",
      "Loss 0.14103922247886658\n",
      "Step no 417\n",
      "Loss 0.14091336727142334\n",
      "Step no 418\n",
      "Loss 0.14078795909881592\n",
      "Step no 419\n",
      "Loss 0.1406630426645279\n",
      "Step no 420\n",
      "Loss 0.14053857326507568\n",
      "Step no 421\n",
      "Loss 0.1404145509004593\n",
      "Step no 422\n",
      "Loss 0.1402909755706787\n",
      "Step no 423\n",
      "Loss 0.14016787707805634\n",
      "Step no 424\n",
      "Loss 0.14004521071910858\n",
      "Step no 425\n",
      "Loss 0.13992299139499664\n",
      "Step no 426\n",
      "Loss 0.1398012340068817\n",
      "Step no 427\n",
      "Loss 0.13967987895011902\n",
      "Step no 428\n",
      "Loss 0.13955898582935333\n",
      "Step no 429\n",
      "Loss 0.13943853974342346\n",
      "Step no 430\n",
      "Loss 0.13931849598884583\n",
      "Step no 431\n",
      "Loss 0.1391988843679428\n",
      "Step no 432\n",
      "Loss 0.1390797346830368\n",
      "Step no 433\n",
      "Loss 0.13896097242832184\n",
      "Step no 434\n",
      "Loss 0.1388426423072815\n",
      "Step no 435\n",
      "Loss 0.13872474431991577\n",
      "Step no 436\n",
      "Loss 0.13860724866390228\n",
      "Step no 437\n",
      "Loss 0.13849018514156342\n",
      "Step no 438\n",
      "Loss 0.1383735090494156\n",
      "Step no 439\n",
      "Loss 0.13825726509094238\n",
      "Step no 440\n",
      "Loss 0.1381414234638214\n",
      "Step no 441\n",
      "Loss 0.13802598416805267\n",
      "Step no 442\n",
      "Loss 0.13791096210479736\n",
      "Step no 443\n",
      "Loss 0.1377963274717331\n",
      "Step no 444\n",
      "Loss 0.13768208026885986\n",
      "Step no 445\n",
      "Loss 0.13756826519966125\n",
      "Step no 446\n",
      "Loss 0.1374548077583313\n",
      "Step no 447\n",
      "Loss 0.13734176754951477\n",
      "Step no 448\n",
      "Loss 0.1372290998697281\n",
      "Step no 449\n",
      "Loss 0.13711681962013245\n",
      "Step no 450\n",
      "Loss 0.13700494170188904\n",
      "Step no 451\n",
      "Loss 0.13689343631267548\n",
      "Step no 452\n",
      "Loss 0.13678231835365295\n",
      "Step no 453\n",
      "Loss 0.13667157292366028\n",
      "Step no 454\n",
      "Loss 0.13656120002269745\n",
      "Step no 455\n",
      "Loss 0.13645121455192566\n",
      "Step no 456\n",
      "Loss 0.13634160161018372\n",
      "Step no 457\n",
      "Loss 0.13623234629631042\n",
      "Step no 458\n",
      "Loss 0.13612347841262817\n",
      "Step no 459\n",
      "Loss 0.13601495325565338\n",
      "Step no 460\n",
      "Loss 0.13590680062770844\n",
      "Step no 461\n",
      "Loss 0.13579902052879333\n",
      "Step no 462\n",
      "Loss 0.1356915980577469\n",
      "Step no 463\n",
      "Loss 0.1355845332145691\n",
      "Step no 464\n",
      "Loss 0.13547782599925995\n",
      "Step no 465\n",
      "Loss 0.13537147641181946\n",
      "Step no 466\n",
      "Loss 0.13526546955108643\n",
      "Step no 467\n",
      "Loss 0.13515980541706085\n",
      "Step no 468\n",
      "Loss 0.13505448400974274\n",
      "Step no 469\n",
      "Loss 0.13494956493377686\n",
      "Step no 470\n",
      "Loss 0.13484494388103485\n",
      "Step no 471\n",
      "Loss 0.1347406804561615\n",
      "Step no 472\n",
      "Loss 0.1346367597579956\n",
      "Step no 473\n",
      "Loss 0.13453316688537598\n",
      "Step no 474\n",
      "Loss 0.134429931640625\n",
      "Step no 475\n",
      "Loss 0.1343270242214203\n",
      "Step no 476\n",
      "Loss 0.13422444462776184\n",
      "Step no 477\n",
      "Loss 0.13412219285964966\n",
      "Step no 478\n",
      "Loss 0.13402028381824493\n",
      "Step no 479\n",
      "Loss 0.13391870260238647\n",
      "Step no 480\n",
      "Loss 0.13381744921207428\n",
      "Step no 481\n",
      "Loss 0.13371650874614716\n",
      "Step no 482\n",
      "Loss 0.1336158961057663\n",
      "Step no 483\n",
      "Loss 0.1335156261920929\n",
      "Step no 484\n",
      "Loss 0.13341566920280457\n",
      "Step no 485\n",
      "Loss 0.1333160102367401\n",
      "Step no 486\n",
      "Loss 0.13321669399738312\n",
      "Step no 487\n",
      "Loss 0.13311767578125\n",
      "Step no 488\n",
      "Loss 0.13301897048950195\n",
      "Step no 489\n",
      "Loss 0.13292059302330017\n",
      "Step no 490\n",
      "Loss 0.13282251358032227\n",
      "Step no 491\n",
      "Loss 0.13272474706172943\n",
      "Step no 492\n",
      "Loss 0.13262729346752167\n",
      "Step no 493\n",
      "Loss 0.13253013789653778\n",
      "Step no 494\n",
      "Loss 0.13243331015110016\n",
      "Step no 495\n",
      "Loss 0.13233675062656403\n",
      "Step no 496\n",
      "Loss 0.13224050402641296\n",
      "Step no 497\n",
      "Loss 0.13214455544948578\n",
      "Step no 498\n",
      "Loss 0.13204890489578247\n",
      "Step no 499\n",
      "Loss 0.13195356726646423\n",
      "Step no 500\n",
      "Loss 0.13185851275920868\n",
      "Step no 501\n",
      "Loss 0.1317637413740158\n",
      "Step no 502\n",
      "Loss 0.131669282913208\n",
      "Step no 503\n",
      "Loss 0.1315751075744629\n",
      "Step no 504\n",
      "Loss 0.13148121535778046\n",
      "Step no 505\n",
      "Loss 0.1313876062631607\n",
      "Step no 506\n",
      "Loss 0.13129429519176483\n",
      "Step no 507\n",
      "Loss 0.13120125234127045\n",
      "Step no 508\n",
      "Loss 0.13110852241516113\n",
      "Step no 509\n",
      "Loss 0.1310160607099533\n",
      "Step no 510\n",
      "Loss 0.13092386722564697\n",
      "Step no 511\n",
      "Loss 0.13083195686340332\n",
      "Step no 512\n",
      "Loss 0.13074032962322235\n",
      "Step no 513\n",
      "Loss 0.13064897060394287\n",
      "Step no 514\n",
      "Loss 0.13055790960788727\n",
      "Step no 515\n",
      "Loss 0.13046711683273315\n",
      "Step no 516\n",
      "Loss 0.13037657737731934\n",
      "Step no 517\n",
      "Loss 0.130286306142807\n",
      "Step no 518\n",
      "Loss 0.13019633293151855\n",
      "Step no 519\n",
      "Loss 0.1301065981388092\n",
      "Step no 520\n",
      "Loss 0.13001716136932373\n",
      "Step no 521\n",
      "Loss 0.12992796301841736\n",
      "Step no 522\n",
      "Loss 0.12983904778957367\n",
      "Step no 523\n",
      "Loss 0.12975040078163147\n",
      "Step no 524\n",
      "Loss 0.12966199219226837\n",
      "Step no 525\n",
      "Loss 0.12957385182380676\n",
      "Step no 526\n",
      "Loss 0.12948597967624664\n",
      "Step no 527\n",
      "Loss 0.129398375749588\n",
      "Step no 528\n",
      "Loss 0.12931101024150848\n",
      "Step no 529\n",
      "Loss 0.12922389805316925\n",
      "Step no 530\n",
      "Loss 0.1291370391845703\n",
      "Step no 531\n",
      "Loss 0.12905044853687286\n",
      "Step no 532\n",
      "Loss 0.12896409630775452\n",
      "Step no 533\n",
      "Loss 0.12887799739837646\n",
      "Step no 534\n",
      "Loss 0.1287921667098999\n",
      "Step no 535\n",
      "Loss 0.12870655953884125\n",
      "Step no 536\n",
      "Loss 0.1286212056875229\n",
      "Step no 537\n",
      "Loss 0.12853609025478363\n",
      "Step no 538\n",
      "Loss 0.12845124304294586\n",
      "Step no 539\n",
      "Loss 0.1283666342496872\n",
      "Step no 540\n",
      "Loss 0.12828224897384644\n",
      "Step no 541\n",
      "Loss 0.12819813191890717\n",
      "Step no 542\n",
      "Loss 0.1281142383813858\n",
      "Step no 543\n",
      "Loss 0.12803058326244354\n",
      "Step no 544\n",
      "Loss 0.12794716656208038\n",
      "Step no 545\n",
      "Loss 0.12786398828029633\n",
      "Step no 546\n",
      "Loss 0.12778104841709137\n",
      "Step no 547\n",
      "Loss 0.12769834697246552\n",
      "Step no 548\n",
      "Loss 0.12761586904525757\n",
      "Step no 549\n",
      "Loss 0.12753362953662872\n",
      "Step no 550\n",
      "Loss 0.12745162844657898\n",
      "Step no 551\n",
      "Loss 0.12736983597278595\n",
      "Step no 552\n",
      "Loss 0.1272883117198944\n",
      "Step no 553\n",
      "Loss 0.12720699608325958\n",
      "Step no 554\n",
      "Loss 0.12712587416172028\n",
      "Step no 555\n",
      "Loss 0.12704502046108246\n",
      "Step no 556\n",
      "Loss 0.12696437537670135\n",
      "Step no 557\n",
      "Loss 0.12688396871089935\n",
      "Step no 558\n",
      "Loss 0.12680377066135406\n",
      "Step no 559\n",
      "Loss 0.12672379612922668\n",
      "Step no 560\n",
      "Loss 0.1266440600156784\n",
      "Step no 561\n",
      "Loss 0.12656451761722565\n",
      "Step no 562\n",
      "Loss 0.1264851987361908\n",
      "Step no 563\n",
      "Loss 0.12640611827373505\n",
      "Step no 564\n",
      "Loss 0.126327246427536\n",
      "Step no 565\n",
      "Loss 0.1262485831975937\n",
      "Step no 566\n",
      "Loss 0.12617014348506927\n",
      "Step no 567\n",
      "Loss 0.12609189748764038\n",
      "Step no 568\n",
      "Loss 0.1260138899087906\n",
      "Step no 569\n",
      "Loss 0.12593607604503632\n",
      "Step no 570\n",
      "Loss 0.12585848569869995\n",
      "Step no 571\n",
      "Loss 0.1257810890674591\n",
      "Step no 572\n",
      "Loss 0.12570394575595856\n",
      "Step no 573\n",
      "Loss 0.12562698125839233\n",
      "Step no 574\n",
      "Loss 0.12555019557476044\n",
      "Step no 575\n",
      "Loss 0.12547364830970764\n",
      "Step no 576\n",
      "Loss 0.12539732456207275\n",
      "Step no 577\n",
      "Loss 0.1253211796283722\n",
      "Step no 578\n",
      "Loss 0.12524524331092834\n",
      "Step no 579\n",
      "Loss 0.1251695156097412\n",
      "Step no 580\n",
      "Loss 0.1250939965248108\n",
      "Step no 581\n",
      "Loss 0.1250186562538147\n",
      "Step no 582\n",
      "Loss 0.12494352459907532\n",
      "Step no 583\n",
      "Loss 0.12486859411001205\n",
      "Step no 584\n",
      "Loss 0.12479385733604431\n",
      "Step no 585\n",
      "Loss 0.12471933662891388\n",
      "Step no 586\n",
      "Loss 0.12464500963687897\n",
      "Step no 587\n",
      "Loss 0.12457086145877838\n",
      "Step no 588\n",
      "Loss 0.12449690699577332\n",
      "Step no 589\n",
      "Loss 0.12442316859960556\n",
      "Step no 590\n",
      "Loss 0.12434962391853333\n",
      "Step no 591\n",
      "Loss 0.12427624315023422\n",
      "Step no 592\n",
      "Loss 0.12420307099819183\n",
      "Step no 593\n",
      "Loss 0.12413009256124496\n",
      "Step no 594\n",
      "Loss 0.12405731528997421\n",
      "Step no 595\n",
      "Loss 0.12398470938205719\n",
      "Step no 596\n",
      "Loss 0.12391230463981628\n",
      "Step no 597\n",
      "Loss 0.1238400787115097\n",
      "Step no 598\n",
      "Loss 0.12376803159713745\n",
      "Step no 599\n",
      "Loss 0.12369618564844131\n",
      "Step no 600\n",
      "Loss 0.1236245185136795\n",
      "Step no 601\n",
      "Loss 0.12355304509401321\n",
      "Step no 602\n",
      "Loss 0.12348173558712006\n",
      "Step no 603\n",
      "Loss 0.12341062724590302\n",
      "Step no 604\n",
      "Loss 0.1233396977186203\n",
      "Step no 605\n",
      "Loss 0.12326894700527191\n",
      "Step no 606\n",
      "Loss 0.12319839000701904\n",
      "Step no 607\n",
      "Loss 0.12312798947095871\n",
      "Step no 608\n",
      "Loss 0.1230577751994133\n",
      "Step no 609\n",
      "Loss 0.12298774719238281\n",
      "Step no 610\n",
      "Loss 0.12291790544986725\n",
      "Step no 611\n",
      "Loss 0.12284823507070541\n",
      "Step no 612\n",
      "Loss 0.12277872860431671\n",
      "Step no 613\n",
      "Loss 0.12270939350128174\n",
      "Step no 614\n",
      "Loss 0.12264023721218109\n",
      "Step no 615\n",
      "Loss 0.12257128208875656\n",
      "Step no 616\n",
      "Loss 0.12250246852636337\n",
      "Step no 617\n",
      "Loss 0.12243384122848511\n",
      "Step no 618\n",
      "Loss 0.12236539274454117\n",
      "Step no 619\n",
      "Loss 0.12229710072278976\n",
      "Step no 620\n",
      "Loss 0.12222898751497269\n",
      "Step no 621\n",
      "Loss 0.12216103821992874\n",
      "Step no 622\n",
      "Loss 0.12209326028823853\n",
      "Step no 623\n",
      "Loss 0.12202565371990204\n",
      "Step no 624\n",
      "Loss 0.12195821851491928\n",
      "Step no 625\n",
      "Loss 0.12189094722270966\n",
      "Step no 626\n",
      "Loss 0.12182383239269257\n",
      "Step no 627\n",
      "Loss 0.1217568963766098\n",
      "Step no 628\n",
      "Loss 0.12169011682271957\n",
      "Step no 629\n",
      "Loss 0.12162351608276367\n",
      "Step no 630\n",
      "Loss 0.12155706435441971\n",
      "Step no 631\n",
      "Loss 0.12149078398942947\n",
      "Step no 632\n",
      "Loss 0.12142466008663177\n",
      "Step no 633\n",
      "Loss 0.12135870009660721\n",
      "Step no 634\n",
      "Loss 0.12129289656877518\n",
      "Step no 635\n",
      "Loss 0.12122726440429688\n",
      "Step no 636\n",
      "Loss 0.12116176635026932\n",
      "Step no 637\n",
      "Loss 0.12109646201133728\n",
      "Step no 638\n",
      "Loss 0.12103128433227539\n",
      "Step no 639\n",
      "Loss 0.12096628546714783\n",
      "Step no 640\n",
      "Loss 0.1209014505147934\n",
      "Step no 641\n",
      "Loss 0.1208367571234703\n",
      "Step no 642\n",
      "Loss 0.12077222019433975\n",
      "Step no 643\n",
      "Loss 0.12070783972740173\n",
      "Step no 644\n",
      "Loss 0.12064360827207565\n",
      "Step no 645\n",
      "Loss 0.1205795407295227\n",
      "Step no 646\n",
      "Loss 0.12051564455032349\n",
      "Step no 647\n",
      "Loss 0.12045186758041382\n",
      "Step no 648\n",
      "Loss 0.12038827687501907\n",
      "Step no 649\n",
      "Loss 0.12032480537891388\n",
      "Step no 650\n",
      "Loss 0.12026149779558182\n",
      "Step no 651\n",
      "Loss 0.1201983392238617\n",
      "Step no 652\n",
      "Loss 0.12013532221317291\n",
      "Step no 653\n",
      "Loss 0.12007248401641846\n",
      "Step no 654\n",
      "Loss 0.12000976502895355\n",
      "Step no 655\n",
      "Loss 0.11994720250368118\n",
      "Step no 656\n",
      "Loss 0.11988479644060135\n",
      "Step no 657\n",
      "Loss 0.11982253193855286\n",
      "Step no 658\n",
      "Loss 0.1197604089975357\n",
      "Step no 659\n",
      "Loss 0.1196984350681305\n",
      "Step no 660\n",
      "Loss 0.11963661015033722\n",
      "Step no 661\n",
      "Loss 0.11957491934299469\n",
      "Step no 662\n",
      "Loss 0.1195133775472641\n",
      "Step no 663\n",
      "Loss 0.11945198476314545\n",
      "Step no 664\n",
      "Loss 0.11939073354005814\n",
      "Step no 665\n",
      "Loss 0.11932961642742157\n",
      "Step no 666\n",
      "Loss 0.11926865577697754\n",
      "Step no 667\n",
      "Loss 0.11920783668756485\n",
      "Step no 668\n",
      "Loss 0.1191471517086029\n",
      "Step no 669\n",
      "Loss 0.1190866008400917\n",
      "Step no 670\n",
      "Loss 0.11902620643377304\n",
      "Step no 671\n",
      "Loss 0.11896592378616333\n",
      "Step no 672\n",
      "Loss 0.11890579760074615\n",
      "Step no 673\n",
      "Loss 0.11884580552577972\n",
      "Step no 674\n",
      "Loss 0.11878595501184464\n",
      "Step no 675\n",
      "Loss 0.11872624605894089\n",
      "Step no 676\n",
      "Loss 0.11866667866706848\n",
      "Step no 677\n",
      "Loss 0.11860722303390503\n",
      "Step no 678\n",
      "Loss 0.11854792386293411\n",
      "Step no 679\n",
      "Loss 0.11848875880241394\n",
      "Step no 680\n",
      "Loss 0.11842970550060272\n",
      "Step no 681\n",
      "Loss 0.11837080866098404\n",
      "Step no 682\n",
      "Loss 0.1183120533823967\n",
      "Step no 683\n",
      "Loss 0.11825340986251831\n",
      "Step no 684\n",
      "Loss 0.11819490790367126\n",
      "Step no 685\n",
      "Loss 0.11813654750585556\n",
      "Step no 686\n",
      "Loss 0.11807831376791\n",
      "Step no 687\n",
      "Loss 0.118020199239254\n",
      "Step no 688\n",
      "Loss 0.11796222627162933\n",
      "Step no 689\n",
      "Loss 0.11790437996387482\n",
      "Step no 690\n",
      "Loss 0.11784666031599045\n",
      "Step no 691\n",
      "Loss 0.11778906732797623\n",
      "Step no 692\n",
      "Loss 0.11773160099983215\n",
      "Step no 693\n",
      "Loss 0.11767428368330002\n",
      "Step no 694\n",
      "Loss 0.11761707812547684\n",
      "Step no 695\n",
      "Loss 0.1175599992275238\n",
      "Step no 696\n",
      "Loss 0.11750305444002151\n",
      "Step no 697\n",
      "Loss 0.11744623631238937\n",
      "Step no 698\n",
      "Loss 0.11738953739404678\n",
      "Step no 699\n",
      "Loss 0.11733297258615494\n",
      "Step no 700\n",
      "Loss 0.11727651953697205\n",
      "Step no 701\n",
      "Loss 0.1172202005982399\n",
      "Step no 702\n",
      "Loss 0.1171639934182167\n",
      "Step no 703\n",
      "Loss 0.11710793524980545\n",
      "Step no 704\n",
      "Loss 0.11705199629068375\n",
      "Step no 705\n",
      "Loss 0.1169961541891098\n",
      "Step no 706\n",
      "Loss 0.1169404536485672\n",
      "Step no 707\n",
      "Loss 0.11688487231731415\n",
      "Step no 708\n",
      "Loss 0.11682941764593124\n",
      "Step no 709\n",
      "Loss 0.1167740672826767\n",
      "Step no 710\n",
      "Loss 0.1167188510298729\n",
      "Step no 711\n",
      "Loss 0.11666374653577805\n",
      "Step no 712\n",
      "Loss 0.11660877615213394\n",
      "Step no 713\n",
      "Loss 0.11655392497777939\n",
      "Step no 714\n",
      "Loss 0.11649918556213379\n",
      "Step no 715\n",
      "Loss 0.11644455045461655\n",
      "Step no 716\n",
      "Loss 0.11639004945755005\n",
      "Step no 717\n",
      "Loss 0.1163356602191925\n",
      "Step no 718\n",
      "Loss 0.11628139019012451\n",
      "Step no 719\n",
      "Loss 0.11622724682092667\n",
      "Step no 720\n",
      "Loss 0.11617320775985718\n",
      "Step no 721\n",
      "Loss 0.11611928045749664\n",
      "Step no 722\n",
      "Loss 0.11606547236442566\n",
      "Step no 723\n",
      "Loss 0.11601178348064423\n",
      "Step no 724\n",
      "Loss 0.11595820635557175\n",
      "Step no 725\n",
      "Loss 0.11590474098920822\n",
      "Step no 726\n",
      "Loss 0.11585140228271484\n",
      "Step no 727\n",
      "Loss 0.11579816788434982\n",
      "Step no 728\n",
      "Loss 0.11574503779411316\n",
      "Step no 729\n",
      "Loss 0.11569204926490784\n",
      "Step no 730\n",
      "Loss 0.11563914269208908\n",
      "Step no 731\n",
      "Loss 0.11558635532855988\n",
      "Step no 732\n",
      "Loss 0.11553369462490082\n",
      "Step no 733\n",
      "Loss 0.11548111587762833\n",
      "Step no 734\n",
      "Loss 0.11542867869138718\n",
      "Step no 735\n",
      "Loss 0.11537633836269379\n",
      "Step no 736\n",
      "Loss 0.11532410234212875\n",
      "Step no 737\n",
      "Loss 0.11527197062969208\n",
      "Step no 738\n",
      "Loss 0.11521996557712555\n",
      "Step no 739\n",
      "Loss 0.11516805738210678\n",
      "Step no 740\n",
      "Loss 0.11511626839637756\n",
      "Step no 741\n",
      "Loss 0.1150645986199379\n",
      "Step no 742\n",
      "Loss 0.1150130107998848\n",
      "Step no 743\n",
      "Loss 0.11496153473854065\n",
      "Step no 744\n",
      "Loss 0.11491017043590546\n",
      "Step no 745\n",
      "Loss 0.11485891044139862\n",
      "Step no 746\n",
      "Loss 0.11480776220560074\n",
      "Step no 747\n",
      "Loss 0.11475671827793121\n",
      "Step no 748\n",
      "Loss 0.11470577865839005\n",
      "Step no 749\n",
      "Loss 0.11465494334697723\n",
      "Step no 750\n",
      "Loss 0.11460421979427338\n",
      "Step no 751\n",
      "Loss 0.11455360800027847\n",
      "Step no 752\n",
      "Loss 0.11450307071208954\n",
      "Step no 753\n",
      "Loss 0.11445266008377075\n",
      "Step no 754\n",
      "Loss 0.11440234631299973\n",
      "Step no 755\n",
      "Loss 0.11435213685035706\n",
      "Step no 756\n",
      "Loss 0.11430202424526215\n",
      "Step no 757\n",
      "Loss 0.1142520159482956\n",
      "Step no 758\n",
      "Loss 0.114202119410038\n",
      "Step no 759\n",
      "Loss 0.11415231227874756\n",
      "Step no 760\n",
      "Loss 0.11410261690616608\n",
      "Step no 761\n",
      "Loss 0.11405301094055176\n",
      "Step no 762\n",
      "Loss 0.1140035018324852\n",
      "Step no 763\n",
      "Loss 0.113954097032547\n",
      "Step no 764\n",
      "Loss 0.11390481889247894\n",
      "Step no 765\n",
      "Loss 0.11385560780763626\n",
      "Step no 766\n",
      "Loss 0.11380651593208313\n",
      "Step no 767\n",
      "Loss 0.11375750601291656\n",
      "Step no 768\n",
      "Loss 0.11370861530303955\n",
      "Step no 769\n",
      "Loss 0.1136598140001297\n",
      "Step no 770\n",
      "Loss 0.11361110210418701\n",
      "Step no 771\n",
      "Loss 0.11356248706579208\n",
      "Step no 772\n",
      "Loss 0.11351397633552551\n",
      "Step no 773\n",
      "Loss 0.1134655624628067\n",
      "Step no 774\n",
      "Loss 0.11341725289821625\n",
      "Step no 775\n",
      "Loss 0.11336904019117355\n",
      "Step no 776\n",
      "Loss 0.11332091689109802\n",
      "Step no 777\n",
      "Loss 0.11327287554740906\n",
      "Step no 778\n",
      "Loss 0.11322493851184845\n",
      "Step no 779\n",
      "Loss 0.1131771057844162\n",
      "Step no 780\n",
      "Loss 0.11312935501337051\n",
      "Step no 781\n",
      "Loss 0.11308170855045319\n",
      "Step no 782\n",
      "Loss 0.11303415149450302\n",
      "Step no 783\n",
      "Loss 0.11298669129610062\n",
      "Step no 784\n",
      "Loss 0.11293932795524597\n",
      "Step no 785\n",
      "Loss 0.1128920465707779\n",
      "Step no 786\n",
      "Loss 0.11284486204385757\n",
      "Step no 787\n",
      "Loss 0.11279777437448502\n",
      "Step no 788\n",
      "Loss 0.11275077611207962\n",
      "Step no 789\n",
      "Loss 0.11270387470722198\n",
      "Step no 790\n",
      "Loss 0.11265706270933151\n",
      "Step no 791\n",
      "Loss 0.1126103401184082\n",
      "Step no 792\n",
      "Loss 0.11256371438503265\n",
      "Step no 793\n",
      "Loss 0.11251717805862427\n",
      "Step no 794\n",
      "Loss 0.11247072368860245\n",
      "Step no 795\n",
      "Loss 0.11242436617612839\n",
      "Step no 796\n",
      "Loss 0.1123780831694603\n",
      "Step no 797\n",
      "Loss 0.11233192682266235\n",
      "Step no 798\n",
      "Loss 0.11228583008050919\n",
      "Step no 799\n",
      "Loss 0.11223983019590378\n",
      "Step no 800\n",
      "Loss 0.11219391226768494\n",
      "Step no 801\n",
      "Loss 0.11214808374643326\n",
      "Step no 802\n",
      "Loss 0.11210235208272934\n",
      "Step no 803\n",
      "Loss 0.11205670982599258\n",
      "Step no 804\n",
      "Loss 0.1120111346244812\n",
      "Step no 805\n",
      "Loss 0.11196569353342056\n",
      "Step no 806\n",
      "Loss 0.1119203045964241\n",
      "Step no 807\n",
      "Loss 0.11187499761581421\n",
      "Step no 808\n",
      "Loss 0.11182978749275208\n",
      "Step no 809\n",
      "Loss 0.11178465932607651\n",
      "Step no 810\n",
      "Loss 0.1117396205663681\n",
      "Step no 811\n",
      "Loss 0.11169467121362686\n",
      "Step no 812\n",
      "Loss 0.11164981126785278\n",
      "Step no 813\n",
      "Loss 0.11160503327846527\n",
      "Step no 814\n",
      "Loss 0.11156032979488373\n",
      "Step no 815\n",
      "Loss 0.11151570826768875\n",
      "Step no 816\n",
      "Loss 0.11147119104862213\n",
      "Step no 817\n",
      "Loss 0.11142674088478088\n",
      "Step no 818\n",
      "Loss 0.1113823726773262\n",
      "Step no 819\n",
      "Loss 0.11133809387683868\n",
      "Step no 820\n",
      "Loss 0.11129391193389893\n",
      "Step no 821\n",
      "Loss 0.11124979704618454\n",
      "Step no 822\n",
      "Loss 0.11120577156543732\n",
      "Step no 823\n",
      "Loss 0.11116183549165726\n",
      "Step no 824\n",
      "Loss 0.11111797392368317\n",
      "Step no 825\n",
      "Loss 0.11107417941093445\n",
      "Step no 826\n",
      "Loss 0.11103048175573349\n",
      "Step no 827\n",
      "Loss 0.11098688095808029\n",
      "Step no 828\n",
      "Loss 0.11094333976507187\n",
      "Step no 829\n",
      "Loss 0.11089988797903061\n",
      "Step no 830\n",
      "Loss 0.11085651069879532\n",
      "Step no 831\n",
      "Loss 0.11081322282552719\n",
      "Step no 832\n",
      "Loss 0.11077001690864563\n",
      "Step no 833\n",
      "Loss 0.11072687804698944\n",
      "Step no 834\n",
      "Loss 0.11068382859230042\n",
      "Step no 835\n",
      "Loss 0.11064085364341736\n",
      "Step no 836\n",
      "Loss 0.11059796810150146\n",
      "Step no 837\n",
      "Loss 0.11055514961481094\n",
      "Step no 838\n",
      "Loss 0.11051240563392639\n",
      "Step no 839\n",
      "Loss 0.1104697585105896\n",
      "Step no 840\n",
      "Loss 0.11042717099189758\n",
      "Step no 841\n",
      "Loss 0.11038467288017273\n",
      "Step no 842\n",
      "Loss 0.11034225672483444\n",
      "Step no 843\n",
      "Loss 0.11029990762472153\n",
      "Step no 844\n",
      "Loss 0.11025765538215637\n",
      "Step no 845\n",
      "Loss 0.1102154478430748\n",
      "Step no 846\n",
      "Loss 0.11017334461212158\n",
      "Step no 847\n",
      "Loss 0.11013131588697433\n",
      "Step no 848\n",
      "Loss 0.11008935421705246\n",
      "Step no 849\n",
      "Loss 0.11004747450351715\n",
      "Step no 850\n",
      "Loss 0.11000564694404602\n",
      "Step no 851\n",
      "Loss 0.10996393114328384\n",
      "Step no 852\n",
      "Loss 0.10992226749658585\n",
      "Step no 853\n",
      "Loss 0.10988069325685501\n",
      "Step no 854\n",
      "Loss 0.10983919352293015\n",
      "Step no 855\n",
      "Loss 0.10979776084423065\n",
      "Step no 856\n",
      "Loss 0.10975640267133713\n",
      "Step no 857\n",
      "Loss 0.10971513390541077\n",
      "Step no 858\n",
      "Loss 0.10967391729354858\n",
      "Step no 859\n",
      "Loss 0.10963278263807297\n",
      "Step no 860\n",
      "Loss 0.10959172993898392\n",
      "Step no 861\n",
      "Loss 0.10955074429512024\n",
      "Step no 862\n",
      "Loss 0.10950983315706253\n",
      "Step no 863\n",
      "Loss 0.1094689890742302\n",
      "Step no 864\n",
      "Loss 0.10942822694778442\n",
      "Step no 865\n",
      "Loss 0.10938753932714462\n",
      "Step no 866\n",
      "Loss 0.10934692621231079\n",
      "Step no 867\n",
      "Loss 0.10930637270212173\n",
      "Step no 868\n",
      "Loss 0.10926589369773865\n",
      "Step no 869\n",
      "Loss 0.10922549664974213\n",
      "Step no 870\n",
      "Loss 0.10918515920639038\n",
      "Step no 871\n",
      "Loss 0.1091449111700058\n",
      "Step no 872\n",
      "Loss 0.1091047078371048\n",
      "Step no 873\n",
      "Loss 0.10906460136175156\n",
      "Step no 874\n",
      "Loss 0.10902455449104309\n",
      "Step no 875\n",
      "Loss 0.1089845672249794\n",
      "Step no 876\n",
      "Loss 0.10894465446472168\n",
      "Step no 877\n",
      "Loss 0.10890482366085052\n",
      "Step no 878\n",
      "Loss 0.10886505991220474\n",
      "Step no 879\n",
      "Loss 0.10882537066936493\n",
      "Step no 880\n",
      "Loss 0.10878574103116989\n",
      "Step no 881\n",
      "Loss 0.10874618589878082\n",
      "Step no 882\n",
      "Loss 0.10870669782161713\n",
      "Step no 883\n",
      "Loss 0.1086672693490982\n",
      "Step no 884\n",
      "Loss 0.10862793028354645\n",
      "Step no 885\n",
      "Loss 0.10858865082263947\n",
      "Step no 886\n",
      "Loss 0.10854943096637726\n",
      "Step no 887\n",
      "Loss 0.10851028561592102\n",
      "Step no 888\n",
      "Loss 0.10847120732069016\n",
      "Step no 889\n",
      "Loss 0.10843221843242645\n",
      "Step no 890\n",
      "Loss 0.10839325934648514\n",
      "Step no 891\n",
      "Loss 0.10835438966751099\n",
      "Step no 892\n",
      "Loss 0.10831558704376221\n",
      "Step no 893\n",
      "Loss 0.1082768365740776\n",
      "Step no 894\n",
      "Loss 0.10823816061019897\n",
      "Step no 895\n",
      "Loss 0.10819955170154572\n",
      "Step no 896\n",
      "Loss 0.10816101729869843\n",
      "Step no 897\n",
      "Loss 0.10812254995107651\n",
      "Step no 898\n",
      "Loss 0.10808414220809937\n",
      "Step no 899\n",
      "Loss 0.1080458015203476\n",
      "Step no 900\n",
      "Loss 0.1080075278878212\n",
      "Step no 901\n",
      "Loss 0.10796931385993958\n",
      "Step no 902\n",
      "Loss 0.10793116688728333\n",
      "Step no 903\n",
      "Loss 0.10789309442043304\n",
      "Step no 904\n",
      "Loss 0.10785506665706635\n",
      "Step no 905\n",
      "Loss 0.10781712085008621\n",
      "Step no 906\n",
      "Loss 0.10777924209833145\n",
      "Step no 907\n",
      "Loss 0.10774142295122147\n",
      "Step no 908\n",
      "Loss 0.10770367085933685\n",
      "Step no 909\n",
      "Loss 0.10766596347093582\n",
      "Step no 910\n",
      "Loss 0.10762834548950195\n",
      "Step no 911\n",
      "Loss 0.10759077966213226\n",
      "Step no 912\n",
      "Loss 0.10755328088998795\n",
      "Step no 913\n",
      "Loss 0.1075158342719078\n",
      "Step no 914\n",
      "Loss 0.10747845470905304\n",
      "Step no 915\n",
      "Loss 0.10744114220142365\n",
      "Step no 916\n",
      "Loss 0.10740390419960022\n",
      "Step no 917\n",
      "Loss 0.10736670345067978\n",
      "Step no 918\n",
      "Loss 0.1073295921087265\n",
      "Step no 919\n",
      "Loss 0.1072925254702568\n",
      "Step no 920\n",
      "Loss 0.10725553333759308\n",
      "Step no 921\n",
      "Loss 0.10721859335899353\n",
      "Step no 922\n",
      "Loss 0.10718171298503876\n",
      "Step no 923\n",
      "Loss 0.10714489966630936\n",
      "Step no 924\n",
      "Loss 0.10710814595222473\n",
      "Step no 925\n",
      "Loss 0.10707146674394608\n",
      "Step no 926\n",
      "Loss 0.10703481733798981\n",
      "Step no 927\n",
      "Loss 0.1069982498884201\n",
      "Step no 928\n",
      "Loss 0.10696173459291458\n",
      "Step no 929\n",
      "Loss 0.10692530870437622\n",
      "Step no 930\n",
      "Loss 0.10688891261816025\n",
      "Step no 931\n",
      "Loss 0.10685257613658905\n",
      "Step no 932\n",
      "Loss 0.10681632161140442\n",
      "Step no 933\n",
      "Loss 0.10678011178970337\n",
      "Step no 934\n",
      "Loss 0.10674396902322769\n",
      "Step no 935\n",
      "Loss 0.1067078709602356\n",
      "Step no 936\n",
      "Loss 0.10667184740304947\n",
      "Step no 937\n",
      "Loss 0.10663588345050812\n",
      "Step no 938\n",
      "Loss 0.10659997910261154\n",
      "Step no 939\n",
      "Loss 0.10656411200761795\n",
      "Step no 940\n",
      "Loss 0.10652831941843033\n",
      "Step no 941\n",
      "Loss 0.10649258643388748\n",
      "Step no 942\n",
      "Loss 0.10645691305398941\n",
      "Step no 943\n",
      "Loss 0.10642129927873611\n",
      "Step no 944\n",
      "Loss 0.1063857302069664\n",
      "Step no 945\n",
      "Loss 0.10635022819042206\n",
      "Step no 946\n",
      "Loss 0.10631479322910309\n",
      "Step no 947\n",
      "Loss 0.1062794104218483\n",
      "Step no 948\n",
      "Loss 0.10624407231807709\n",
      "Step no 949\n",
      "Loss 0.10620880872011185\n",
      "Step no 950\n",
      "Loss 0.10617359727621078\n",
      "Step no 951\n",
      "Loss 0.1061384528875351\n",
      "Step no 952\n",
      "Loss 0.1061033308506012\n",
      "Step no 953\n",
      "Loss 0.10606829822063446\n",
      "Step no 954\n",
      "Loss 0.1060333102941513\n",
      "Step no 955\n",
      "Loss 0.10599839687347412\n",
      "Step no 956\n",
      "Loss 0.10596350580453873\n",
      "Step no 957\n",
      "Loss 0.1059287041425705\n",
      "Step no 958\n",
      "Loss 0.10589393973350525\n",
      "Step no 959\n",
      "Loss 0.10585922002792358\n",
      "Step no 960\n",
      "Loss 0.10582457482814789\n",
      "Step no 961\n",
      "Loss 0.10578998178243637\n",
      "Step no 962\n",
      "Loss 0.10575544834136963\n",
      "Step no 963\n",
      "Loss 0.10572097450494766\n",
      "Step no 964\n",
      "Loss 0.10568653792142868\n",
      "Step no 965\n",
      "Loss 0.10565216094255447\n",
      "Step no 966\n",
      "Loss 0.10561785101890564\n",
      "Step no 967\n",
      "Loss 0.10558357834815979\n",
      "Step no 968\n",
      "Loss 0.10554936528205872\n",
      "Step no 969\n",
      "Loss 0.10551521182060242\n",
      "Step no 970\n",
      "Loss 0.10548112541437149\n",
      "Step no 971\n",
      "Loss 0.10544705390930176\n",
      "Step no 972\n",
      "Loss 0.10541307181119919\n",
      "Step no 973\n",
      "Loss 0.1053791344165802\n",
      "Step no 974\n",
      "Loss 0.10534526407718658\n",
      "Step no 975\n",
      "Loss 0.10531143099069595\n",
      "Step no 976\n",
      "Loss 0.1052776426076889\n",
      "Step no 977\n",
      "Loss 0.10524392127990723\n",
      "Step no 978\n",
      "Loss 0.10521025210618973\n",
      "Step no 979\n",
      "Loss 0.10517663508653641\n",
      "Step no 980\n",
      "Loss 0.10514307767152786\n",
      "Step no 981\n",
      "Loss 0.1051095575094223\n",
      "Step no 982\n",
      "Loss 0.10507610440254211\n",
      "Step no 983\n",
      "Loss 0.10504268109798431\n",
      "Step no 984\n",
      "Loss 0.10500932484865189\n",
      "Step no 985\n",
      "Loss 0.10497601330280304\n",
      "Step no 986\n",
      "Loss 0.10494277626276016\n",
      "Step no 987\n",
      "Loss 0.10490957647562027\n",
      "Step no 988\n",
      "Loss 0.10487643629312515\n",
      "Step no 989\n",
      "Loss 0.10484334081411362\n",
      "Step no 990\n",
      "Loss 0.10481029003858566\n",
      "Step no 991\n",
      "Loss 0.10477729886770248\n",
      "Step no 992\n",
      "Loss 0.10474435985088348\n",
      "Step no 993\n",
      "Loss 0.10471147298812866\n",
      "Step no 994\n",
      "Loss 0.10467862337827682\n",
      "Step no 995\n",
      "Loss 0.10464582592248917\n",
      "Step no 996\n",
      "Loss 0.10461309552192688\n",
      "Step no 997\n",
      "Loss 0.10458040237426758\n",
      "Step no 998\n",
      "Loss 0.10454777628183365\n",
      "Step no 999\n",
      "Loss 0.1045151799917221\n"
     ]
    }
   ],
   "source": [
    "logs = logistic2.train(X_train_scaled, y_train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de560eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(logs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af5c7ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9785)\n"
     ]
    }
   ],
   "source": [
    "train_preds = logistic(X_train_scaled)\n",
    "print(torch.mean((1*(train_preds > 0.5) == y_train)*1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7e26f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9825)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = logistic(X_test_scaled)\n",
    "torch.mean((1*(preds > 0.5) == y_test)*1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d3f2d9",
   "metadata": {},
   "source": [
    "## Coding Logistic Regression as a (very simple) Neural Network in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb31c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLR(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(p, 1) # Linear layer means vanilla fully connected layer (with both weights and biases)\n",
    "        # p represents no. of input features and only one output since we're doing binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.lin1(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b830e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalently, we can use the sequential API to define the layers if we have a straightforward flow\n",
    "\n",
    "class NeuralLR(nn.Sequential):\n",
    "    def __init__(self, p):\n",
    "        super().__init__(\n",
    "            nn.Linear(p,1),\n",
    "            nn.Sigmoid()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b86f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generally define the train function separately from the model class\n",
    "\n",
    "def train(model, X_train, y_train, optimiser, loss_fn, n_steps):\n",
    "    logs = []\n",
    "    for i in range(n_steps):\n",
    "        optimiser.zero_grad # zero the gradients before each step so that they dont accumulate\n",
    "        preds = model(X_train) # forward pass\n",
    "        loss = loss_fn(preds, y_train) # loss computation\n",
    "        loss.backward() # automatically computes the gradients of loss wrt params\n",
    "        optimiser.step() # updates the parameters acc to the optimiser hyperparams and gradient\n",
    "\n",
    "        print(f\"Step {i}\")\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        \n",
    "        logs.append(loss.item())\n",
    "        \n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a1ecea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import binary_cross_entropy\n",
    "\n",
    "model = NeuralLR(30)\n",
    "loss_fn = binary_cross_entropy\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "787cf726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Loss: 0.5475033521652222\n",
      "Step 1\n",
      "Loss: 0.5474904775619507\n",
      "Step 2\n",
      "Loss: 0.5474647283554077\n",
      "Step 3\n",
      "Loss: 0.5474261045455933\n",
      "Step 4\n",
      "Loss: 0.5473746061325073\n",
      "Step 5\n",
      "Loss: 0.5473102331161499\n",
      "Step 6\n",
      "Loss: 0.547232985496521\n",
      "Step 7\n",
      "Loss: 0.5471428632736206\n",
      "Step 8\n",
      "Loss: 0.5470399856567383\n",
      "Step 9\n",
      "Loss: 0.5469242930412292\n",
      "Step 10\n",
      "Loss: 0.5467957258224487\n",
      "Step 11\n",
      "Loss: 0.5466544032096863\n",
      "Step 12\n",
      "Loss: 0.5465002655982971\n",
      "Step 13\n",
      "Loss: 0.546333372592926\n",
      "Step 14\n",
      "Loss: 0.5461537837982178\n",
      "Step 15\n",
      "Loss: 0.5459614396095276\n",
      "Step 16\n",
      "Loss: 0.5457563996315002\n",
      "Step 17\n",
      "Loss: 0.5455386638641357\n",
      "Step 18\n",
      "Loss: 0.5453082323074341\n",
      "Step 19\n",
      "Loss: 0.5450652837753296\n",
      "Step 20\n",
      "Loss: 0.5448096990585327\n",
      "Step 21\n",
      "Loss: 0.5445415377616882\n",
      "Step 22\n",
      "Loss: 0.5442608594894409\n",
      "Step 23\n",
      "Loss: 0.5439676642417908\n",
      "Step 24\n",
      "Loss: 0.5436620116233826\n",
      "Step 25\n",
      "Loss: 0.5433439016342163\n",
      "Step 26\n",
      "Loss: 0.5430133938789368\n",
      "Step 27\n",
      "Loss: 0.5426706075668335\n",
      "Step 28\n",
      "Loss: 0.5423154234886169\n",
      "Step 29\n",
      "Loss: 0.5419479608535767\n",
      "Step 30\n",
      "Loss: 0.5415682792663574\n",
      "Step 31\n",
      "Loss: 0.5411764979362488\n",
      "Step 32\n",
      "Loss: 0.5407724380493164\n",
      "Step 33\n",
      "Loss: 0.540356457233429\n",
      "Step 34\n",
      "Loss: 0.5399283170700073\n",
      "Step 35\n",
      "Loss: 0.5394881963729858\n",
      "Step 36\n",
      "Loss: 0.5390361547470093\n",
      "Step 37\n",
      "Loss: 0.5385721921920776\n",
      "Step 38\n",
      "Loss: 0.5380963683128357\n",
      "Step 39\n",
      "Loss: 0.5376087427139282\n",
      "Step 40\n",
      "Loss: 0.5371094346046448\n",
      "Step 41\n",
      "Loss: 0.5365984439849854\n",
      "Step 42\n",
      "Loss: 0.5360758304595947\n",
      "Step 43\n",
      "Loss: 0.5355416536331177\n",
      "Step 44\n",
      "Loss: 0.534995973110199\n",
      "Step 45\n",
      "Loss: 0.5344389081001282\n",
      "Step 46\n",
      "Loss: 0.5338704586029053\n",
      "Step 47\n",
      "Loss: 0.533290684223175\n",
      "Step 48\n",
      "Loss: 0.5326996445655823\n",
      "Step 49\n",
      "Loss: 0.532097578048706\n",
      "Step 50\n",
      "Loss: 0.5314843058586121\n",
      "Step 51\n",
      "Loss: 0.5308600068092346\n",
      "Step 52\n",
      "Loss: 0.5302246809005737\n",
      "Step 53\n",
      "Loss: 0.5295786261558533\n",
      "Step 54\n",
      "Loss: 0.5289216041564941\n",
      "Step 55\n",
      "Loss: 0.5282539129257202\n",
      "Step 56\n",
      "Loss: 0.5275754928588867\n",
      "Step 57\n",
      "Loss: 0.5268865823745728\n",
      "Step 58\n",
      "Loss: 0.5261870622634888\n",
      "Step 59\n",
      "Loss: 0.5254771709442139\n",
      "Step 60\n",
      "Loss: 0.524756908416748\n",
      "Step 61\n",
      "Loss: 0.5240262746810913\n",
      "Step 62\n",
      "Loss: 0.5232856273651123\n",
      "Step 63\n",
      "Loss: 0.5225346684455872\n",
      "Step 64\n",
      "Loss: 0.5217738151550293\n",
      "Step 65\n",
      "Loss: 0.521003007888794\n",
      "Step 66\n",
      "Loss: 0.5202222466468811\n",
      "Step 67\n",
      "Loss: 0.5194317698478699\n",
      "Step 68\n",
      "Loss: 0.5186315774917603\n",
      "Step 69\n",
      "Loss: 0.5178217887878418\n",
      "Step 70\n",
      "Loss: 0.5170025825500488\n",
      "Step 71\n",
      "Loss: 0.5161738395690918\n",
      "Step 72\n",
      "Loss: 0.5153357982635498\n",
      "Step 73\n",
      "Loss: 0.5144885182380676\n",
      "Step 74\n",
      "Loss: 0.51363205909729\n",
      "Step 75\n",
      "Loss: 0.5127665996551514\n",
      "Step 76\n",
      "Loss: 0.5118921399116516\n",
      "Step 77\n",
      "Loss: 0.5110087990760803\n",
      "Step 78\n",
      "Loss: 0.510116696357727\n",
      "Step 79\n",
      "Loss: 0.5092159509658813\n",
      "Step 80\n",
      "Loss: 0.5083065032958984\n",
      "Step 81\n",
      "Loss: 0.5073886513710022\n",
      "Step 82\n",
      "Loss: 0.5064624547958374\n",
      "Step 83\n",
      "Loss: 0.5055278539657593\n",
      "Step 84\n",
      "Loss: 0.5045850872993469\n",
      "Step 85\n",
      "Loss: 0.5036342144012451\n",
      "Step 86\n",
      "Loss: 0.5026753544807434\n",
      "Step 87\n",
      "Loss: 0.5017086863517761\n",
      "Step 88\n",
      "Loss: 0.5007340908050537\n",
      "Step 89\n",
      "Loss: 0.49975183606147766\n",
      "Step 90\n",
      "Loss: 0.4987620413303375\n",
      "Step 91\n",
      "Loss: 0.4977646768093109\n",
      "Step 92\n",
      "Loss: 0.49675995111465454\n",
      "Step 93\n",
      "Loss: 0.4957479238510132\n",
      "Step 94\n",
      "Loss: 0.4947287440299988\n",
      "Step 95\n",
      "Loss: 0.4937024712562561\n",
      "Step 96\n",
      "Loss: 0.49266916513442993\n",
      "Step 97\n",
      "Loss: 0.491629034280777\n",
      "Step 98\n",
      "Loss: 0.49058210849761963\n",
      "Step 99\n",
      "Loss: 0.48952847719192505\n",
      "Step 100\n",
      "Loss: 0.48846837878227234\n",
      "Step 101\n",
      "Loss: 0.4874017834663391\n",
      "Step 102\n",
      "Loss: 0.48632878065109253\n",
      "Step 103\n",
      "Loss: 0.4852495789527893\n",
      "Step 104\n",
      "Loss: 0.4841642677783966\n",
      "Step 105\n",
      "Loss: 0.48307281732559204\n",
      "Step 106\n",
      "Loss: 0.4819755554199219\n",
      "Step 107\n",
      "Loss: 0.48087239265441895\n",
      "Step 108\n",
      "Loss: 0.47976353764533997\n",
      "Step 109\n",
      "Loss: 0.4786490499973297\n",
      "Step 110\n",
      "Loss: 0.4775291085243225\n",
      "Step 111\n",
      "Loss: 0.47640371322631836\n",
      "Step 112\n",
      "Loss: 0.4752730131149292\n",
      "Step 113\n",
      "Loss: 0.47413718700408936\n",
      "Step 114\n",
      "Loss: 0.4729962646961212\n",
      "Step 115\n",
      "Loss: 0.47185030579566956\n",
      "Step 116\n",
      "Loss: 0.4706995189189911\n",
      "Step 117\n",
      "Loss: 0.4695439636707306\n",
      "Step 118\n",
      "Loss: 0.4683837294578552\n",
      "Step 119\n",
      "Loss: 0.4672189950942993\n",
      "Step 120\n",
      "Loss: 0.46604979038238525\n",
      "Step 121\n",
      "Loss: 0.4648762047290802\n",
      "Step 122\n",
      "Loss: 0.4636984169483185\n",
      "Step 123\n",
      "Loss: 0.4625164866447449\n",
      "Step 124\n",
      "Loss: 0.4613305628299713\n",
      "Step 125\n",
      "Loss: 0.4601406753063202\n",
      "Step 126\n",
      "Loss: 0.45894700288772583\n",
      "Step 127\n",
      "Loss: 0.4577495753765106\n",
      "Step 128\n",
      "Loss: 0.4565486013889313\n",
      "Step 129\n",
      "Loss: 0.4553440511226654\n",
      "Step 130\n",
      "Loss: 0.4541361331939697\n",
      "Step 131\n",
      "Loss: 0.45292484760284424\n",
      "Step 132\n",
      "Loss: 0.45171043276786804\n",
      "Step 133\n",
      "Loss: 0.4504929184913635\n",
      "Step 134\n",
      "Loss: 0.44927239418029785\n",
      "Step 135\n",
      "Loss: 0.44804897904396057\n",
      "Step 136\n",
      "Loss: 0.44682276248931885\n",
      "Step 137\n",
      "Loss: 0.44559383392333984\n",
      "Step 138\n",
      "Loss: 0.4443623423576355\n",
      "Step 139\n",
      "Loss: 0.4431283175945282\n",
      "Step 140\n",
      "Loss: 0.4418919086456299\n",
      "Step 141\n",
      "Loss: 0.4406532347202301\n",
      "Step 142\n",
      "Loss: 0.43941235542297363\n",
      "Step 143\n",
      "Loss: 0.43816938996315\n",
      "Step 144\n",
      "Loss: 0.4369243383407593\n",
      "Step 145\n",
      "Loss: 0.4356774091720581\n",
      "Step 146\n",
      "Loss: 0.43442869186401367\n",
      "Step 147\n",
      "Loss: 0.43317824602127075\n",
      "Step 148\n",
      "Loss: 0.4319261610507965\n",
      "Step 149\n",
      "Loss: 0.4306725859642029\n",
      "Step 150\n",
      "Loss: 0.42941755056381226\n",
      "Step 151\n",
      "Loss: 0.4281611740589142\n",
      "Step 152\n",
      "Loss: 0.42690351605415344\n",
      "Step 153\n",
      "Loss: 0.42564472556114197\n",
      "Step 154\n",
      "Loss: 0.4243848919868469\n",
      "Step 155\n",
      "Loss: 0.4231240749359131\n",
      "Step 156\n",
      "Loss: 0.42186230421066284\n",
      "Step 157\n",
      "Loss: 0.4205997884273529\n",
      "Step 158\n",
      "Loss: 0.4193365275859833\n",
      "Step 159\n",
      "Loss: 0.4180726706981659\n",
      "Step 160\n",
      "Loss: 0.41680824756622314\n",
      "Step 161\n",
      "Loss: 0.41554343700408936\n",
      "Step 162\n",
      "Loss: 0.41427817940711975\n",
      "Step 163\n",
      "Loss: 0.41301271319389343\n",
      "Step 164\n",
      "Loss: 0.411747008562088\n",
      "Step 165\n",
      "Loss: 0.41048115491867065\n",
      "Step 166\n",
      "Loss: 0.4092152714729309\n",
      "Step 167\n",
      "Loss: 0.4079495072364807\n",
      "Step 168\n",
      "Loss: 0.4066838324069977\n",
      "Step 169\n",
      "Loss: 0.405418336391449\n",
      "Step 170\n",
      "Loss: 0.40415316820144653\n",
      "Step 171\n",
      "Loss: 0.40288835763931274\n",
      "Step 172\n",
      "Loss: 0.40162399411201477\n",
      "Step 173\n",
      "Loss: 0.4003601372241974\n",
      "Step 174\n",
      "Loss: 0.39909693598747253\n",
      "Step 175\n",
      "Loss: 0.39783433079719543\n",
      "Step 176\n",
      "Loss: 0.396572470664978\n",
      "Step 177\n",
      "Loss: 0.39531150460243225\n",
      "Step 178\n",
      "Loss: 0.39405137300491333\n",
      "Step 179\n",
      "Loss: 0.3927922248840332\n",
      "Step 180\n",
      "Loss: 0.39153409004211426\n",
      "Step 181\n",
      "Loss: 0.39027711749076843\n",
      "Step 182\n",
      "Loss: 0.38902127742767334\n",
      "Step 183\n",
      "Loss: 0.3877667188644409\n",
      "Step 184\n",
      "Loss: 0.38651344180107117\n",
      "Step 185\n",
      "Loss: 0.38526153564453125\n",
      "Step 186\n",
      "Loss: 0.3840111196041107\n",
      "Step 187\n",
      "Loss: 0.38276219367980957\n",
      "Step 188\n",
      "Loss: 0.38151484727859497\n",
      "Step 189\n",
      "Loss: 0.3802691102027893\n",
      "Step 190\n",
      "Loss: 0.3790251314640045\n",
      "Step 191\n",
      "Loss: 0.3777828812599182\n",
      "Step 192\n",
      "Loss: 0.37654247879981995\n",
      "Step 193\n",
      "Loss: 0.3753039240837097\n",
      "Step 194\n",
      "Loss: 0.37406736612319946\n",
      "Step 195\n",
      "Loss: 0.3728327751159668\n",
      "Step 196\n",
      "Loss: 0.37160027027130127\n",
      "Step 197\n",
      "Loss: 0.37036988139152527\n",
      "Step 198\n",
      "Loss: 0.3691416382789612\n",
      "Step 199\n",
      "Loss: 0.36791568994522095\n",
      "Step 200\n",
      "Loss: 0.3666919469833374\n",
      "Step 201\n",
      "Loss: 0.36547064781188965\n",
      "Step 202\n",
      "Loss: 0.3642517030239105\n",
      "Step 203\n",
      "Loss: 0.3630351126194\n",
      "Step 204\n",
      "Loss: 0.36182111501693726\n",
      "Step 205\n",
      "Loss: 0.36060965061187744\n",
      "Step 206\n",
      "Loss: 0.35940074920654297\n",
      "Step 207\n",
      "Loss: 0.3581945300102234\n",
      "Step 208\n",
      "Loss: 0.3569909632205963\n",
      "Step 209\n",
      "Loss: 0.3557901382446289\n",
      "Step 210\n",
      "Loss: 0.3545921742916107\n",
      "Step 211\n",
      "Loss: 0.3533969223499298\n",
      "Step 212\n",
      "Loss: 0.3522046208381653\n",
      "Step 213\n",
      "Loss: 0.35101518034935\n",
      "Step 214\n",
      "Loss: 0.3498287796974182\n",
      "Step 215\n",
      "Loss: 0.34864532947540283\n",
      "Step 216\n",
      "Loss: 0.34746497869491577\n",
      "Step 217\n",
      "Loss: 0.34628766775131226\n",
      "Step 218\n",
      "Loss: 0.34511348605155945\n",
      "Step 219\n",
      "Loss: 0.34394246339797974\n",
      "Step 220\n",
      "Loss: 0.3427746295928955\n",
      "Step 221\n",
      "Loss: 0.3416100740432739\n",
      "Step 222\n",
      "Loss: 0.340448796749115\n",
      "Step 223\n",
      "Loss: 0.3392908275127411\n",
      "Step 224\n",
      "Loss: 0.338136225938797\n",
      "Step 225\n",
      "Loss: 0.3369849622249603\n",
      "Step 226\n",
      "Loss: 0.33583715558052063\n",
      "Step 227\n",
      "Loss: 0.33469274640083313\n",
      "Step 228\n",
      "Loss: 0.3335518538951874\n",
      "Step 229\n",
      "Loss: 0.33241450786590576\n",
      "Step 230\n",
      "Loss: 0.3312806487083435\n",
      "Step 231\n",
      "Loss: 0.3301503658294678\n",
      "Step 232\n",
      "Loss: 0.32902371883392334\n",
      "Step 233\n",
      "Loss: 0.3279006779193878\n",
      "Step 234\n",
      "Loss: 0.32678133249282837\n",
      "Step 235\n",
      "Loss: 0.3256656527519226\n",
      "Step 236\n",
      "Loss: 0.3245536983013153\n",
      "Step 237\n",
      "Loss: 0.32344546914100647\n",
      "Step 238\n",
      "Loss: 0.32234102487564087\n",
      "Step 239\n",
      "Loss: 0.32124030590057373\n",
      "Step 240\n",
      "Loss: 0.320143461227417\n",
      "Step 241\n",
      "Loss: 0.31905046105384827\n",
      "Step 242\n",
      "Loss: 0.31796130537986755\n",
      "Step 243\n",
      "Loss: 0.31687605381011963\n",
      "Step 244\n",
      "Loss: 0.3157946467399597\n",
      "Step 245\n",
      "Loss: 0.31471720337867737\n",
      "Step 246\n",
      "Loss: 0.3136436939239502\n",
      "Step 247\n",
      "Loss: 0.3125741481781006\n",
      "Step 248\n",
      "Loss: 0.31150856614112854\n",
      "Step 249\n",
      "Loss: 0.31044697761535645\n",
      "Step 250\n",
      "Loss: 0.3093894124031067\n",
      "Step 251\n",
      "Loss: 0.30833590030670166\n",
      "Step 252\n",
      "Loss: 0.30728641152381897\n",
      "Step 253\n",
      "Loss: 0.306240975856781\n",
      "Step 254\n",
      "Loss: 0.30519968271255493\n",
      "Step 255\n",
      "Loss: 0.3041624426841736\n",
      "Step 256\n",
      "Loss: 0.30312928557395935\n",
      "Step 257\n",
      "Loss: 0.302100270986557\n",
      "Step 258\n",
      "Loss: 0.30107539892196655\n",
      "Step 259\n",
      "Loss: 0.3000546395778656\n",
      "Step 260\n",
      "Loss: 0.2990380823612213\n",
      "Step 261\n",
      "Loss: 0.2980256676673889\n",
      "Step 262\n",
      "Loss: 0.2970174551010132\n",
      "Step 263\n",
      "Loss: 0.29601338505744934\n",
      "Step 264\n",
      "Loss: 0.29501354694366455\n",
      "Step 265\n",
      "Loss: 0.29401788115501404\n",
      "Step 266\n",
      "Loss: 0.29302650690078735\n",
      "Step 267\n",
      "Loss: 0.29203927516937256\n",
      "Step 268\n",
      "Loss: 0.2910563349723816\n",
      "Step 269\n",
      "Loss: 0.2900776267051697\n",
      "Step 270\n",
      "Loss: 0.2891031503677368\n",
      "Step 271\n",
      "Loss: 0.288132905960083\n",
      "Step 272\n",
      "Loss: 0.28716692328453064\n",
      "Step 273\n",
      "Loss: 0.2862052023410797\n",
      "Step 274\n",
      "Loss: 0.285247802734375\n",
      "Step 275\n",
      "Loss: 0.28429457545280457\n",
      "Step 276\n",
      "Loss: 0.2833457291126251\n",
      "Step 277\n",
      "Loss: 0.28240105509757996\n",
      "Step 278\n",
      "Loss: 0.2814607322216034\n",
      "Step 279\n",
      "Loss: 0.28052467107772827\n",
      "Step 280\n",
      "Loss: 0.279592901468277\n",
      "Step 281\n",
      "Loss: 0.2786654233932495\n",
      "Step 282\n",
      "Loss: 0.2777422070503235\n",
      "Step 283\n",
      "Loss: 0.2768232822418213\n",
      "Step 284\n",
      "Loss: 0.2759086787700653\n",
      "Step 285\n",
      "Loss: 0.27499833703041077\n",
      "Step 286\n",
      "Loss: 0.27409228682518005\n",
      "Step 287\n",
      "Loss: 0.27319052815437317\n",
      "Step 288\n",
      "Loss: 0.2722930610179901\n",
      "Step 289\n",
      "Loss: 0.2713998854160309\n",
      "Step 290\n",
      "Loss: 0.2705110013484955\n",
      "Step 291\n",
      "Loss: 0.2696263790130615\n",
      "Step 292\n",
      "Loss: 0.268746018409729\n",
      "Step 293\n",
      "Loss: 0.2678699791431427\n",
      "Step 294\n",
      "Loss: 0.26699820160865784\n",
      "Step 295\n",
      "Loss: 0.2661306858062744\n",
      "Step 296\n",
      "Loss: 0.26526740193367004\n",
      "Step 297\n",
      "Loss: 0.2644084095954895\n",
      "Step 298\n",
      "Loss: 0.2635536789894104\n",
      "Step 299\n",
      "Loss: 0.26270318031311035\n",
      "Step 300\n",
      "Loss: 0.26185694336891174\n",
      "Step 301\n",
      "Loss: 0.2610149681568146\n",
      "Step 302\n",
      "Loss: 0.2601771950721741\n",
      "Step 303\n",
      "Loss: 0.259343683719635\n",
      "Step 304\n",
      "Loss: 0.258514404296875\n",
      "Step 305\n",
      "Loss: 0.25768929719924927\n",
      "Step 306\n",
      "Loss: 0.2568684220314026\n",
      "Step 307\n",
      "Loss: 0.2560517489910126\n",
      "Step 308\n",
      "Loss: 0.25523924827575684\n",
      "Step 309\n",
      "Loss: 0.25443094968795776\n",
      "Step 310\n",
      "Loss: 0.25362685322761536\n",
      "Step 311\n",
      "Loss: 0.2528269290924072\n",
      "Step 312\n",
      "Loss: 0.252031147480011\n",
      "Step 313\n",
      "Loss: 0.251239538192749\n",
      "Step 314\n",
      "Loss: 0.25045207142829895\n",
      "Step 315\n",
      "Loss: 0.24966871738433838\n",
      "Step 316\n",
      "Loss: 0.24888953566551208\n",
      "Step 317\n",
      "Loss: 0.2481144219636917\n",
      "Step 318\n",
      "Loss: 0.24734345078468323\n",
      "Step 319\n",
      "Loss: 0.24657659232616425\n",
      "Step 320\n",
      "Loss: 0.24581381678581238\n",
      "Step 321\n",
      "Loss: 0.24505506455898285\n",
      "Step 322\n",
      "Loss: 0.24430042505264282\n",
      "Step 323\n",
      "Loss: 0.24354985356330872\n",
      "Step 324\n",
      "Loss: 0.24280333518981934\n",
      "Step 325\n",
      "Loss: 0.2420608252286911\n",
      "Step 326\n",
      "Loss: 0.2413223534822464\n",
      "Step 327\n",
      "Loss: 0.24058789014816284\n",
      "Step 328\n",
      "Loss: 0.23985743522644043\n",
      "Step 329\n",
      "Loss: 0.23913097381591797\n",
      "Step 330\n",
      "Loss: 0.23840844631195068\n",
      "Step 331\n",
      "Loss: 0.23768994212150574\n",
      "Step 332\n",
      "Loss: 0.23697537183761597\n",
      "Step 333\n",
      "Loss: 0.23626475036144257\n",
      "Step 334\n",
      "Loss: 0.23555803298950195\n",
      "Step 335\n",
      "Loss: 0.23485523462295532\n",
      "Step 336\n",
      "Loss: 0.23415635526180267\n",
      "Step 337\n",
      "Loss: 0.23346136510372162\n",
      "Step 338\n",
      "Loss: 0.23277024924755096\n",
      "Step 339\n",
      "Loss: 0.23208299279212952\n",
      "Step 340\n",
      "Loss: 0.23139959573745728\n",
      "Step 341\n",
      "Loss: 0.23072004318237305\n",
      "Step 342\n",
      "Loss: 0.23004430532455444\n",
      "Step 343\n",
      "Loss: 0.22937236726284027\n",
      "Step 344\n",
      "Loss: 0.22870422899723053\n",
      "Step 345\n",
      "Loss: 0.2280399054288864\n",
      "Step 346\n",
      "Loss: 0.22737930715084076\n",
      "Step 347\n",
      "Loss: 0.22672247886657715\n",
      "Step 348\n",
      "Loss: 0.22606942057609558\n",
      "Step 349\n",
      "Loss: 0.2254200428724289\n",
      "Step 350\n",
      "Loss: 0.22477442026138306\n",
      "Step 351\n",
      "Loss: 0.2241324782371521\n",
      "Step 352\n",
      "Loss: 0.22349423170089722\n",
      "Step 353\n",
      "Loss: 0.22285960614681244\n",
      "Step 354\n",
      "Loss: 0.22222867608070374\n",
      "Step 355\n",
      "Loss: 0.22160139679908752\n",
      "Step 356\n",
      "Loss: 0.22097770869731903\n",
      "Step 357\n",
      "Loss: 0.22035765647888184\n",
      "Step 358\n",
      "Loss: 0.21974116563796997\n",
      "Step 359\n",
      "Loss: 0.21912826597690582\n",
      "Step 360\n",
      "Loss: 0.2185189574956894\n",
      "Step 361\n",
      "Loss: 0.2179131805896759\n",
      "Step 362\n",
      "Loss: 0.21731093525886536\n",
      "Step 363\n",
      "Loss: 0.21671222150325775\n",
      "Step 364\n",
      "Loss: 0.2161169946193695\n",
      "Step 365\n",
      "Loss: 0.215525284409523\n",
      "Step 366\n",
      "Loss: 0.2149370014667511\n",
      "Step 367\n",
      "Loss: 0.21435222029685974\n",
      "Step 368\n",
      "Loss: 0.21377089619636536\n",
      "Step 369\n",
      "Loss: 0.21319293975830078\n",
      "Step 370\n",
      "Loss: 0.2126184105873108\n",
      "Step 371\n",
      "Loss: 0.21204732358455658\n",
      "Step 372\n",
      "Loss: 0.2114795744419098\n",
      "Step 373\n",
      "Loss: 0.2109152227640152\n",
      "Step 374\n",
      "Loss: 0.21035419404506683\n",
      "Step 375\n",
      "Loss: 0.2097964882850647\n",
      "Step 376\n",
      "Loss: 0.20924213528633118\n",
      "Step 377\n",
      "Loss: 0.2086910456418991\n",
      "Step 378\n",
      "Loss: 0.20814326405525208\n",
      "Step 379\n",
      "Loss: 0.2075987607240677\n",
      "Step 380\n",
      "Loss: 0.20705749094486237\n",
      "Step 381\n",
      "Loss: 0.2065194845199585\n",
      "Step 382\n",
      "Loss: 0.2059846818447113\n",
      "Step 383\n",
      "Loss: 0.20545309782028198\n",
      "Step 384\n",
      "Loss: 0.20492468774318695\n",
      "Step 385\n",
      "Loss: 0.2043994814157486\n",
      "Step 386\n",
      "Loss: 0.20387740433216095\n",
      "Step 387\n",
      "Loss: 0.2033585011959076\n",
      "Step 388\n",
      "Loss: 0.20284271240234375\n",
      "Step 389\n",
      "Loss: 0.20233005285263062\n",
      "Step 390\n",
      "Loss: 0.2018204629421234\n",
      "Step 391\n",
      "Loss: 0.20131395757198334\n",
      "Step 392\n",
      "Loss: 0.20081055164337158\n",
      "Step 393\n",
      "Loss: 0.20031017065048218\n",
      "Step 394\n",
      "Loss: 0.19981282949447632\n",
      "Step 395\n",
      "Loss: 0.19931849837303162\n",
      "Step 396\n",
      "Loss: 0.19882717728614807\n",
      "Step 397\n",
      "Loss: 0.1983388513326645\n",
      "Step 398\n",
      "Loss: 0.19785349071025848\n",
      "Step 399\n",
      "Loss: 0.19737106561660767\n",
      "Step 400\n",
      "Loss: 0.19689159095287323\n",
      "Step 401\n",
      "Loss: 0.19641505181789398\n",
      "Step 402\n",
      "Loss: 0.19594141840934753\n",
      "Step 403\n",
      "Loss: 0.1954706758260727\n",
      "Step 404\n",
      "Loss: 0.19500280916690826\n",
      "Step 405\n",
      "Loss: 0.19453781843185425\n",
      "Step 406\n",
      "Loss: 0.19407564401626587\n",
      "Step 407\n",
      "Loss: 0.19361631572246552\n",
      "Step 408\n",
      "Loss: 0.1931597888469696\n",
      "Step 409\n",
      "Loss: 0.19270609319210052\n",
      "Step 410\n",
      "Loss: 0.1922551691532135\n",
      "Step 411\n",
      "Loss: 0.19180698692798615\n",
      "Step 412\n",
      "Loss: 0.19136159121990204\n",
      "Step 413\n",
      "Loss: 0.1909189224243164\n",
      "Step 414\n",
      "Loss: 0.19047895073890686\n",
      "Step 415\n",
      "Loss: 0.19004172086715698\n",
      "Step 416\n",
      "Loss: 0.189607173204422\n",
      "Step 417\n",
      "Loss: 0.1891753077507019\n",
      "Step 418\n",
      "Loss: 0.18874609470367432\n",
      "Step 419\n",
      "Loss: 0.18831954896450043\n",
      "Step 420\n",
      "Loss: 0.18789561092853546\n",
      "Step 421\n",
      "Loss: 0.1874742954969406\n",
      "Step 422\n",
      "Loss: 0.1870555877685547\n",
      "Step 423\n",
      "Loss: 0.1866394579410553\n",
      "Step 424\n",
      "Loss: 0.18622590601444244\n",
      "Step 425\n",
      "Loss: 0.18581490218639374\n",
      "Step 426\n",
      "Loss: 0.18540644645690918\n",
      "Step 427\n",
      "Loss: 0.18500052392482758\n",
      "Step 428\n",
      "Loss: 0.18459710478782654\n",
      "Step 429\n",
      "Loss: 0.18419618904590607\n",
      "Step 430\n",
      "Loss: 0.18379774689674377\n",
      "Step 431\n",
      "Loss: 0.18340177834033966\n",
      "Step 432\n",
      "Loss: 0.18300826847553253\n",
      "Step 433\n",
      "Loss: 0.1826171875\n",
      "Step 434\n",
      "Loss: 0.18222855031490326\n",
      "Step 435\n",
      "Loss: 0.18184231221675873\n",
      "Step 436\n",
      "Loss: 0.1814584732055664\n",
      "Step 437\n",
      "Loss: 0.1810770183801651\n",
      "Step 438\n",
      "Loss: 0.18069791793823242\n",
      "Step 439\n",
      "Loss: 0.18032117187976837\n",
      "Step 440\n",
      "Loss: 0.17994678020477295\n",
      "Step 441\n",
      "Loss: 0.17957469820976257\n",
      "Step 442\n",
      "Loss: 0.17920492589473724\n",
      "Step 443\n",
      "Loss: 0.17883747816085815\n",
      "Step 444\n",
      "Loss: 0.17847228050231934\n",
      "Step 445\n",
      "Loss: 0.17810934782028198\n",
      "Step 446\n",
      "Loss: 0.1777486950159073\n",
      "Step 447\n",
      "Loss: 0.17739029228687286\n",
      "Step 448\n",
      "Loss: 0.17703408002853394\n",
      "Step 449\n",
      "Loss: 0.17668010294437408\n",
      "Step 450\n",
      "Loss: 0.17632831633090973\n",
      "Step 451\n",
      "Loss: 0.17597872018814087\n",
      "Step 452\n",
      "Loss: 0.1756312996149063\n",
      "Step 453\n",
      "Loss: 0.17528603971004486\n",
      "Step 454\n",
      "Loss: 0.17494291067123413\n",
      "Step 455\n",
      "Loss: 0.1746019423007965\n",
      "Step 456\n",
      "Loss: 0.1742630898952484\n",
      "Step 457\n",
      "Loss: 0.17392632365226746\n",
      "Step 458\n",
      "Loss: 0.17359164357185364\n",
      "Step 459\n",
      "Loss: 0.17325904965400696\n",
      "Step 460\n",
      "Loss: 0.17292854189872742\n",
      "Step 461\n",
      "Loss: 0.17260007560253143\n",
      "Step 462\n",
      "Loss: 0.172273650765419\n",
      "Step 463\n",
      "Loss: 0.17194926738739014\n",
      "Step 464\n",
      "Loss: 0.17162688076496124\n",
      "Step 465\n",
      "Loss: 0.17130649089813232\n",
      "Step 466\n",
      "Loss: 0.17098809778690338\n",
      "Step 467\n",
      "Loss: 0.17067168653011322\n",
      "Step 468\n",
      "Loss: 0.17035724222660065\n",
      "Step 469\n",
      "Loss: 0.17004474997520447\n",
      "Step 470\n",
      "Loss: 0.1697341799736023\n",
      "Step 471\n",
      "Loss: 0.16942554712295532\n",
      "Step 472\n",
      "Loss: 0.16911882162094116\n",
      "Step 473\n",
      "Loss: 0.16881400346755981\n",
      "Step 474\n",
      "Loss: 0.1685110628604889\n",
      "Step 475\n",
      "Loss: 0.1682099997997284\n",
      "Step 476\n",
      "Loss: 0.16791082918643951\n",
      "Step 477\n",
      "Loss: 0.16761349141597748\n",
      "Step 478\n",
      "Loss: 0.16731800138950348\n",
      "Step 479\n",
      "Loss: 0.16702434420585632\n",
      "Step 480\n",
      "Loss: 0.16673250496387482\n",
      "Step 481\n",
      "Loss: 0.16644245386123657\n",
      "Step 482\n",
      "Loss: 0.16615420579910278\n",
      "Step 483\n",
      "Loss: 0.16586773097515106\n",
      "Step 484\n",
      "Loss: 0.1655830442905426\n",
      "Step 485\n",
      "Loss: 0.16530010104179382\n",
      "Step 486\n",
      "Loss: 0.16501890122890472\n",
      "Step 487\n",
      "Loss: 0.1647394597530365\n",
      "Step 488\n",
      "Loss: 0.16446174681186676\n",
      "Step 489\n",
      "Loss: 0.16418570280075073\n",
      "Step 490\n",
      "Loss: 0.1639113873243332\n",
      "Step 491\n",
      "Loss: 0.16363875567913055\n",
      "Step 492\n",
      "Loss: 0.16336782276630402\n",
      "Step 493\n",
      "Loss: 0.16309854388237\n",
      "Step 494\n",
      "Loss: 0.1628309041261673\n",
      "Step 495\n",
      "Loss: 0.1625649333000183\n",
      "Step 496\n",
      "Loss: 0.16230060160160065\n",
      "Step 497\n",
      "Loss: 0.16203784942626953\n",
      "Step 498\n",
      "Loss: 0.16177675127983093\n",
      "Step 499\n",
      "Loss: 0.16151724755764008\n",
      "Step 500\n",
      "Loss: 0.16125933825969696\n",
      "Step 501\n",
      "Loss: 0.1610029935836792\n",
      "Step 502\n",
      "Loss: 0.16074822843074799\n",
      "Step 503\n",
      "Loss: 0.16049504280090332\n",
      "Step 504\n",
      "Loss: 0.16024337708950043\n",
      "Step 505\n",
      "Loss: 0.1599932610988617\n",
      "Step 506\n",
      "Loss: 0.15974466502666473\n",
      "Step 507\n",
      "Loss: 0.15949760377407074\n",
      "Step 508\n",
      "Loss: 0.15925203263759613\n",
      "Step 509\n",
      "Loss: 0.1590079665184021\n",
      "Step 510\n",
      "Loss: 0.15876539051532745\n",
      "Step 511\n",
      "Loss: 0.1585242748260498\n",
      "Step 512\n",
      "Loss: 0.15828466415405273\n",
      "Step 513\n",
      "Loss: 0.15804646909236908\n",
      "Step 514\n",
      "Loss: 0.15780974924564362\n",
      "Step 515\n",
      "Loss: 0.15757444500923157\n",
      "Step 516\n",
      "Loss: 0.15734057128429413\n",
      "Step 517\n",
      "Loss: 0.1571081280708313\n",
      "Step 518\n",
      "Loss: 0.15687710046768188\n",
      "Step 519\n",
      "Loss: 0.1566474437713623\n",
      "Step 520\n",
      "Loss: 0.15641920268535614\n",
      "Step 521\n",
      "Loss: 0.15619230270385742\n",
      "Step 522\n",
      "Loss: 0.15596681833267212\n",
      "Step 523\n",
      "Loss: 0.15574267506599426\n",
      "Step 524\n",
      "Loss: 0.15551988780498505\n",
      "Step 525\n",
      "Loss: 0.15529842674732208\n",
      "Step 526\n",
      "Loss: 0.15507830679416656\n",
      "Step 527\n",
      "Loss: 0.1548595279455185\n",
      "Step 528\n",
      "Loss: 0.1546420305967331\n",
      "Step 529\n",
      "Loss: 0.15442585945129395\n",
      "Step 530\n",
      "Loss: 0.15421096980571747\n",
      "Step 531\n",
      "Loss: 0.15399737656116486\n",
      "Step 532\n",
      "Loss: 0.15378506481647491\n",
      "Step 533\n",
      "Loss: 0.15357400476932526\n",
      "Step 534\n",
      "Loss: 0.15336422622203827\n",
      "Step 535\n",
      "Loss: 0.15315568447113037\n",
      "Step 536\n",
      "Loss: 0.15294840931892395\n",
      "Step 537\n",
      "Loss: 0.15274232625961304\n",
      "Step 538\n",
      "Loss: 0.1525375247001648\n",
      "Step 539\n",
      "Loss: 0.15233388543128967\n",
      "Step 540\n",
      "Loss: 0.15213148295879364\n",
      "Step 541\n",
      "Loss: 0.15193027257919312\n",
      "Step 542\n",
      "Loss: 0.1517302691936493\n",
      "Step 543\n",
      "Loss: 0.1515314131975174\n",
      "Step 544\n",
      "Loss: 0.1513337641954422\n",
      "Step 545\n",
      "Loss: 0.15113727748394012\n",
      "Step 546\n",
      "Loss: 0.15094193816184998\n",
      "Step 547\n",
      "Loss: 0.15074774622917175\n",
      "Step 548\n",
      "Loss: 0.15055471658706665\n",
      "Step 549\n",
      "Loss: 0.1503628045320511\n",
      "Step 550\n",
      "Loss: 0.15017202496528625\n",
      "Step 551\n",
      "Loss: 0.14998234808444977\n",
      "Step 552\n",
      "Loss: 0.149793803691864\n",
      "Step 553\n",
      "Loss: 0.1496063619852066\n",
      "Step 554\n",
      "Loss: 0.14942000806331635\n",
      "Step 555\n",
      "Loss: 0.14923474192619324\n",
      "Step 556\n",
      "Loss: 0.1490505486726761\n",
      "Step 557\n",
      "Loss: 0.1488674283027649\n",
      "Step 558\n",
      "Loss: 0.14868536591529846\n",
      "Step 559\n",
      "Loss: 0.148504376411438\n",
      "Step 560\n",
      "Loss: 0.14832442998886108\n",
      "Step 561\n",
      "Loss: 0.14814552664756775\n",
      "Step 562\n",
      "Loss: 0.1479676514863968\n",
      "Step 563\n",
      "Loss: 0.147790789604187\n",
      "Step 564\n",
      "Loss: 0.1476149559020996\n",
      "Step 565\n",
      "Loss: 0.1474401354789734\n",
      "Step 566\n",
      "Loss: 0.14726632833480835\n",
      "Step 567\n",
      "Loss: 0.1470935195684433\n",
      "Step 568\n",
      "Loss: 0.14692167937755585\n",
      "Step 569\n",
      "Loss: 0.14675085246562958\n",
      "Step 570\n",
      "Loss: 0.14658097922801971\n",
      "Step 571\n",
      "Loss: 0.14641207456588745\n",
      "Step 572\n",
      "Loss: 0.1462441384792328\n",
      "Step 573\n",
      "Loss: 0.14607717096805573\n",
      "Step 574\n",
      "Loss: 0.14591112732887268\n",
      "Step 575\n",
      "Loss: 0.14574603736400604\n",
      "Step 576\n",
      "Loss: 0.14558188617229462\n",
      "Step 577\n",
      "Loss: 0.1454186588525772\n",
      "Step 578\n",
      "Loss: 0.14525634050369263\n",
      "Step 579\n",
      "Loss: 0.14509496092796326\n",
      "Step 580\n",
      "Loss: 0.14493447542190552\n",
      "Step 581\n",
      "Loss: 0.1447748988866806\n",
      "Step 582\n",
      "Loss: 0.14461620151996613\n",
      "Step 583\n",
      "Loss: 0.14445841312408447\n",
      "Step 584\n",
      "Loss: 0.14430150389671326\n",
      "Step 585\n",
      "Loss: 0.14414545893669128\n",
      "Step 586\n",
      "Loss: 0.14399026334285736\n",
      "Step 587\n",
      "Loss: 0.14383597671985626\n",
      "Step 588\n",
      "Loss: 0.14368250966072083\n",
      "Step 589\n",
      "Loss: 0.14352990686893463\n",
      "Step 590\n",
      "Loss: 0.1433781534433365\n",
      "Step 591\n",
      "Loss: 0.143227219581604\n",
      "Step 592\n",
      "Loss: 0.14307713508605957\n",
      "Step 593\n",
      "Loss: 0.142927885055542\n",
      "Step 594\n",
      "Loss: 0.1427794247865677\n",
      "Step 595\n",
      "Loss: 0.14263179898262024\n",
      "Step 596\n",
      "Loss: 0.14248494803905487\n",
      "Step 597\n",
      "Loss: 0.14233894646167755\n",
      "Step 598\n",
      "Loss: 0.1421937197446823\n",
      "Step 599\n",
      "Loss: 0.14204929769039154\n",
      "Step 600\n",
      "Loss: 0.14190563559532166\n",
      "Step 601\n",
      "Loss: 0.14176276326179504\n",
      "Step 602\n",
      "Loss: 0.1416206657886505\n",
      "Step 603\n",
      "Loss: 0.14147934317588806\n",
      "Step 604\n",
      "Loss: 0.1413387656211853\n",
      "Step 605\n",
      "Loss: 0.14119894802570343\n",
      "Step 606\n",
      "Loss: 0.14105987548828125\n",
      "Step 607\n",
      "Loss: 0.14092156291007996\n",
      "Step 608\n",
      "Loss: 0.14078398048877716\n",
      "Step 609\n",
      "Loss: 0.14064712822437286\n",
      "Step 610\n",
      "Loss: 0.14051100611686707\n",
      "Step 611\n",
      "Loss: 0.14037561416625977\n",
      "Step 612\n",
      "Loss: 0.14024092257022858\n",
      "Step 613\n",
      "Loss: 0.1401069611310959\n",
      "Step 614\n",
      "Loss: 0.1399737000465393\n",
      "Step 615\n",
      "Loss: 0.13984113931655884\n",
      "Step 616\n",
      "Loss: 0.13970929384231567\n",
      "Step 617\n",
      "Loss: 0.13957810401916504\n",
      "Step 618\n",
      "Loss: 0.1394476294517517\n",
      "Step 619\n",
      "Loss: 0.1393178254365921\n",
      "Step 620\n",
      "Loss: 0.1391887068748474\n",
      "Step 621\n",
      "Loss: 0.13906022906303406\n",
      "Step 622\n",
      "Loss: 0.13893245160579681\n",
      "Step 623\n",
      "Loss: 0.1388053297996521\n",
      "Step 624\n",
      "Loss: 0.13867883384227753\n",
      "Step 625\n",
      "Loss: 0.13855303823947906\n",
      "Step 626\n",
      "Loss: 0.13842785358428955\n",
      "Step 627\n",
      "Loss: 0.13830330967903137\n",
      "Step 628\n",
      "Loss: 0.13817942142486572\n",
      "Step 629\n",
      "Loss: 0.13805615901947021\n",
      "Step 630\n",
      "Loss: 0.13793349266052246\n",
      "Step 631\n",
      "Loss: 0.13781146705150604\n",
      "Step 632\n",
      "Loss: 0.13769009709358215\n",
      "Step 633\n",
      "Loss: 0.13756930828094482\n",
      "Step 634\n",
      "Loss: 0.13744913041591644\n",
      "Step 635\n",
      "Loss: 0.137329563498497\n",
      "Step 636\n",
      "Loss: 0.13721057772636414\n",
      "Step 637\n",
      "Loss: 0.13709218800067902\n",
      "Step 638\n",
      "Loss: 0.13697439432144165\n",
      "Step 639\n",
      "Loss: 0.13685718178749084\n",
      "Step 640\n",
      "Loss: 0.136740580201149\n",
      "Step 641\n",
      "Loss: 0.1366245299577713\n",
      "Step 642\n",
      "Loss: 0.13650904595851898\n",
      "Step 643\n",
      "Loss: 0.13639414310455322\n",
      "Step 644\n",
      "Loss: 0.13627979159355164\n",
      "Step 645\n",
      "Loss: 0.13616600632667542\n",
      "Step 646\n",
      "Loss: 0.13605278730392456\n",
      "Step 647\n",
      "Loss: 0.13594010472297668\n",
      "Step 648\n",
      "Loss: 0.13582797348499298\n",
      "Step 649\n",
      "Loss: 0.13571637868881226\n",
      "Step 650\n",
      "Loss: 0.1356053352355957\n",
      "Step 651\n",
      "Loss: 0.13549482822418213\n",
      "Step 652\n",
      "Loss: 0.13538482785224915\n",
      "Step 653\n",
      "Loss: 0.13527537882328033\n",
      "Step 654\n",
      "Loss: 0.1351664513349533\n",
      "Step 655\n",
      "Loss: 0.13505801558494568\n",
      "Step 656\n",
      "Loss: 0.13495013117790222\n",
      "Step 657\n",
      "Loss: 0.13484273850917816\n",
      "Step 658\n",
      "Loss: 0.1347358375787735\n",
      "Step 659\n",
      "Loss: 0.13462945818901062\n",
      "Step 660\n",
      "Loss: 0.13452357053756714\n",
      "Step 661\n",
      "Loss: 0.13441817462444305\n",
      "Step 662\n",
      "Loss: 0.13431328535079956\n",
      "Step 663\n",
      "Loss: 0.13420887291431427\n",
      "Step 664\n",
      "Loss: 0.13410493731498718\n",
      "Step 665\n",
      "Loss: 0.1340014934539795\n",
      "Step 666\n",
      "Loss: 0.13389849662780762\n",
      "Step 667\n",
      "Loss: 0.13379600644111633\n",
      "Step 668\n",
      "Loss: 0.13369397819042206\n",
      "Step 669\n",
      "Loss: 0.1335923969745636\n",
      "Step 670\n",
      "Loss: 0.13349129259586334\n",
      "Step 671\n",
      "Loss: 0.1333906501531601\n",
      "Step 672\n",
      "Loss: 0.13329043984413147\n",
      "Step 673\n",
      "Loss: 0.13319070637226105\n",
      "Step 674\n",
      "Loss: 0.13309140503406525\n",
      "Step 675\n",
      "Loss: 0.13299256563186646\n",
      "Step 676\n",
      "Loss: 0.1328941434621811\n",
      "Step 677\n",
      "Loss: 0.13279616832733154\n",
      "Step 678\n",
      "Loss: 0.13269862532615662\n",
      "Step 679\n",
      "Loss: 0.13260149955749512\n",
      "Step 680\n",
      "Loss: 0.13250482082366943\n",
      "Step 681\n",
      "Loss: 0.13240854442119598\n",
      "Step 682\n",
      "Loss: 0.13231270015239716\n",
      "Step 683\n",
      "Loss: 0.13221728801727295\n",
      "Step 684\n",
      "Loss: 0.1321222484111786\n",
      "Step 685\n",
      "Loss: 0.13202767074108124\n",
      "Step 686\n",
      "Loss: 0.13193346560001373\n",
      "Step 687\n",
      "Loss: 0.13183967769145966\n",
      "Step 688\n",
      "Loss: 0.13174627721309662\n",
      "Step 689\n",
      "Loss: 0.13165327906608582\n",
      "Step 690\n",
      "Loss: 0.13156068325042725\n",
      "Step 691\n",
      "Loss: 0.13146847486495972\n",
      "Step 692\n",
      "Loss: 0.13137665390968323\n",
      "Step 693\n",
      "Loss: 0.13128520548343658\n",
      "Step 694\n",
      "Loss: 0.13119417428970337\n",
      "Step 695\n",
      "Loss: 0.1311035007238388\n",
      "Step 696\n",
      "Loss: 0.1310131698846817\n",
      "Step 697\n",
      "Loss: 0.13092325627803802\n",
      "Step 698\n",
      "Loss: 0.130833700299263\n",
      "Step 699\n",
      "Loss: 0.13074450194835663\n",
      "Step 700\n",
      "Loss: 0.1306556761264801\n",
      "Step 701\n",
      "Loss: 0.13056720793247223\n",
      "Step 702\n",
      "Loss: 0.130479097366333\n",
      "Step 703\n",
      "Loss: 0.13039134442806244\n",
      "Step 704\n",
      "Loss: 0.13030393421649933\n",
      "Step 705\n",
      "Loss: 0.13021688163280487\n",
      "Step 706\n",
      "Loss: 0.13013017177581787\n",
      "Step 707\n",
      "Loss: 0.13004381954669952\n",
      "Step 708\n",
      "Loss: 0.12995779514312744\n",
      "Step 709\n",
      "Loss: 0.12987211346626282\n",
      "Step 710\n",
      "Loss: 0.12978677451610565\n",
      "Step 711\n",
      "Loss: 0.12970177829265594\n",
      "Step 712\n",
      "Loss: 0.12961708009243011\n",
      "Step 713\n",
      "Loss: 0.12953273952007294\n",
      "Step 714\n",
      "Loss: 0.12944871187210083\n",
      "Step 715\n",
      "Loss: 0.1293649822473526\n",
      "Step 716\n",
      "Loss: 0.12928159534931183\n",
      "Step 717\n",
      "Loss: 0.12919853627681732\n",
      "Step 718\n",
      "Loss: 0.12911579012870789\n",
      "Step 719\n",
      "Loss: 0.12903335690498352\n",
      "Step 720\n",
      "Loss: 0.12895120680332184\n",
      "Step 721\n",
      "Loss: 0.12886939942836761\n",
      "Step 722\n",
      "Loss: 0.1287878453731537\n",
      "Step 723\n",
      "Loss: 0.12870663404464722\n",
      "Step 724\n",
      "Loss: 0.12862572073936462\n",
      "Step 725\n",
      "Loss: 0.1285451054573059\n",
      "Step 726\n",
      "Loss: 0.12846477329730988\n",
      "Step 727\n",
      "Loss: 0.12838473916053772\n",
      "Step 728\n",
      "Loss: 0.12830498814582825\n",
      "Step 729\n",
      "Loss: 0.12822553515434265\n",
      "Step 730\n",
      "Loss: 0.12814635038375854\n",
      "Step 731\n",
      "Loss: 0.1280674785375595\n",
      "Step 732\n",
      "Loss: 0.12798886001110077\n",
      "Step 733\n",
      "Loss: 0.12791050970554352\n",
      "Step 734\n",
      "Loss: 0.12783245742321014\n",
      "Step 735\n",
      "Loss: 0.12775467336177826\n",
      "Step 736\n",
      "Loss: 0.12767715752124786\n",
      "Step 737\n",
      "Loss: 0.12759992480278015\n",
      "Step 738\n",
      "Loss: 0.12752293050289154\n",
      "Step 739\n",
      "Loss: 0.1274462342262268\n",
      "Step 740\n",
      "Loss: 0.12736979126930237\n",
      "Step 741\n",
      "Loss: 0.12729360163211823\n",
      "Step 742\n",
      "Loss: 0.12721765041351318\n",
      "Step 743\n",
      "Loss: 0.12714198231697083\n",
      "Step 744\n",
      "Loss: 0.12706655263900757\n",
      "Step 745\n",
      "Loss: 0.1269913613796234\n",
      "Step 746\n",
      "Loss: 0.12691646814346313\n",
      "Step 747\n",
      "Loss: 0.12684178352355957\n",
      "Step 748\n",
      "Loss: 0.1267673522233963\n",
      "Step 749\n",
      "Loss: 0.12669315934181213\n",
      "Step 750\n",
      "Loss: 0.12661920487880707\n",
      "Step 751\n",
      "Loss: 0.1265455037355423\n",
      "Step 752\n",
      "Loss: 0.12647204101085663\n",
      "Step 753\n",
      "Loss: 0.12639881670475006\n",
      "Step 754\n",
      "Loss: 0.126325786113739\n",
      "Step 755\n",
      "Loss: 0.12625300884246826\n",
      "Step 756\n",
      "Loss: 0.1261804848909378\n",
      "Step 757\n",
      "Loss: 0.12610816955566406\n",
      "Step 758\n",
      "Loss: 0.12603607773780823\n",
      "Step 759\n",
      "Loss: 0.1259642094373703\n",
      "Step 760\n",
      "Loss: 0.12589256465435028\n",
      "Step 761\n",
      "Loss: 0.12582114338874817\n",
      "Step 762\n",
      "Loss: 0.12574991583824158\n",
      "Step 763\n",
      "Loss: 0.12567894160747528\n",
      "Step 764\n",
      "Loss: 0.1256081759929657\n",
      "Step 765\n",
      "Loss: 0.12553758919239044\n",
      "Step 766\n",
      "Loss: 0.1254672110080719\n",
      "Step 767\n",
      "Loss: 0.12539708614349365\n",
      "Step 768\n",
      "Loss: 0.12532714009284973\n",
      "Step 769\n",
      "Loss: 0.12525738775730133\n",
      "Step 770\n",
      "Loss: 0.12518785893917084\n",
      "Step 771\n",
      "Loss: 0.12511850893497467\n",
      "Step 772\n",
      "Loss: 0.1250493824481964\n",
      "Step 773\n",
      "Loss: 0.12498044222593307\n",
      "Step 774\n",
      "Loss: 0.12491169571876526\n",
      "Step 775\n",
      "Loss: 0.12484315037727356\n",
      "Step 776\n",
      "Loss: 0.12477479875087738\n",
      "Step 777\n",
      "Loss: 0.12470662593841553\n",
      "Step 778\n",
      "Loss: 0.1246386393904686\n",
      "Step 779\n",
      "Loss: 0.12457084655761719\n",
      "Step 780\n",
      "Loss: 0.12450326234102249\n",
      "Step 781\n",
      "Loss: 0.12443584203720093\n",
      "Step 782\n",
      "Loss: 0.12436861544847488\n",
      "Step 783\n",
      "Loss: 0.12430156022310257\n",
      "Step 784\n",
      "Loss: 0.12423466145992279\n",
      "Step 785\n",
      "Loss: 0.12416796386241913\n",
      "Step 786\n",
      "Loss: 0.12410144507884979\n",
      "Step 787\n",
      "Loss: 0.12403512001037598\n",
      "Step 788\n",
      "Loss: 0.1239689365029335\n",
      "Step 789\n",
      "Loss: 0.12390293180942535\n",
      "Step 790\n",
      "Loss: 0.12383709847927094\n",
      "Step 791\n",
      "Loss: 0.12377143651247025\n",
      "Step 792\n",
      "Loss: 0.1237059235572815\n",
      "Step 793\n",
      "Loss: 0.12364060431718826\n",
      "Step 794\n",
      "Loss: 0.12357542663812637\n",
      "Step 795\n",
      "Loss: 0.1235104501247406\n",
      "Step 796\n",
      "Loss: 0.12344557046890259\n",
      "Step 797\n",
      "Loss: 0.12338089942932129\n",
      "Step 798\n",
      "Loss: 0.12331636995077133\n",
      "Step 799\n",
      "Loss: 0.12325199693441391\n",
      "Step 800\n",
      "Loss: 0.12318778783082962\n",
      "Step 801\n",
      "Loss: 0.12312372773885727\n",
      "Step 802\n",
      "Loss: 0.12305981665849686\n",
      "Step 803\n",
      "Loss: 0.12299606204032898\n",
      "Step 804\n",
      "Loss: 0.12293243408203125\n",
      "Step 805\n",
      "Loss: 0.12286899983882904\n",
      "Step 806\n",
      "Loss: 0.12280566990375519\n",
      "Step 807\n",
      "Loss: 0.12274252623319626\n",
      "Step 808\n",
      "Loss: 0.12267947196960449\n",
      "Step 809\n",
      "Loss: 0.12261659651994705\n",
      "Step 810\n",
      "Loss: 0.12255385518074036\n",
      "Step 811\n",
      "Loss: 0.1224912703037262\n",
      "Step 812\n",
      "Loss: 0.12242881208658218\n",
      "Step 813\n",
      "Loss: 0.12236647307872772\n",
      "Step 814\n",
      "Loss: 0.1223042905330658\n",
      "Step 815\n",
      "Loss: 0.12224224209785461\n",
      "Step 816\n",
      "Loss: 0.12218033522367477\n",
      "Step 817\n",
      "Loss: 0.12211855500936508\n",
      "Step 818\n",
      "Loss: 0.12205687165260315\n",
      "Step 819\n",
      "Loss: 0.12199535220861435\n",
      "Step 820\n",
      "Loss: 0.1219339519739151\n",
      "Step 821\n",
      "Loss: 0.12187270075082779\n",
      "Step 822\n",
      "Loss: 0.12181153893470764\n",
      "Step 823\n",
      "Loss: 0.12175054103136063\n",
      "Step 824\n",
      "Loss: 0.12168965488672256\n",
      "Step 825\n",
      "Loss: 0.12162886559963226\n",
      "Step 826\n",
      "Loss: 0.1215682253241539\n",
      "Step 827\n",
      "Loss: 0.12150771170854568\n",
      "Step 828\n",
      "Loss: 0.12144726514816284\n",
      "Step 829\n",
      "Loss: 0.12138697504997253\n",
      "Step 830\n",
      "Loss: 0.12132682651281357\n",
      "Step 831\n",
      "Loss: 0.12126676738262177\n",
      "Step 832\n",
      "Loss: 0.12120680510997772\n",
      "Step 833\n",
      "Loss: 0.12114697694778442\n",
      "Step 834\n",
      "Loss: 0.12108726799488068\n",
      "Step 835\n",
      "Loss: 0.12102766335010529\n",
      "Step 836\n",
      "Loss: 0.12096819281578064\n",
      "Step 837\n",
      "Loss: 0.12090878933668137\n",
      "Step 838\n",
      "Loss: 0.12084950506687164\n",
      "Step 839\n",
      "Loss: 0.12079036235809326\n",
      "Step 840\n",
      "Loss: 0.12073128670454025\n",
      "Step 841\n",
      "Loss: 0.1206723302602768\n",
      "Step 842\n",
      "Loss: 0.12061350047588348\n",
      "Step 843\n",
      "Loss: 0.12055474519729614\n",
      "Step 844\n",
      "Loss: 0.12049607932567596\n",
      "Step 845\n",
      "Loss: 0.12043754756450653\n",
      "Step 846\n",
      "Loss: 0.12037909775972366\n",
      "Step 847\n",
      "Loss: 0.12032076716423035\n",
      "Step 848\n",
      "Loss: 0.1202625185251236\n",
      "Step 849\n",
      "Loss: 0.12020434439182281\n",
      "Step 850\n",
      "Loss: 0.1201462671160698\n",
      "Step 851\n",
      "Loss: 0.12008830159902573\n",
      "Step 852\n",
      "Loss: 0.12003042548894882\n",
      "Step 853\n",
      "Loss: 0.11997267603874207\n",
      "Step 854\n",
      "Loss: 0.11991500109434128\n",
      "Step 855\n",
      "Loss: 0.11985744535923004\n",
      "Step 856\n",
      "Loss: 0.11979992687702179\n",
      "Step 857\n",
      "Loss: 0.11974252760410309\n",
      "Step 858\n",
      "Loss: 0.11968521028757095\n",
      "Step 859\n",
      "Loss: 0.11962798237800598\n",
      "Step 860\n",
      "Loss: 0.11957081407308578\n",
      "Step 861\n",
      "Loss: 0.11951374262571335\n",
      "Step 862\n",
      "Loss: 0.11945679783821106\n",
      "Step 863\n",
      "Loss: 0.11939990520477295\n",
      "Step 864\n",
      "Loss: 0.1193430945277214\n",
      "Step 865\n",
      "Loss: 0.11928637325763702\n",
      "Step 866\n",
      "Loss: 0.11922971904277802\n",
      "Step 867\n",
      "Loss: 0.11917318403720856\n",
      "Step 868\n",
      "Loss: 0.11911667138338089\n",
      "Step 869\n",
      "Loss: 0.11906029284000397\n",
      "Step 870\n",
      "Loss: 0.11900398135185242\n",
      "Step 871\n",
      "Loss: 0.11894772946834564\n",
      "Step 872\n",
      "Loss: 0.11889155209064484\n",
      "Step 873\n",
      "Loss: 0.11883547157049179\n",
      "Step 874\n",
      "Loss: 0.11877945065498352\n",
      "Step 875\n",
      "Loss: 0.11872348934412003\n",
      "Step 876\n",
      "Loss: 0.11866764724254608\n",
      "Step 877\n",
      "Loss: 0.11861183494329453\n",
      "Step 878\n",
      "Loss: 0.11855614185333252\n",
      "Step 879\n",
      "Loss: 0.11850045621395111\n",
      "Step 880\n",
      "Loss: 0.11844490468502045\n",
      "Step 881\n",
      "Loss: 0.11838935315608978\n",
      "Step 882\n",
      "Loss: 0.11833396553993225\n",
      "Step 883\n",
      "Loss: 0.11827858537435532\n",
      "Step 884\n",
      "Loss: 0.11822328716516495\n",
      "Step 885\n",
      "Loss: 0.11816802620887756\n",
      "Step 886\n",
      "Loss: 0.11811283230781555\n",
      "Step 887\n",
      "Loss: 0.1180577501654625\n",
      "Step 888\n",
      "Loss: 0.11800272017717361\n",
      "Step 889\n",
      "Loss: 0.11794774979352951\n",
      "Step 890\n",
      "Loss: 0.11789281666278839\n",
      "Step 891\n",
      "Loss: 0.11783799529075623\n",
      "Step 892\n",
      "Loss: 0.11778321862220764\n",
      "Step 893\n",
      "Loss: 0.11772850155830383\n",
      "Step 894\n",
      "Loss: 0.1176738366484642\n",
      "Step 895\n",
      "Loss: 0.11761920899152756\n",
      "Step 896\n",
      "Loss: 0.11756468564271927\n",
      "Step 897\n",
      "Loss: 0.11751015484333038\n",
      "Step 898\n",
      "Loss: 0.11745573580265045\n",
      "Step 899\n",
      "Loss: 0.11740140616893768\n",
      "Step 900\n",
      "Loss: 0.11734708398580551\n",
      "Step 901\n",
      "Loss: 0.11729281395673752\n",
      "Step 902\n",
      "Loss: 0.11723862588405609\n",
      "Step 903\n",
      "Loss: 0.11718445271253586\n",
      "Step 904\n",
      "Loss: 0.11713038384914398\n",
      "Step 905\n",
      "Loss: 0.11707635223865509\n",
      "Step 906\n",
      "Loss: 0.11702241748571396\n",
      "Step 907\n",
      "Loss: 0.11696843057870865\n",
      "Step 908\n",
      "Loss: 0.1169145405292511\n",
      "Step 909\n",
      "Loss: 0.1168607547879219\n",
      "Step 910\n",
      "Loss: 0.1168069839477539\n",
      "Step 911\n",
      "Loss: 0.11675325781106949\n",
      "Step 912\n",
      "Loss: 0.11669963598251343\n",
      "Step 913\n",
      "Loss: 0.11664599180221558\n",
      "Step 914\n",
      "Loss: 0.11659238487482071\n",
      "Step 915\n",
      "Loss: 0.11653885990381241\n",
      "Step 916\n",
      "Loss: 0.11648544669151306\n",
      "Step 917\n",
      "Loss: 0.11643201857805252\n",
      "Step 918\n",
      "Loss: 0.11637861281633377\n",
      "Step 919\n",
      "Loss: 0.11632530391216278\n",
      "Step 920\n",
      "Loss: 0.1162719875574112\n",
      "Step 921\n",
      "Loss: 0.11621875315904617\n",
      "Step 922\n",
      "Loss: 0.11616556346416473\n",
      "Step 923\n",
      "Loss: 0.11611239612102509\n",
      "Step 924\n",
      "Loss: 0.11605934053659439\n",
      "Step 925\n",
      "Loss: 0.1160062924027443\n",
      "Step 926\n",
      "Loss: 0.11595320701599121\n",
      "Step 927\n",
      "Loss: 0.11590023338794708\n",
      "Step 928\n",
      "Loss: 0.11584731191396713\n",
      "Step 929\n",
      "Loss: 0.11579441279172897\n",
      "Step 930\n",
      "Loss: 0.11574156582355499\n",
      "Step 931\n",
      "Loss: 0.11568878591060638\n",
      "Step 932\n",
      "Loss: 0.11563602089881897\n",
      "Step 933\n",
      "Loss: 0.11558324098587036\n",
      "Step 934\n",
      "Loss: 0.1155306026339531\n",
      "Step 935\n",
      "Loss: 0.11547794193029404\n",
      "Step 936\n",
      "Loss: 0.11542531847953796\n",
      "Step 937\n",
      "Loss: 0.11537279188632965\n",
      "Step 938\n",
      "Loss: 0.11532025039196014\n",
      "Step 939\n",
      "Loss: 0.11526768654584885\n",
      "Step 940\n",
      "Loss: 0.11521530896425247\n",
      "Step 941\n",
      "Loss: 0.11516286432743073\n",
      "Step 942\n",
      "Loss: 0.11511046439409256\n",
      "Step 943\n",
      "Loss: 0.11505810171365738\n",
      "Step 944\n",
      "Loss: 0.11500576883554459\n",
      "Step 945\n",
      "Loss: 0.11495350301265717\n",
      "Step 946\n",
      "Loss: 0.11490125954151154\n",
      "Step 947\n",
      "Loss: 0.11484910547733307\n",
      "Step 948\n",
      "Loss: 0.11479692906141281\n",
      "Step 949\n",
      "Loss: 0.11474476754665375\n",
      "Step 950\n",
      "Loss: 0.11469270288944244\n",
      "Step 951\n",
      "Loss: 0.11464059352874756\n",
      "Step 952\n",
      "Loss: 0.11458856612443924\n",
      "Step 953\n",
      "Loss: 0.11453654617071152\n",
      "Step 954\n",
      "Loss: 0.11448459327220917\n",
      "Step 955\n",
      "Loss: 0.11443265527486801\n",
      "Step 956\n",
      "Loss: 0.11438075453042984\n",
      "Step 957\n",
      "Loss: 0.11432883143424988\n",
      "Step 958\n",
      "Loss: 0.11427696794271469\n",
      "Step 959\n",
      "Loss: 0.11422520875930786\n",
      "Step 960\n",
      "Loss: 0.11417341232299805\n",
      "Step 961\n",
      "Loss: 0.11412167549133301\n",
      "Step 962\n",
      "Loss: 0.11406996101140976\n",
      "Step 963\n",
      "Loss: 0.11401820182800293\n",
      "Step 964\n",
      "Loss: 0.11396656185388565\n",
      "Step 965\n",
      "Loss: 0.11391497403383255\n",
      "Step 966\n",
      "Loss: 0.11386333405971527\n",
      "Step 967\n",
      "Loss: 0.11381174623966217\n",
      "Step 968\n",
      "Loss: 0.11376024782657623\n",
      "Step 969\n",
      "Loss: 0.11370868980884552\n",
      "Step 970\n",
      "Loss: 0.11365719884634018\n",
      "Step 971\n",
      "Loss: 0.11360570043325424\n",
      "Step 972\n",
      "Loss: 0.11355429887771606\n",
      "Step 973\n",
      "Loss: 0.1135028600692749\n",
      "Step 974\n",
      "Loss: 0.11345147341489792\n",
      "Step 975\n",
      "Loss: 0.11340010166168213\n",
      "Step 976\n",
      "Loss: 0.1133488118648529\n",
      "Step 977\n",
      "Loss: 0.1132974922657013\n",
      "Step 978\n",
      "Loss: 0.11324624717235565\n",
      "Step 979\n",
      "Loss: 0.11319497227668762\n",
      "Step 980\n",
      "Loss: 0.11314372718334198\n",
      "Step 981\n",
      "Loss: 0.11309254914522171\n",
      "Step 982\n",
      "Loss: 0.11304135620594025\n",
      "Step 983\n",
      "Loss: 0.11299023777246475\n",
      "Step 984\n",
      "Loss: 0.11293917149305344\n",
      "Step 985\n",
      "Loss: 0.11288806796073914\n",
      "Step 986\n",
      "Loss: 0.11283695697784424\n",
      "Step 987\n",
      "Loss: 0.11278590559959412\n",
      "Step 988\n",
      "Loss: 0.11273497343063354\n",
      "Step 989\n",
      "Loss: 0.11268390715122223\n",
      "Step 990\n",
      "Loss: 0.11263290047645569\n",
      "Step 991\n",
      "Loss: 0.11258197575807571\n",
      "Step 992\n",
      "Loss: 0.11253107339143753\n",
      "Step 993\n",
      "Loss: 0.11248011142015457\n",
      "Step 994\n",
      "Loss: 0.11242923140525818\n",
      "Step 995\n",
      "Loss: 0.11237838864326477\n",
      "Step 996\n",
      "Loss: 0.11232756078243256\n",
      "Step 997\n",
      "Loss: 0.1122768372297287\n",
      "Step 998\n",
      "Loss: 0.11222599446773529\n",
      "Step 999\n",
      "Loss: 0.11217529326677322\n"
     ]
    }
   ],
   "source": [
    "logs = train(model, X_train_scaled, y_train, optimiser, loss_fn, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dc31e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(logs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "835c149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9609)\n"
     ]
    }
   ],
   "source": [
    "# train accuracy\n",
    "train_preds = model(X_train_scaled)\n",
    "print(torch.mean((1*(train_preds > 0.5) == y_train)*1.)) # ~ 96.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "934c3cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9474)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "preds = model(X_test_scaled)\n",
    "torch.mean((1*(preds > 0.5) == y_test)*1.) # ~ 96.5%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
