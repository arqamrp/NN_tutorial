{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4384ea2",
   "metadata": {},
   "source": [
    "Authored by Arqam Patel. \n",
    "\n",
    "CC BY 4.0 License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b94490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0fd1b",
   "metadata": {},
   "source": [
    "$$ \\hat{y}_i = m x_i + c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec7c6ce",
   "metadata": {},
   "source": [
    "$$ L = \\frac{1}{N} \\sum (y_i - \\hat{y}_i )^2  = \\frac{1}{N} \\sum (y_i - mx_i - c)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839bc4eb",
   "metadata": {},
   "source": [
    "$$ \\frac{dL}{dm} = - \\frac{2}{N} \\sum (y_i - \\hat{y}_i) x_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa039f",
   "metadata": {},
   "source": [
    "$$ \\frac{dL}{dm} = - \\frac{2}{N} \\sum (y_i - \\hat{y}_i) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a3354a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralSLR():\n",
    "    def __init__(self, lr = 0.1):\n",
    "        # Parameters\n",
    "        self.m = 0.\n",
    "        self.c = 0.\n",
    "        \n",
    "        # NOT A PARAMETER. This is what's called a hyperparameter. \n",
    "        self.lr = lr\n",
    "    \n",
    "    # makes the forward pass i.e. computes model's predictions given the current parameters\n",
    "    def forward(self, data):\n",
    "        pred = self.m*data + self.c\n",
    "        return pred\n",
    "    \n",
    "    # computes the loss given current parameters\n",
    "    def loss(self, data, true):\n",
    "        N = len(x)\n",
    "        pred = self.forward(data)\n",
    "        loss = 1/N* np.sum((pred-true)**2)\n",
    "        return loss\n",
    "    \n",
    "    # computes gradients wrt params\n",
    "    def grad(self, data, true):\n",
    "        N = len(x)\n",
    "        pred = self.forward(data)\n",
    "        dldm = -2/N* np.sum((true- pred)*data)\n",
    "        dldc = -2/N * np.sum((true- pred))\n",
    "        return dldm, dldc\n",
    "    \n",
    "    # performs single grad descent step\n",
    "    def step(self, data, true):\n",
    "        dldm, dldc = self.grad(data, true)\n",
    "        self.m = self.m - self.lr*dldm\n",
    "        self.c = self.c - self.lr*dldc\n",
    "    \n",
    "    # just a wrapper function to conveniently perform grad descent multiple times\n",
    "    def train(self, data, true, n_steps):\n",
    "        for i in range(n_steps):\n",
    "            print(f\"Step no {i}\")\n",
    "            self.step(data, true)\n",
    "            print(f\"m: {self.m}\")\n",
    "            print(f\"c: {self.c}\")\n",
    "            print(f\"Loss {self.loss(data, true)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4f769d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating training data \n",
    "x = np.random.randn(40)\n",
    "y = 5*x - 1 + np.random.randn(40)/100 # adding a little bit of noise in the true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2441090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03175921,  0.44806265,  0.19392662, -1.16852062,  0.05050875,\n",
       "       -1.75192881, -0.02878816,  0.86087693,  1.69576665,  0.61179224,\n",
       "       -0.49353732, -0.39522401,  1.26148996,  0.46887088,  1.32129613,\n",
       "       -0.85409231, -0.44458541, -2.30406965, -1.42827572,  0.9172523 ,\n",
       "       -0.11919319, -0.68664254,  0.07233606,  0.57468015, -0.73574885,\n",
       "        0.31318194,  1.92184232, -0.12083301,  1.24597831, -0.19361131,\n",
       "       -1.6018749 ,  0.36796196, -0.16712353, -0.76188552, -1.12055819,\n",
       "       -0.45243497, -1.33295597, -0.20193059,  1.13357446, -0.71531842])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "126cbaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.82793079,   1.23618882,  -0.02689421,  -6.84205638,\n",
       "        -0.74778569,  -9.7536547 ,  -1.16972151,   3.30212752,\n",
       "         7.46909761,   2.06838191,  -3.4699038 ,  -2.96896461,\n",
       "         5.30482791,   1.3384159 ,   5.60115392,  -5.28745568,\n",
       "        -3.21838186, -12.53606868,  -8.13000479,   3.58799181,\n",
       "        -1.60278286,  -4.44325782,  -0.63225428,   1.85862149,\n",
       "        -4.66314865,   0.56661144,   8.60308913,  -1.61371708,\n",
       "         5.25467805,  -1.96121569,  -9.01008994,   0.81863379,\n",
       "        -1.84197863,  -4.79259935,  -6.62571367,  -3.25874329,\n",
       "        -7.67069634,  -2.0260326 ,   4.67541111,  -4.55627991])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e226595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisiing the model\n",
    "model1 = NeuralSLR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bef23966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step no 0\n",
      "m: 4.99902128170198\n",
      "c: -0.9992428118105702\n",
      "Loss 0.00013996169856231612\n",
      "Step no 1\n",
      "m: 4.999283527985586\n",
      "c: -0.9996729325173602\n",
      "Loss 0.000137692586938622\n",
      "Step no 2\n",
      "m: 4.999489692605844\n",
      "c: -1.0000123244166244\n",
      "Loss 0.00013628261365388722\n",
      "Step no 3\n",
      "m: 4.99965174621118\n",
      "c: -1.0002801393680307\n",
      "Loss 0.0001354064788417823\n",
      "Step no 4\n",
      "m: 4.999779108154706\n",
      "c: -1.0004914841073493\n",
      "Loss 0.00013486205576065057\n",
      "Step no 5\n",
      "m: 4.999879189591526\n",
      "c: -1.0006582750411561\n",
      "Loss 0.00013452375091176663\n",
      "Step no 6\n",
      "m: 4.9999578210792075\n",
      "c: -1.000789912339498\n",
      "Loss 0.00013431352481079383\n",
      "Step no 7\n",
      "m: 5.000019589223949\n",
      "c: -1.0008938115389256\n",
      "Loss 0.00013418288593054791\n",
      "Step no 8\n",
      "m: 5.000068101703146\n",
      "c: -1.0009758227855257\n",
      "Loss 0.0001341017026843343\n",
      "Step no 9\n",
      "m: 5.000106195889412\n",
      "c: -1.001040561474878\n",
      "Loss 0.00013405125173386503\n",
      "Step no 10\n",
      "m: 5.00013610306701\n",
      "c: -1.00109166902133\n",
      "Loss 0.00013401989851199067\n",
      "Step no 11\n",
      "m: 5.000159577684148\n",
      "c: -1.001132018527393\n",
      "Loss 0.00013400041326644386\n",
      "Step no 12\n",
      "m: 5.000177999077932\n",
      "c: -1.001163877000491\n",
      "Loss 0.00013398830333807623\n",
      "Step no 13\n",
      "m: 5.000192451528342\n",
      "c: -1.0011890333014237\n",
      "Loss 0.00013398077688322395\n",
      "Step no 14\n",
      "m: 5.000203787252681\n",
      "c: -1.0012088990669819\n",
      "Loss 0.00013397609895154523\n",
      "Step no 15\n",
      "m: 5.0002126759716\n",
      "c: -1.0012245883179238\n",
      "Loss 0.00013397319136077012\n",
      "Step no 16\n",
      "m: 5.000219643905644\n",
      "c: -1.0012369802561503\n",
      "Loss 0.00013397138406057589\n",
      "Step no 17\n",
      "m: 5.000225104453195\n",
      "c: -1.001246768802849\n",
      "Loss 0.0001339702606288406\n",
      "Step no 18\n",
      "m: 5.000229382321865\n",
      "c: -1.0012545016786547\n",
      "Loss 0.00013396956226071295\n",
      "Step no 19\n",
      "m: 5.000232732508304\n",
      "c: -1.0012606112348599\n",
      "Loss 0.00013396912810491134\n",
      "Step no 20\n",
      "m: 5.000235355224514\n",
      "c: -1.00126543877789\n",
      "Loss 0.00013396885818642016\n",
      "Step no 21\n",
      "m: 5.000237407634959\n",
      "c: -1.0012692537611072\n",
      "Loss 0.00013396869036473658\n",
      "Step no 22\n",
      "m: 5.000239013084739\n",
      "c: -1.0012722689276892\n",
      "Loss 0.00013396858601417448\n",
      "Step no 23\n",
      "m: 5.000240268354175\n",
      "c: -1.0012746522593825\n",
      "Loss 0.00013396852112445383\n",
      "Step no 24\n",
      "m: 5.000241249361093\n",
      "c: -1.0012765364053575\n",
      "Loss 0.0001339684807696855\n",
      "Step no 25\n",
      "m: 5.000242015642293\n",
      "c: -1.0012780261229937\n",
      "Loss 0.00013396845567074746\n",
      "Step no 26\n",
      "m: 5.000242613875037\n",
      "c: -1.0012792041501117\n",
      "Loss 0.00013396844005863449\n",
      "Step no 27\n",
      "m: 5.000243080643722\n",
      "c: -1.0012801358395842\n",
      "Loss 0.00013396843034642542\n",
      "Step no 28\n",
      "m: 5.000243444613166\n",
      "c: -1.0012808728173892\n",
      "Loss 0.00013396842430374125\n",
      "Step no 29\n",
      "m: 5.000243728235449\n",
      "c: -1.001281455870066\n",
      "Loss 0.0001339684205436167\n",
      "Step no 30\n",
      "m: 5.000243949090172\n",
      "c: -1.0012819172240586\n",
      "Loss 0.0001339684182034795\n",
      "Step no 31\n",
      "m: 5.000244120936641\n",
      "c: -1.001282282345146\n",
      "Loss 0.00013396841674683275\n",
      "Step no 32\n",
      "m: 5.000244254539717\n",
      "c: -1.0012825713591111\n",
      "Loss 0.00013396841583996328\n",
      "Step no 33\n",
      "m: 5.000244358317875\n",
      "c: -1.0012828001734606\n",
      "Loss 0.00013396841527525062\n",
      "Step no 34\n",
      "m: 5.000244438851616\n",
      "c: -1.0012829813631727\n",
      "Loss 0.00013396841492352557\n",
      "Step no 35\n",
      "m: 5.000244501282222\n",
      "c: -1.001283124870177\n",
      "Loss 0.0001339684147044017\n",
      "Step no 36\n",
      "m: 5.000244549624428\n",
      "c: -1.001283238555783\n",
      "Loss 0.0001339684145678527\n",
      "Step no 37\n",
      "m: 5.00024458701151\n",
      "c: -1.0012833286370146\n",
      "Loss 0.00013396841448273743\n",
      "Step no 38\n",
      "m: 5.00024461588735\n",
      "c: -1.0012834000312802\n",
      "Loss 0.00013396841442966413\n",
      "Step no 39\n",
      "m: 5.000244638156896\n",
      "c: -1.0012834566286637\n",
      "Loss 0.00013396841439655878\n",
      "Step no 40\n",
      "m: 5.00024465530398\n",
      "c: -1.0012835015070574\n",
      "Loss 0.0001339684143759008\n",
      "Step no 41\n",
      "m: 5.000244668483563\n",
      "c: -1.0012835371021558\n",
      "Loss 0.00013396841436300332\n",
      "Step no 42\n",
      "m: 5.0002446785939005\n",
      "c: -1.0012835653417944\n",
      "Loss 0.00013396841435494935\n",
      "Step no 43\n",
      "m: 5.000244686333001\n",
      "c: -1.0012835877521271\n",
      "Loss 0.00013396841434991822\n",
      "Step no 44\n",
      "m: 5.000244692242762\n",
      "c: -1.0012836055415548\n",
      "Loss 0.0001339684143467702\n",
      "Step no 45\n",
      "m: 5.000244696743462\n",
      "c: -1.0012836196670765\n",
      "Loss 0.00013396841434480403\n",
      "Step no 46\n",
      "m: 5.0002447001607\n",
      "c: -1.0012836308867519\n",
      "Loss 0.00013396841434357116\n",
      "Step no 47\n",
      "m: 5.000244702746428\n",
      "c: -1.0012836398011873\n",
      "Loss 0.00013396841434279983\n",
      "Step no 48\n",
      "m: 5.000244704695352\n",
      "c: -1.001283646886348\n",
      "Loss 0.0001339684143423159\n",
      "Step no 49\n",
      "m: 5.000244706157735\n",
      "c: -1.0012836525195132\n",
      "Loss 0.0001339684143420117\n",
      "Step no 50\n",
      "m: 5.000244707249357\n",
      "c: -1.0012836569998105\n",
      "Loss 0.00013396841434182213\n",
      "Step no 51\n",
      "m: 5.000244708059278\n",
      "c: -1.0012836605644646\n",
      "Loss 0.00013396841434170284\n",
      "Step no 52\n",
      "m: 5.000244708655878\n",
      "c: -1.001283663401658\n",
      "Loss 0.00013396841434162664\n",
      "Step no 53\n",
      "m: 5.00024470909155\n",
      "c: -1.0012836656607098\n",
      "Loss 0.00013396841434158013\n",
      "Step no 54\n",
      "m: 5.000244709406346\n",
      "c: -1.0012836674601353\n",
      "Loss 0.0001339684143415496\n",
      "Step no 55\n",
      "m: 5.0002447096308025\n",
      "c: -1.0012836688940283\n",
      "Loss 0.00013396841434152953\n",
      "Step no 56\n",
      "m: 5.000244709788139\n",
      "c: -1.001283670037116\n",
      "Loss 0.00013396841434151847\n",
      "Step no 57\n",
      "m: 5.000244709895951\n",
      "c: -1.0012836709487636\n",
      "Loss 0.00013396841434150863\n",
      "Step no 58\n",
      "m: 5.000244709967526\n",
      "c: -1.0012836716761475\n",
      "Loss 0.00013396841434150535\n",
      "Step no 59\n",
      "m: 5.000244710012851\n",
      "c: -1.0012836722567706\n",
      "Loss 0.0001339684143415028\n",
      "Step no 60\n",
      "m: 5.0002447100394\n",
      "c: -1.001283672720456\n",
      "Loss 0.0001339684143415008\n",
      "Step no 61\n",
      "m: 5.000244710052734\n",
      "c: -1.001283673090928\n",
      "Loss 0.00013396841434150058\n",
      "Step no 62\n",
      "m: 5.000244710056964\n",
      "c: -1.0012836733870665\n",
      "Loss 0.0001339684143415009\n",
      "Step no 63\n",
      "m: 5.0002447100551\n",
      "c: -1.0012836736239012\n",
      "Loss 0.0001339684143414985\n",
      "Step no 64\n",
      "m: 5.000244710049332\n",
      "c: -1.0012836738134026\n",
      "Loss 0.0001339684143414989\n",
      "Step no 65\n",
      "m: 5.000244710041227\n",
      "c: -1.0012836739651072\n",
      "Loss 0.00013396841434149968\n",
      "Step no 66\n",
      "m: 5.000244710031896\n",
      "c: -1.0012836740866162\n",
      "Loss 0.00013396841434149852\n",
      "Step no 67\n",
      "m: 5.0002447100221055\n",
      "c: -1.0012836741839908\n",
      "Loss 0.0001339684143414968\n",
      "Step no 68\n",
      "m: 5.000244710012374\n",
      "c: -1.001283674262066\n",
      "Loss 0.00013396841434149868\n",
      "Step no 69\n",
      "m: 5.000244710003036\n",
      "c: -1.001283674324701\n",
      "Loss 0.00013396841434149754\n",
      "Step no 70\n",
      "m: 5.0002447099942975\n",
      "c: -1.0012836743749762\n",
      "Loss 0.00013396841434149703\n",
      "Step no 71\n",
      "m: 5.000244709986268\n",
      "c: -1.0012836744153533\n",
      "Loss 0.0001339684143414966\n",
      "Step no 72\n",
      "m: 5.000244709978996\n",
      "c: -1.001283674447799\n",
      "Loss 0.00013396841434149586\n",
      "Step no 73\n",
      "m: 5.000244709972482\n",
      "c: -1.001283674473886\n",
      "Loss 0.00013396841434149643\n",
      "Step no 74\n",
      "m: 5.000244709966702\n",
      "c: -1.0012836744948725\n",
      "Loss 0.00013396841434149738\n",
      "Step no 75\n",
      "m: 5.000244709961611\n",
      "c: -1.0012836745117655\n",
      "Loss 0.00013396841434149903\n",
      "Step no 76\n",
      "m: 5.000244709957156\n",
      "c: -1.001283674525371\n",
      "Loss 0.000133968414341496\n",
      "Step no 77\n",
      "m: 5.000244709953279\n",
      "c: -1.0012836745363354\n",
      "Loss 0.00013396841434149884\n",
      "Step no 78\n",
      "m: 5.00024470994992\n",
      "c: -1.0012836745451765\n",
      "Loss 0.00013396841434149798\n",
      "Step no 79\n",
      "m: 5.0002447099470215\n",
      "c: -1.0012836745523097\n",
      "Loss 0.00013396841434149657\n",
      "Step no 80\n",
      "m: 5.00024470994453\n",
      "c: -1.0012836745580682\n",
      "Loss 0.00013396841434149592\n",
      "Step no 81\n",
      "m: 5.000244709942395\n",
      "c: -1.0012836745627198\n",
      "Loss 0.00013396841434149833\n",
      "Step no 82\n",
      "m: 5.00024470994057\n",
      "c: -1.0012836745664793\n",
      "Loss 0.00013396841434149705\n",
      "Step no 83\n",
      "m: 5.000244709939014\n",
      "c: -1.0012836745695195\n",
      "Loss 0.000133968414341496\n",
      "Step no 84\n",
      "m: 5.00024470993769\n",
      "c: -1.0012836745719798\n",
      "Loss 0.00013396841434149917\n",
      "Step no 85\n",
      "m: 5.000244709936567\n",
      "c: -1.0012836745739717\n",
      "Loss 0.00013396841434149781\n",
      "Step no 86\n",
      "m: 5.000244709935615\n",
      "c: -1.0012836745755853\n",
      "Loss 0.00013396841434149722\n",
      "Step no 87\n",
      "m: 5.000244709934809\n",
      "c: -1.0012836745768934\n",
      "Loss 0.00013396841434149757\n",
      "Step no 88\n",
      "m: 5.000244709934129\n",
      "c: -1.0012836745779543\n",
      "Loss 0.00013396841434149939\n",
      "Step no 89\n",
      "m: 5.000244709933555\n",
      "c: -1.0012836745788152\n",
      "Loss 0.0001339684143414973\n",
      "Step no 90\n",
      "m: 5.000244709933072\n",
      "c: -1.0012836745795142\n",
      "Loss 0.00013396841434149673\n",
      "Step no 91\n",
      "m: 5.000244709932665\n",
      "c: -1.001283674580082\n",
      "Loss 0.0001339684143414965\n",
      "Step no 92\n",
      "m: 5.000244709932323\n",
      "c: -1.0012836745805436\n",
      "Loss 0.00013396841434149852\n",
      "Step no 93\n",
      "m: 5.000244709932036\n",
      "c: -1.001283674580919\n",
      "Loss 0.0001339684143414973\n",
      "Step no 94\n",
      "m: 5.0002447099317955\n",
      "c: -1.0012836745812246\n",
      "Loss 0.00013396841434149386\n",
      "Step no 95\n",
      "m: 5.000244709931593\n",
      "c: -1.0012836745814733\n",
      "Loss 0.00013396841434149998\n",
      "Step no 96\n",
      "m: 5.000244709931423\n",
      "c: -1.0012836745816758\n",
      "Loss 0.0001339684143414983\n",
      "Step no 97\n",
      "m: 5.000244709931282\n",
      "c: -1.001283674581841\n",
      "Loss 0.0001339684143414997\n",
      "Step no 98\n",
      "m: 5.000244709931164\n",
      "c: -1.0012836745819755\n",
      "Loss 0.00013396841434149963\n",
      "Step no 99\n",
      "m: 5.0002447099310645\n",
      "c: -1.0012836745820854\n",
      "Loss 0.0001339684143414975\n"
     ]
    }
   ],
   "source": [
    "# the model parameters should converge towards the true parameters we used to generate the data\n",
    "model1.train(x, y, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d9f5b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.00024471]\n",
      " [-1.00128367]]\n"
     ]
    }
   ],
   "source": [
    "# btw you didn't really need grad descent because Linear Regression generally has an analytical solution\n",
    "X = np.vstack((x, np.ones(40))).T\n",
    "params = (np.linalg.inv(X.T@X) @ X.T)@np.expand_dims(y, axis = 1)\n",
    "print(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
